\documentclass[a4paper,12pt]{article}
\textwidth 16.5cm
\textheight 22.5cm
\oddsidemargin 0pt
\topmargin -1cm
%\textwidth 17cm
%\textheight 24.5cm
%\oddsidemargin 0pt
%\topmargin -2cm

%\renewcommand{\baselinestretch}{0.98}

%\setlength{\parindent}{.3in}
%\setlength{\parskip}{.05in}

\usepackage{latexsym,amsmath,amssymb,amsfonts,amsthm,bbm,color,xcolor,breakcites,dsfont}
\usepackage[round]{natbib}
\usepackage[normalem]{ulem}
\usepackage{algorithm, algpseudocode}
%\usepackage[ruled, vlined]{algorithm2e, algpseudocode}
\usepackage{dsfont,url}
\usepackage{enumerate}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue,breaklinks]{hyperref}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{psfrag}
\usepackage{caption, subcaption}
\usepackage{comment}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{assumption}{A\!\!}
\renewcommand{\baselinestretch}{1.25}


\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\sargmin}{sargmin}
\DeclareMathOperator*{\sargmax}{sargmax}

\newcommand{\teal}[1]{{\color{teal}#1}} % Tengyao's edits
\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\blue}[1]{{\color{blue}#1}}
\newcommand{\orange}[1]{{\color{orange}#1}}

\newenvironment{definition}[1][Definition:]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example:]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark:]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}


\title{High-dimensional nonparametric density estimation via symmetry and shape constraints}
\author{Min Xu\footnote{Email address: minx@wharton.upenn.edu} and Richard J. Samworth\footnote{Email address: r.samworth@statslab.cam.ac.uk} \\ Wharton Business School, University of Pennsylvania and \\
Statistical Laboratory, University of Cambridge}
\date{September 9, 2017}

\begin{document}

\maketitle
%\tableofcontents
%\newpage



\abstract{
  We tackle the problem of high-dimensional nonparametric density estimation by taking the class of log-concave densities on $\mathbb{R}^p$ and incorporating within it a symmetry assumption, which both mitigates the curse of dimensionality and admits a scalable estimation algorithm.  The case of spherical symmetry is studied in detail through the notion of spherically symmetric log-concave projection.  
%To study the resulting estimation problem, we formalise the notion of maximum likelihood projection of an arbitrary distribution onto the class of spherically symmetric log-concave densities and characterize the existence, continuity, and other basic properties of the projection. 
In the setting of independent and identically distributed data in $\mathbb{R}^p$, we prove that the spherically symmetric log-concave maximum likelihood estimator has a worst case risk bound with respect to, e.g., squared Hellinger loss, of $O(n^{-4/5})$, independent of $p$.  Moreover, we show that the estimator is adaptive in the sense that if the data generating density admits a special form, then a nearly parametric rate may be attained.  Our estimation algorithm is fast even when $n$ and $p$ are on the order of hundreds of thousands, and we illustrate its strong performance on simulated data.
}


\section{Introduction}

Density estimation emerged as one of the fundamental challenges in Statistics very soon after its inception as a field.  Up until halfway through the last century, approaches based on parametric (often Gaussian) assumptions or histograms/contingency tables were dominant \citep{Fisher1922,Fisher1925}.  However, the restrictions of these techniques has led, since the 1950s, to an enormous research effort devoted to exploring nonparametric methods, primarily based on smoothness assumptions, but also on shape constraints.  These include kernel density estimation \citep{rosenblatt1956remarks,WandJones1995}, wavelets \citep{donoho1996density} and other orthogonal series methods, splines \citep{gu1993smoothing}, as well as techniques based on monotonicity \citep{grenander1956theory}, log-concavity \citep{CSS2010} and others.  Although highly successful for low-dimensional data, these approaches all encounter two serious difficulties in moderate- or high-dimensional regimes: first, theoretical performance is limited by minimax lower bounds that characterise the `curse of dimensionality' \citep[e.g.][]{ibragimov1983estimation}; and second, computational issues may become a bottleneck, often exacerbated by the need to choose (multiple) smoothing parameters.  

In parallel to these developments, modern technology now allows the routine collection of extremely high-dimensional data sets, leading to a great demand for reliable and scalable density estimation algorithms.  To emphasise the challenge here, let $\mathcal{F}_p$ denote the class of upper semi-continuous, log-concave densities on $\mathbb{R}^p$, and let $X_1,\ldots,X_n$ be independent and identically distributed random vectors with density $f_0 \in \mathcal{F}_p$.  \citet{kim2016global} proved that for each $p \in \mathbb{N}$, there exists $c_p > 0$ such that 
\[
\inf_{\tilde{f}_n} \sup_{f_0 \in \mathcal{F}_p} \mathbb{E}d_{\mathrm{H}}^2(\tilde{f}_n,f_0) \geq \left\{ \begin{array}{ll} c_1 n^{-4/5} & \mbox{if $p=1$} \\
c_p n^{-2/(p+1)} & \mbox{if $p \geq 2$}, \end{array} \right.
\]
where $d_{\mathrm{H}}^2(f,g) := \int_{\mathbb{R}^p} (f^{1/2}-g^{1/2})^2$ denotes the squared Hellinger distance between densities $f$ and $g$ on $\mathbb{R}^p$, and where the infimum is taken over all estimators $\tilde{f}_n$ of $f_0$ based on $X_1,\ldots,X_n$.  This suggests that very large sample sizes would be required for an adequate approximation to the true density, even for $p=5$.  In view of these fundamental theoretical limitations, it is natural to consider imposing additional structure on the problem, while simultaneously seeking to retain the desirable flexibility of the nonparametric paradigm.   

In this paper, we propose a new method for high-dimensional, nonparametric density estimation by incorporating symmetry constraints into the shape-constrained class.  We demonstrate that this approach facilitates an efficient algorithm that can evade the curse of dimensionality in terms of its rate of convergence.  In this preliminary work, we focus on perhaps the simplest symmetry constraint, and consider the class of upper semi-continuous, spherically symmetric log-concave densities $\mathcal{F}_p^{\mathrm{SS}}$.  In particular, we study the maximum likelihood estimator (MLE) $\hat{f}_n$ of $f_0$ over the class $\mathcal{F}_p^{\mathrm{SS}}$.  An attractive feature of this estimator is that, despite the potentially high-dimensional nature of the problem, it does not require the choice of any tuning parameters.

Writing $\Phi$ for the class of upper semi-continuous, concave and decreasing functions on $[0,\infty)$, it turns out (cf.~Proposition~\ref{Prop:SSLC} below) that the density $f_0$ belongs to $\mathcal{F}_p^{\mathrm{SS}}$ if and only if $f_0(x) = e^{\phi_0(\|x\|)}$ for some $\phi_0 \in \Phi$.  We further note that the univariate random variables $Z_i := \|X_i\|$ for $i=1,\ldots,n$ each have (log-concave) density $h_0(r) := c_pr^{p-1}e^{\phi_0(r)}$, where $c_p := 2\pi^{p/2}/\Gamma(p/2)$.  These observations enable us to reduce the problem of computing $\hat{f}_n$ to obtaining the MLE $\hat{\phi}_n$ of $\phi_0$ based on $Z_1,\ldots,Z_n$, over the subclass of $\phi \in \Phi$ such that $r \mapsto c_pr^{p-1}e^{\phi(r)}$ is a density.

Our results on the theoretical performance of $\hat{f}_n$ are presented in terms of the divergence measure
\[
d_X^2(\hat{f}_n,f_0) := \frac{1}{n}\sum_{i=1}^n \log \frac{\hat{f}_n(X_i)}{f_0(X_i)}.
\]
We show in Proposition~\ref{Prop:ProjectionBasicProperties}(iv) below that $d_X^2(\hat{f}_n,f_0) \geq \int_{\mathbb{R}^p} \hat{f}_n \log (\hat{f}_n/f_0) =: d_{\mathrm{KL}}^2(\hat{f}_n,f_0)$, so that our upper bounds on $\mathbb{E} d_X^2(\hat{f}_n,f_0)$ immediately yield the same upper bounds on the expected Kullback--Leibler divergence (as well as the risks in the squared total variation and squared Hellinger distances, for instance).  One of our main results (Theorem~\ref{Thm:WorstCase}) is that there exists a universal constant $C > 0$ such that
\[
\sup_{f_0 \in \mathcal{F}_p^{\mathrm{SS}}} \mathbb{E}d_X^2(\hat{f}_n,f_0) \leq Cn^{-4/5}.
\]
Thus, there is no dependence on $p$ in this worst case risk bound, and we can expect good performance even in high dimensions.  See Section~\ref{Sec:Simulations} for empirical verification.  We also elucidate the adaptation behaviour of $\hat{f}_n$.  More precisely, for $k \in \mathbb{N}$, we let $\Phi^{(k)}$ denote the set of $\phi \in \Phi$ that are piecewise linear on $\mathrm{dom}(\phi) := \{r:\phi(r) > -\infty\}$, with at most $k$ linear pieces. We also let $\mathcal{H}^{(k)}$ denote the class of densities $h$ of form $h(r) = r^{p-1} e^{\phi(r)}$ for $\phi \in \Phi^{(k)}$ and let $h_0(r) := r^{p-1} e^{\phi_0(r)}$. In Theorem~\ref{Thm:Adaptation}, we derive an oracle inequality: writing $\nu_k^2 := 2 \wedge \inf_{h \in \mathcal{H}^{(k)}} d_{\mathrm{KL}}^2(h_0, h)$, there exists a universal constant $C > 0$ such that for every $f_0 \in \mathcal{F}_p^{\mathrm{SS}}$, 
\[
\mathbb{E}d_X^2(\hat{f}_n,f_0) \leq C \min_{k =1,\ldots,n} \biggl\{\frac{k^{4/5}\nu_k^{2/5}}{n^{4/5}}\log\Bigl(\frac{en}{k\nu_k}\Bigr) + \frac{k}{n}\log^{5/4}\Bigl(\frac{en}{k}\Bigr)\biggr\}.
\]
This result reveals that $\hat{f}_n$ adapts to densities $f_0$ that are close to being of the form $e^\phi$ for some $\phi \in \Phi^{(k)}$ with $k$ not too large; in particular, an almost parametric rate of $\frac{k}{n}\log^{5/4}\bigl(\frac{en}{k}\bigr)$ can be attained.  In fact, this result is stronger than what is typically referred to as a sharp oracle inequality; see the remark after Theorem~\ref{Thm:Adaptation}.

We remark that an alternative to our proposal, which relies only on existing methods, would be to compute the estimator $\tilde{f}_n$ given by
\begin{equation}
\label{Eq:fntilde}
\tilde{f}_n(x) := \left\{ \begin{array}{ll} \tilde{h}_n(\|x\|)/(c_p\|x\|^{p-1}) & \mbox{if $x \neq 0$} \\
0 & \mbox{if $x=0$,} \end{array} \right. 
\end{equation}
where $\tilde{h}_n$ is the ordinary log-concave MLE of $h_0$ based on $Z_1,\ldots,Z_n$.  This estimator $\tilde{f}_n$ ignores the special structure of $h_0$, but is similarly straightforward to compute and attains the same worst case rate of convergence as $\hat{f}_n$ \citep[][Theorem~5]{kim2016global}\footnote{Although Theorem~5 of \citet{kim2016global} is stated for the squared Hellinger risk, it can easily be extended to a bound for the $d_X^2$ risk by appealing to Corollary~7.5 of \citet{vandegeer2000empirical}.}; this is in fact the minimax rate of convergence over the class of log-concave densities \citep[][Theorem~1]{kim2016global}.  We argue, however, that $\hat{f}_n$ has several advantages over $\tilde{f}_n$ in this context, and list these in roughly decreasing order of importance:
\begin{enumerate}
\item The estimator $\tilde{f}_n$ is inconsistent at $x=0$.  Indeed $\tilde{h}_n(x) = 0$ whenever $\|x\| < \min_i Z_i$, and the division by $\|x\|^{p-1}$ in~\eqref{Eq:fntilde} means that the estimator behaves poorly for small $\|x\|$; see~Figure~\ref{Fig:SSLC}.  By contrast, $\hat{f}_n$ is uniformly consistent over closed Euclidean balls contained in the interior of the support of $f_0$ (Proposition~\ref{Prop:Continuity});
\item As mentioned in our discussion above, the estimator $\hat{f}_n$ attains faster rates of convergence when the true density has a simple structure;
\item The estimator $\hat{f}_n$ takes values in the relevant class $\mathcal{F}_p^{\mathrm{SS}}$, whereas $\tilde{f}_n$ does not;
\item The estimator $\hat{f}_n$ exists in slightly greater generality than $\tilde{f}_n$ (cf.~the remark following Proposition~\ref{Prop:ProjectionExistence}(iv)).
\end{enumerate}
The differences between $\hat{f}_n$ and $\tilde{f}_n$ are illustrated in Figure~\ref{Fig:SSLC}.  
\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.4\linewidth}
    \centering
    \includegraphics[scale=0.3, trim=140 0 0 0]{figs/sslc2.pdf}
   % \caption{\label{Fig:sslc2d} sslc-MLE}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\linewidth}
    \centering
    \includegraphics[scale=0.3, trim=140 0 0 0]{figs/lc2.pdf}
  %  \caption{\label{Fig:lc2d} ordinary MLE}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\linewidth}
    \centering
    \includegraphics[scale=0.3]{figs/sslc1d.pdf}
  %  \caption{\label{Fig:sslc1d}}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\linewidth}
    \centering
    \includegraphics[scale=0.3]{figs/lc1d.pdf}
  %  \caption{\label{Fig:lc1d}}
  \end{subfigure}
\caption{\label{Fig:SSLC}A comparison of the spherically-symmetric log-concave MLE $\hat{f}_n$ (left) and the ordinary log-concave MLE $\tilde{f}_n$ (right) based on a sample of size $n=1000$ from a standard bivariate normal distribution.  The top plots give the bivariate density estimates, while the bottom ones present the corresponding estimates $e^{\hat{\phi}_n}$ and $e^{\tilde{\phi}_n}$ of $e^{\phi_0}$.}
\end{figure}

Central to our approach is the notion of spherically symmetric log-concave projection, whose existence, uniqueness and properties we study in detail in Section~\ref{Sec:SSLCProjections}.  Significant additional technical challenges, relative to the study of the ordinary log-concave MLE, are caused by the fact that this projection does not have the same moment preservation properties.  In Section~\ref{Sec:Moments}, we therefore develop new arguments to understand how moments of the underlying distribution change under projection; these are crucial to prove that, with high probability, $\hat{\phi}_n$ belongs to a class with sufficiently small global and local bracketing entropy bounds (Sections~\ref{Sec:BracketExtremeEvent} and~\ref{Sec:LocalBrackets}) for our risk bounds to hold.  Our algorithm for computing $\hat{f}_n$, presented in Section~\ref{Sec:Algorithm}, is based on an Active Set approach, and is a variant of that proposed by \citet{dumbgen2007active} for computing the ordinary log-concave MLE in the univariate case.  Its algorithmic complexity only depends on $p$ through the need to compute $Z_1,\ldots,Z_n$ at the outset, and therefore scales extremely well to high-dimensional settings.

In addition to the work mentioned above on estimation over the class $\mathcal{F}_p$ of log-concave densities on $\mathbb{R}^p$, multivariate shape-constrained density estimation has also been considered over other classes, including the set of block decreasing densities on $[0,1]^p$ \citep{polonik1995density,polonik1998silhouette,biau2003risk,gao2007entropy}, and the class of $s$-concave densities on $\mathbb{R}^p$ \citep{doss2016global,han2016approximation}.  In the former case, for uniformly bounded densities, \citet{biau2003risk} established a minimax lower bound in total variation distance of order $n^{-1/(p+2)}$, revealing a similar curse of dimensionality, while in the latter case, the main interest has been in the classes with $s < 0$, which contain the class $\mathcal{F}_p$, so the same minimax lower bounds apply as for $\mathcal{F}_p$.  Various other simplifying structures and methods have also been considered for nonparametric high-dimensional density estimation, including kernel approaches for forest density estimation \citep{liu2011forest} and star-shaped density estimation \citep{liebscher2016estimation}, as well as nonparametric maximum likelihood methods for independent component analysis \citep{samworth2012independent}.  Perhaps most closely related to this work is the approach of \citet{bhattacharya2012adaptive}, who consider a maximum likelihood approach (as well as spline approximations) to estimating the generator of an elliptically symmetric distribution with decreasing generator.

We conclude the introduction with some notation used throughout the paper.  We use $\| \cdot \|$ to denote the $\ell_2$ norm of a vector and $\|\cdot\|_\infty$ for the supremum norm of a function.  For $x \in \mathbb{R}$, we write $x_+ := \max(x, 0)$ and $x_- := \max(-x, 0)$. We use $\lambda(\cdot)$ to denote Lebesgue measure on Euclidean space.  Given a sample $Z_1,\ldots, Z_n$, we write $Z_{(1)} \leq \ldots \leq Z_{(n)}$ for the corresponding order statistics.  If $h$ and $h'$ are log-concave functions defined on a subset of $\mathbb{R}$, we write $h \ll h'$ if $\mathrm{dom}(\log h) \subseteq \mathrm{dom}(\log h')$.  Let $S \subseteq \mathbb{R}$, and let $\mathcal{G}$ be a class of non-negative functions whose domains include $S$. For $\epsilon > 0$, let $N_{[]}(\epsilon, \mathcal{G}, d_{\mathrm{H}},S)$ denote the smallest $M \in \mathbb{N}$ for which there exist pairs of functions $\{[g_{L, j}, g_{U, j}]: j = 1, \dots, M\}$ such that     
  \begin{equation*}
    \int_S (g_{U, j}^{1/2} - g_{L, j}^{1/2})^2 \leq \epsilon^2  
  \end{equation*}
for every $j=1,\ldots,M$, and such that for every $g \in \mathcal{G}$, there exists $j^* \in \{1,\ldots,M\}$ with $g_{L, j^*}(x) \leq g(x) \leq g_{U, j^*}(x)$ for every $x \in S$.  We also define the $\epsilon$-bracketing entropy of $\mathcal{G}$ over $S$ with respect to the Hellinger distance by $H_{[]}(\epsilon, \mathcal{G}, d_{\mathrm{H}},S) := \log N_{[]}(\epsilon, \mathcal{G}, d_{\mathrm{H}},S)$ and write $H_{[]}(\epsilon, \mathcal{G}, d_{\mathrm{H}}) := H_{[]}(\epsilon, \mathcal{G}, d_{\mathrm{H}},\mathbb{R})$ when $S = \mathbb{R}$.  

\section{Spherically symmetric log-concave densities and projections}
\label{Sec:SSLCProjections}

%\subsection{Spherically-symmetric log-concave densities}

Recall from the introduction that $\mathcal{F}_p^{\mathrm{SS}}$ denotes the class of upper semi-continuous, spherically symmetric log-concave densities on $\mathbb{R}^p$ and $\Phi$ denotes the class of upper semi-continuous, decreasing and concave functions on $[0,\infty)$.  The following basic characterisation of $\mathcal{F}_p^{\mathrm{SS}}$ underpins our methodological and theoretical development.
\begin{proposition}
\label{Prop:SSLC}
Let $f$ be a density on $\mathbb{R}^p$.  Then $f \in \mathcal{F}_p^{\mathrm{SS}}$ if and only if we can write $f(x) = e^{\phi(\|x\|)}$ for some $\phi \in \Phi$.
\end{proposition}

%\subsection{Projection Operators}

For any fixed $a \geq 0$, let $\Phi_{a}$ be the set of upper semi-continuous functions $\phi : [a, \infty) \rightarrow [-\infty, \infty)$ such that $\phi$ is decreasing and concave, and set $\mathcal{H}_{a} := \bigl\{r \mapsto r^{p-1}e^{\phi(r)} : \phi \in \Phi_{a}, \int_{a}^\infty r^{p-1}e^{\phi(r)} \, dr = 1\bigr\}$.  We continue to write $\Phi = \Phi_0$ and also write $\mathcal{H} = \mathcal{H}_0$ as shorthand.
%\begin{definition}

Let $P$ be a probability measure on $\mathbb{R}^p$. We define
\begin{equation}
  \label{Eq:RpProjection}
\tilde{L}(\phi, P) := \int_{\mathbb{R}^p} \phi(\| x \|) \, dP(x) - \int_0^{\infty} r^{p-1} e^{\phi(r)} \, dr + 1,
\end{equation}
and write $\tilde{\phi}^*(P) := \argmax_{\phi \in \Phi} \tilde{L}(\phi,P)$.  Since $\tilde{L}(\cdot,P)$ is strictly concave, any maximiser of $\tilde{L}(\cdot,P)$ over $\Phi$ is unique.
%\end{definition}
%\begin{remark}
If $\tilde{L}(\phi,P) \in \mathbb{R}$, then %then $h^*(r) := r^{p-1}e^{\tilde{\phi}^*(r)}$ is a densitythat 
\[
\frac{\partial}{\partial c} \tilde{L}(\phi + c, P) = 1 - e^c \int_0^\infty r^{p-1}e^{\phi(r)} \, dr,
\]
so $\tilde{L}(\phi + c, P)$ is maximised by choosing $c = -\log \bigl(\int_0^\infty r^{p-1}e^{\phi(r)} \, dr\bigr)$.  It follows that if $\tilde{\phi}^*(P)$ exists, and $\tilde{L}(\tilde{\phi}^*,P) \in \mathbb{R}$, then we can define the \emph{spherically symmetric log-concave projection} $f^*(P) \in \mathcal{F}_p^{\mathrm{SS}}$ by $f^*(P)(x) := e^{\tilde{\phi}^*(P)(\|x\|)}$.  

In fact, in order to study $\tilde{\phi}^*(P)$, it will be convenient also to define a related projection of a one-dimensional probability distribution.  To this end, for $a \geq 0$, a probability measure $Q$ on $[a, \infty)$ and $\phi \in \Phi_{a}$, let
\begin{equation}
  \label{Eq:QProjection}
  L(\phi, Q) := \int_{[a,\infty)} \phi \, dQ - \int_{a}^\infty r^{p-1} e^{\phi(r)} \, dr + 1.
\end{equation}
Here, we incorporate the greater generality of the translation by $a$ in order to facilitate our analysis of the adaptivity properties of the spherically symmetric log-concave MLE in Section~\ref{Sec:WorstCase}.  Similarly to before, we let $\phi^*(Q) := \argmax_{\phi \in \Phi_{a}} L(\phi,Q)$, and set $h^*(Q)(r) := r^{p-1}e^{\phi^*(Q)(r)}$.  Again, any maximiser of $L(\cdot,Q)$ over $\Phi_{a}$ is unique, and if $\phi^*(Q)$ exists with $L(\phi^*, Q) \in \mathbb{R}$, then, writing $h^*(r) := r^{p-1}e^{\phi^*(Q)(r)}$, we have that $h^* \in \mathcal{H}_{a}$, so in particular, $h^*$ is a (log-concave) density.  

The following proposition gives necessary and sufficient conditions for the spherically symmetric log-concave projection to be well-defined.  We write $\mathcal{P}_p$ for the set of probability distributions on $\mathbb{R}^p$ with  $\int_{\mathbb{R}^p} \| x \| \, dP(x) < \infty$ and $P(\{0 \}) < 1$.  We let $\mathcal{Q}_{a}$ denote the class of probability measures $Q$ on $[a,\infty)$ with $\int_{a}^\infty r \, dQ(r) < \infty$ and $Q(\{a\}) < 1$, and let $\mathcal{Q} := \mathcal{Q}_0$.
\begin{proposition}
\label{Prop:ProjectionExistence}
We have 
\begin{enumerate}[(i)]
\item if $\int_{a}^\infty r \, dQ(r) = \infty$, then $L(\phi, Q) = - \infty$ for all $\phi \in \Phi_{a}$;
\item if $Q(\{a\}) = 1$, then $\sup_{\phi \in \Phi_{a}} L(\phi, Q) = \infty$;
\item if $Q \in \mathcal{Q}_{a}$, then $\sup_{\phi \in \Phi_{a}} L(\phi, Q) \in \mathbb{R}$ and $\phi_{a}^*$ is a well-defined map from $\mathcal{Q}_{a}$ to $\Phi_{a}$;
\item if $P$ is a probability measure on $\mathbb{R}^p$ and we define a probability measure $Q$ on $[0,\infty)$ by $Q\bigl([0,r)\bigr) := P(\{x:\|x\| < r\})$, then $\tilde{L}(\phi,P) = L(\phi,Q)$ for every $\phi \in \Phi$.  In particular, if $P \in \mathcal{P}_p$, then $Q \in \mathcal{Q}$ and $\tilde{\phi}^*(P) = \phi^*(Q)$.
\end{enumerate}
\end{proposition}
\begin{remark}
From Proposition~\ref{Prop:ProjectionExistence}(iv), we see that the conditions on $P$ required for the spherically symmetric log-concave projection to exist, namely $\int_{\mathbb{R}^p} \| x \| \, dP(x) < \infty$ and $P(\{0 \}) < 1$, are weaker than the corresponding conditions for the ordinary log-concave projection to exist, namely $\int_{\mathbb{R}^p} \| x \| \, dP(x) < \infty$ and $P(H) < 1$ for every hyperplane $H$; cf.~\citet[][Theorem~2.2]{dumbgen2011approximation}.
\end{remark}
The following constitute basic properties of $\phi^*_{a}(Q)$.
\begin{proposition}
  \label{Prop:ProjectionBasicProperties}
Let $Q \in \mathcal{Q}_{a}$, let $\phi^* := \phi_{a}^*(Q)$ and let $h^*(r) := r^{p-1}e^{\phi^*(r)}$.  
  \begin{enumerate}[(i)]
  \item The projection $\phi_{a}^*$ is scale equivariant in the sense that if $\alpha > 0$, and $Q_\alpha \in \mathcal{Q}_{\alpha a}$ is defined by $Q_{\alpha}( [\alpha a, r)) := Q( [a, r/\alpha) )$ for all $r \in [\alpha a, \infty)$, then $\phi^*_{\alpha a}(Q_\alpha)(r) = \phi^*_{a}(Q)(r/\alpha) - p\log \alpha$.
  \item Let $\Delta : [a, \infty) \rightarrow [-\infty, \infty)$ be a function satisfying the property that there exists $t > 0$ such that
    $r \mapsto \phi^*(r) + t \Delta(r) \in \Phi_{a}$. Then
    \[
      \int_{[a,\infty)} \Delta \, dQ \leq \int_{a}^\infty \Delta(r) h^*(r) \, dr.
    \]
  \item $\int_{a}^\infty r h^*(r) \, dr \leq \int_{[a,\infty)} r \, dQ(r)$.
  \item For any $h_0 \in \mathcal{H}_a$, we have $\int_a^\infty h^* \log (h^*/h_0) \leq \int_a^\infty \log (h^*/h_0) \, dQ$.
  \end{enumerate}
\end{proposition}
\begin{remark}
Let $X_1,\ldots,X_n \stackrel{\mathrm{iid}}{\sim} P \in \mathcal{P}_p$ with empirical distribution $\mathbb{P}_n$, and for a measurable set $A \subseteq \mathbb{R}$, let $Q(A) := P(\{x:\|x\| \in A\})$.  Let $Z_i := \|X_i\|$ for $i=1,\ldots,n$ and let $\mathbb{Q}_n$ denote the empirical distribution of $\mathbb{Q}_n$.  Writing $\hat{f}_n := f^*(\mathbb{P}_n)$, $f^* := f^*(P)$, $\hat{h}_n := h^*(\mathbb{Q}_n)$ and $h^* := h^*(Q)$, we have by Proposition~\ref{Prop:ProjectionBasicProperties}(iv) that
\begin{align*}
d_X^2(\hat{f}_n,f^*) = \int_{\mathbb{R}^p} \log \frac{\hat{f}_n}{f^*} \, d\mathbb{P}_n = \int_0^\infty \log \frac{\hat{h}_n}{h^*} \, d\mathbb{Q}_n \geq \int_0^\infty \hat{h}_n \log \frac{\hat{h}_n}{h^*} &= \int_0^\infty \hat{f}_n \log \frac{\hat{f}_n}{f^*} \\
&= d_{\mathrm{KL}}^2(\hat{f}_n,f^*).
\end{align*}
\end{remark}
As a final basic property of our projections, we establish continuity with respect to the Wasserstein distance.  Recall that if $P,P'$ are probability measures on a Euclidean space with finite first moments, then the Wasserstein distance between $P$ and $P'$ is defined as
\[
d_\mathrm{W}(P,P') := \inf_{(X,X') \sim (P,P')} \mathbb{E}\|X-X'\|,
\]
where the infimum is taken over all pairs of random vectors $X,X'$, defined on the same probability space, with $X \sim P$ and $Y \sim Q$.  We also recall that if $P$ has a finite first moment, then $d_\mathrm{W}(P_n,P) \rightarrow 0$ if and only if both $P_n \stackrel{d}{\rightarrow} P$ and $\int \|x\| \, dP_n(x) \rightarrow \int \|x\| \, dP(x)$.
\begin{proposition}
\label{Prop:Continuity}
Suppose that $P \in \mathcal{P}_p$ and $d_\mathrm{W}(P_n,P) \rightarrow 0$.  Then $\sup_{\phi \in \Phi} L(\phi,P_n) \rightarrow \sup_{\phi \in \Phi} L(\phi,P)$, $f^*(P_n)$ is well-defined for large $n$ and 
\begin{equation}
\label{Eq:TVConv}
\int_{\mathbb{R}^p} |f^*(P_n)(x) - f^*(P)(x)| \, dx \rightarrow 0.
\end{equation}
Moreover, given any compact set $K$ contained in the interior of the support of $P$, we have $\sup_{x \in K} |f^*(P_n)(x) - f^*(P)(x)| \rightarrow 0$. 
\end{proposition}
\begin{remark}
Proposition~\ref{Prop:Continuity} immediately yields a consistency (and robustness to misspecification) result for the spherically symmetric log-concave MLE.  In particular, suppose that  $X_1,\ldots,X_n \stackrel{\mathrm{iid}}{\sim} P \in \mathcal{P}_p$ with empirical distribution $\mathbb{P}_n$, and let $\hat{f}_n := f^*(\mathbb{P}_n)$, $f^* := f^*(P)$.  Then, by the strong law of large numbers and Varadarajan's theorem \citep[e.g.][Theorem~11.4.1]{dudley2002real}, we have $d_{\mathrm{W}}(\mathbb{P}_n,P) \stackrel{\mathrm{a.s.}}{\rightarrow} 0$, so
\[
\int_{\mathbb{R}^p} |\hat{f}_n - f^*| \stackrel{\mathrm{a.s.}}{\rightarrow} 0.
\]
\end{remark}
\begin{remark}
In fact, the conclusion of Proposition~\ref{Prop:Continuity} also holds for stronger norms than the total variation norm.  In particular, taking $a_0 > 0$ and $b_0 \in \mathbb{R}$ such that $f^*(P)(x) \leq e^{-a_0\|x\|+b_0}$ for all $x \in \mathbb{R}^p$, we have by, e.g., \citet[][Proposition~2]{cule2010theoretical} that for every $a < a_0$,
\[
\int_{\mathbb{R}^p} e^{a\|x\|}|f^*(P_n)(x) - f^*(P)(x)| \, dx \rightarrow 0.
\]
\end{remark}

\section{Moment preservation properties}
\label{Sec:Moments}

The aim of this section is to elucidate the way in which the first two moments of the empirical distribution $\mathbb{Q}_n$ of a set of $n$ data points in $[0,\infty)$ change under the projection $h_a^*$.  These results enable us to show that if the data are drawn independently from a common distribution on $[0,\infty)$, then with high probability, the first two moments of $\hat{h}_n := h_a^*(\mathbb{Q}_n)$ are close to their population analogues.

For $f \in \mathcal{F}_1$, define $\mu_f := \int_{-\infty}^\infty rf(r) \, dr$ and 
\[
\sigma_f^2 := \int_{-\infty}^\infty r^2 f(r) \, dr - \mu_f^2.
\]
Our first proposition concerns bounds on $\mu_{\hat{h}_n}$:
\begin{proposition}
  \label{Prop:MLEMeanPreservation}
Fix $a \geq 0$, and suppose that $Z_1,\ldots,Z_n$ are real numbers in the interval $[a,\infty)$ that are not all equal to $a$.  Let $\mathbb{Q}_n$ be the empirical distribution corresponding to $Z_1,\ldots,Z_n$.  Let $\hat{h}_n := h_{a}^*(\mathbb{Q}_n)$. Then, writing $\bar{Z} := n^{-1}\sum_{i=1}^n Z_i$, there exists $s \in (0, \infty)$ such that
\[
\bar{Z}  - \min \biggl\{ \frac{\bar{Z} - a}{\rho(s)}, \frac{s}{\rho(s)} \biggr\}  \leq \mu_{\hat{h}_n} \leq \bar{Z}.
\]
%Now suppose that $h_0 \in \mathcal{H}_{a}$ with $\sigma_{h_0} = 1$, and that $Z_1,\ldots,Z_n \stackrel{\mathrm{iid}}{\sim} h_0$.  Then there exists a universal constant $C_\mu > 0$ such that 
%\[
%\mathbb{P}\bigl(| \mu_{\hat{h}_n}- \mu_{h_0} | > C_\mu\bigr) \leq \frac{1}{n}.
%\]
\end{proposition}
We now study bounds for $\sigma_{\hat{h}_n}$, and their consequences for $\sup_{r \geq a} \log \hat{h}_n(r)$.  
\begin{proposition}
  \label{Prop:MLEVarPreservation}
  Let $a \geq 0$ and let $\mathbb{Q}_n$ be an empirical distribution of $n$ points in $[a,\infty)$ that are not all equal to $a$.  Let $\delta_{\mathbb{Q}_n} > 0$ be such that $\mathbb{Q}_n(A) \leq 1/2$ for every interval $A$ of length at most $\delta_{\mathbb{Q}_n}$.  Writing $\hat{h}_n := h_{a}^*(\mathbb{Q}_n)$, let $\ell_{\hat{h}_n} \leq \int_{a}^\infty \log \hat{h}_n \, d\mathbb{Q}_n$.  Then there exists a universal constant $C_{\sigma} > 0$ such that
  \[
 \frac{1}{C_\sigma} \min(\delta_{\mathbb{Q}_n}^2, e^{\ell_{\hat{h}_n}},1) \leq \sigma_{\hat{h}_n} \leq C_\sigma e^{-\ell_{\hat{h}_n}}.
  \]
Moreover,
  \begin{equation}
\label{Eq:Sup}
    \sup_{r \geq a} \log \hat{h}_n(r) \leq  \max \biggl\{- 2 \log \Bigl( \frac{\delta_{\mathbb{Q}_n} }{6} \Bigr),\,
      - 2 \ell_{\hat{h}_n} \, , \, 3\biggr\}.
  \end{equation}
\end{proposition}
We are now in a position to argue that, with high probability, $\hat{h}_n$ belongs to a subclass of $\mathcal{H}_a$ with restricted first two moments.  These moment restrictions are important for enabling us to obtain the bracketing entropy bounds that drive the rates of convergence of the spherically symmetric log-concave MLE.  For $C_\mu,C_\sigma > 0$, $a_0 \geq 0$ and $h_0 \in \mathcal{H}_{a_0}$ with $\sigma_{h_0}=1$, let
\[
\mathcal{H}(h_0,C_\mu,C_\sigma) := \biggl\{h \in \mathcal{H}_{a_0} : |\mu_h - \mu_{h_0}| \leq C_\mu \, , \, \frac{1}{C_\sigma} \leq \sigma_h \leq C_{\sigma}\biggr\}.
\]
\begin{proposition}
  \label{Prop:MLEMeanVarPreservation}
Let $a_0 \geq 0$, fix a density $h_0 \in \mathcal{H}_{a_0}$ with $\sigma_{h_0}=1$, and suppose that $Z_1,\ldots,Z_n \stackrel{\mathrm{iid}}{\sim} h_0$, with empirical distribution $\mathbb{Q}_n$.  Writing $\hat{h}_n := h_{a_0}^*(\mathbb{Q}_n)$, there exist universal constants $C_\mu, C_\sigma, C > 0$ such that
  \begin{align*}
   \mathbb{P}\bigl(\hat{h}_n \notin \mathcal{H}(h_0,C_\mu,C_\sigma)\bigr) \leq \frac{C}{n}.
  \end{align*}
\end{proposition}

\section{Worst case and adaptive risk bounds}
\label{Sec:WorstCase}

Let $f_0 \in \mathcal{F}_p^{\mathrm{SS}}$, and suppose that $X_1,\ldots, X_n \stackrel{\mathrm{iid}}{\sim} f_0$ with empirical distribution $\mathbb{P}_n$. Let $\hat{f}_n := f^*(\mathbb{P}_n)$ be the spherically symmetric log-concave MLE.  Our first main result below provides a worst case risk bound for $\hat{f}_n$ as an estimator of $f_0$ in terms of the $d_X^2$ measure defined in the introduction.
\begin{theorem}
\label{Thm:WorstCase}
Let $X_1,\ldots, X_n \stackrel{\mathrm{iid}}{\sim} f_0 \in \mathcal{F}_p^{\mathrm{SS}}$ with empirical distribution $\mathbb{P}_n$. Let $\hat{f}_n := f^*(\mathbb{P}_n)$ be the spherically symmetric log-concave MLE.  There exists a universal constant $C > 0$ such that 
  \[
\sup_{f_0 \in \mathcal{F}_p^{\mathrm{SS}}} \mathbb{E}d^2_X(\hat{f}_n,f_0) \leq \frac{C}{n^{4/5}}.
  \]
\end{theorem}
As mentioned in the introduction, the attractive aspect of this bound is that it does not depend on $p$.  The proof relies heavily on the special moment preservation properties of the spherically symmetric log-concave MLE.

We now turn to the adaptation properties of $\hat{f}_n$.  For $k \in \mathbb{N}$ and $a > 0$, we say $\phi \in \Phi_a$ is \emph{$k$-affine}, and write $\phi \in \Phi_a^{(k)}$, if there exist $r_0 \in (a,\infty]$ and a partition $I_1,\ldots,I_{k}$ of $[a,r_0)$ into intervals such that $\phi$ is affine on each $I_j$ for $j=1,\ldots,k$, and $\phi(r) = -\infty$ for $r > r_0$.  Define $\mathcal{H}_a^{(k)} := \bigl\{ h \in \mathcal{H}_a\,:\, h(r) = r^{p-1}e^{\phi(r)} \textrm{ for some } \phi \in \Phi_a^{(k)}\bigr\}$.  Again, we write $\Phi^{(k)} := \Phi_0^{(k)}$ and $\mathcal{H}^{(k)} := \mathcal{H}_0^{(k)}$.
\begin{theorem}
\label{Thm:Adaptation}
Let $f_0 \in \mathcal{F}_p^{\mathrm{SS}}$ be given by $f_0(x) = e^{\phi_0(\|x\|)}$ and let $X_1,\ldots, X_n \stackrel{\mathrm{iid}}{\sim} f_0$ with empirical distribution $\mathbb{P}_n$. Let $\hat{f}_n := f^*(\mathbb{P}_n)$ be the spherically symmetric log-concave MLE.  Define $h_0 \in \mathcal{H}$ by $h_0(r) := r^{p-1}e^{\phi_0(r)}$.  Then, writing $\nu_k^2 := 2 \wedge \inf_{h \in \mathcal{H}^{(k)}} d_{\mathrm{KL}}^2(h_0,h)$, there exists a universal constant $C > 0$ such that
  \[
    \mathbb{E}d_X^2(\hat{f}_n,f_0) \leq C \min_{k =1,\ldots,n} \biggl(\frac{k^{4/5}\nu_k^{2/5}}{n^{4/5}} \log \frac{en}{k\nu_k} + \frac{k}{n}\log^{5/4}\frac{en}{k}\biggr).
  \]
\end{theorem}
\begin{remark}
Taking the universal constant $C > 0$ from the conclusion of Theorem~\ref{Thm:Adaptation} and setting $C_* := \max\{(3C/2)^{5/4},1\}$, we see that if $\nu_k^2 \geq C_* \frac{k}{n} \log^{5/4} \bigl(\frac{en}{k}\bigr)$ and $k \in \{1,\ldots,n\}$, then
\[
C\frac{k^{4/5}\nu_k^{2/5}}{n^{4/5}}\log\frac{en}{k\nu_k} \leq C\nu_k^2\biggl\{\frac{1}{C_*^{4/5}\log(en/k)}\log\biggl(\frac{en^{3/2}}{C_*^{1/2}k^{3/2} \log^{5/8}(en/k)}\biggr)\biggr\} \leq \nu_k^2.
\]
On the other hand, if $\nu_k^2 \leq C_* \frac{k}{n} \log^{5/4} \bigl(\frac{en}{k}\bigr)$ and $k \in \{1,\ldots,n\}$, then
\[
\frac{k^{4/5}\nu_k^{2/5}}{n^{4/5}}\log\frac{en}{k\nu_k} \lesssim \frac{k}{n}\log^{1/4}\Bigl(\frac{en}{k}\Bigr)\log\biggl(\frac{en^{3/2}}{k^{3/2}\log^{5/8}(en/k)}\biggr) \lesssim \frac{k}{n}\log^{5/4} \frac{en}{k}.
\]
It follows that Theorem~\ref{Thm:Adaptation} implies the following sharp oracle inequality: there exists a universal constant $C > 0$ such that  
\[
 \mathbb{E}_{h_0} d^2_X(\hat{f}_n,f_0) \leq \min_{k = 1,\ldots,n}\biggl(\nu_k^2 + C\frac{k}{n}\log^{5/4} \frac{en}{k}\biggr).
\]
\end{remark}
The proof of Theorem~\ref{Thm:Adaptation} proceeds by first considering the case $k=1$, described in Proposition~\ref{Prop:AdaptiveRateAffine} below, for which we obtain a slightly different approximation error term. 
\begin{proposition}
  \label{Prop:AdaptiveRateAffine}
  Let $a \in [0, \infty)$ and suppose that $Z_1,\ldots,Z_n \stackrel{\mathrm{iid}}{\sim} h_0$ for some $h_0 \in \mathcal{H}_{a}$ with empirical distribution function $\mathbb{Q}_n$, and let $\hat{h}_n := h_{a}^*(\mathbb{Q}_n)$.  Set $\nu := \inf \{  d_{\mathrm{H}}(h_0, h) : h \in \mathcal{H}_{a}^{(1)},\, h_0 \ll  h\}$.  Then there exists a universal constant $C > 0$ such that 
  \[
    \mathbb{E}_{h_0} d^2_X(\hat{h}_n,h_0) \leq  C\biggl(\frac{\nu^{2/5}}{n^{4/5}}\log\frac{en}{\nu} + \frac{1}{n}\log^{5/4}(en)\biggr).
  \]
\end{proposition}
\begin{remark}
  Since $2^{1/2} \leq n e^{-3/2}$ for $n \geq 8$ and the function $x \mapsto x^{2/5} \log(en /x)$ is increasing for $x \leq n e^{-3/2}$, Proposition~\ref{Prop:AdaptiveRateAffine} remains true if we redefine $\nu^2 := 2 \wedge \inf_{ h \in \mathcal{H}_a^{(1)}} d^2_{\mathrm{KL}}(h_0, h) $. Hence, the conclusion of Proposition~\ref{Prop:AdaptiveRateAffine} is stronger than that obtained by specialising Theorem~\ref{Thm:Adaptation} to the case $k=1$.
\end{remark}

\section{Algorithm}
\label{Sec:Algorithm}

In this section, we assume we are given data $X_1, \ldots, X_n$ in $\mathbb{R}^p$ and let $Z_i := \| X_i \|$ for $i=1,\ldots,n$.  We let $\mathbb{P}_n$ and $\mathbb{Q}_n$ denote the empirical distributions of $X_1,\ldots,X_n$ and $Z_1,\ldots,Z_n$ respectively.  Proposition~\ref{Prop:ProjectionExistence} above shows that, provided at least one of $Z_1,\ldots,Z_n$ is non-zero, the function $\hat{\phi}_n := \phi^*(\mathbb{Q}_n)$ is well-defined, and we can then set $\hat{f}_n(x) := e^{\hat{\phi}_n(\|x\|)}$.  Our aim is therefore to provide an algorithm for computing $\hat{\phi}_n$.

Let $\bar{\Phi}$ denote the set of $\phi \in \Phi$ with the property that $\phi$ is constant on the interval $[0,Z_{(1)}]$ and affine on the intervals $[Z_{(i-1)},Z_{(i)}]$ for $i=2,\ldots,n$, with $\phi(r) = -\infty$ for $r > Z_{(n)}$.  Observe that if we fix $\phi \in \Phi$, and $\bar{\phi} \in \bar{\Phi}$ be such that $\bar{\phi}(0) = \phi(0)$ and $\bar{\phi}(Z_i) = \phi(Z_i)$ for all $i=1,\ldots,n$. Then by concavity of $\phi$, we have $\phi(r) \geq \bar{\phi}(r)$ for all $r \in [0, \infty)$.  Hence
\begin{align}
\label{Eq:phibar}
L(\phi, \mathbb{Q}_n) = \frac{1}{n}\sum_{i=1}^n \phi(Z_i) - \int_0^\infty r^{p-1} e^{\phi(r)} \, dr +1 &\leq
    \frac{1}{n}\sum_{i=1}^n \bar{\phi}(Z_i) - \int_0^\infty  r^{p-1} e^{\bar{\phi}(r)} \, dr +1 \nonumber \\
&= L(\bar{\phi},\mathbb{Q}_n).
\end{align}
We now assume for simplicity of exposition that $Z_1,\ldots,Z_n$ are distinct.  The more general case can be treated similarly by assigning appropriate weights to duplicated points.  Any $\phi \in \bar{\Phi}$ can be identified with $\mathbf{\phi} = (\phi_1,\ldots,\phi_n)^\top \in \mathbb{R}^n$ given by $\phi_i := \phi(Z_i)$ for $i=1,\ldots,n$.  For $i=1,\ldots,n-1$, let $\delta_i := Z_{(i+1)} - Z_{(i)}$.  Define $v_1 = (v_{1,j})_{j=1}^n \in \mathbb{R}^n$ to have two non-zero entries, namely $v_{1,1} := -1$, $v_{1,2} := 1$.  Further, for $i=2,\ldots,n-1$, let $v_i = (v_{i,j})_{j=1}^n \in \mathbb{R}^n$ have three non-zero entries, namely
\[
v_{i,i-1} := \frac{1}{\delta_{i-1}}, \quad v_{i,i} := -\frac{1}{\delta_i} - \frac{1}{\delta_{i-1}} \quad \text{and} \quad v_{i,i+1} := \frac{1}{\delta_i}.
\]
Finally, let $\bar{\mathbf{\Phi}}_n := \bigl\{\phi \in \mathbb{R}^n: v_i^{\top} \phi \leq 0 \text{ for } i=1,\ldots, n-1\bigr\}$.  By~\eqref{Eq:phibar}, we see that it suffices to compute $\phi^* = (\phi_1^*,\ldots,\phi_n^*)^\top \in \argmax_{\phi \in \bar{\mathbf{\Phi}}_n} F(\phi)$, where
\begin{equation}
\label{Eq:Optimization}
F(\phi) := \frac{1}{n} \sum_{i=1}^n \phi_i - \frac{1}{p}Z_{(1)}^p - \sum_{i=1}^{n-1} \int_{Z_{(i)}}^{Z_{(i+1)}}
   r^{p-1} \exp \biggl( \frac{Z_{(i+1)} - r}{\delta_i} \phi_i +
    \frac{r - Z_{(i)}}{\delta_i} \phi_{i+1} \biggr) \,dr + 1. 
\end{equation}
This is a finite-dimensional convex optimisation problem with linear inequality constraints.  We propose an active set algorithm for the optimisation of~\eqref{Eq:Optimization}, a variant of the algorithm used in \cite{dumbgen2007active} to compute the ordinary univariate log-concave MLE. For $\phi \in \bar{\Phi}_n$, we define $A(\phi) := \bigl\{i \in \{1,\ldots,n-1\}:v_i^\top\phi = 0 \bigr\}$ to be the set of `active' constraints.  Note that this is the complement in $\{1,\ldots,n\}$ of the set of `knots' of $\phi$.
%$\{1 \} \cup \bigl \{ i \in \{2,...,n-1\} : (\phi_{i} - \phi_{i-1})/(Z_{(i)} - Z_{(i-1)}) = (\phi_{i+1} - \phi_{i})/(Z_{(i+1)} - Z_{(i)}) \bigr \}$; otherwise, we define $A(\phi) := \bigl \{ i \in \{2,...,n-1\} : (\phi_{i} - \phi_{i-1})/(Z_{(i)} - Z_{(i-1)}) = (\phi_{i+1} - \phi_{i})/(Z_{(i+1)} - Z_{(i)}) \bigr \}$.
Given a set $A \subseteq \{1,\ldots,n-1\}$, we define $V(A) := \bigl \{ \phi \in \mathbb{R}^n : v_i^\top\phi = 0, \, \forall \, i \in A\bigr\}$, and 
%\phi_1 = \phi_2 \textrm{ if $1 \in A$},\, (v_{i} - v_{i-1})/(Z_{(i)} - Z_{(i-1)}) = (v_{i+1} - v_{i})/(Z_{(i+1)} - Z_{(i)}), \forall i \in A \bigr \}.
%\]
%We note that $V(A)$ need not be a subset of $\bar{\Phi}_n$. 
% Finally, we define
  \begin{equation}
    \label{Eq:OptimizationFixedA}
    V^*(A) := \argmax_{\phi \in V(A)} F(\phi).
  \end{equation}
Here, the maximiser is unique because $F(\cdot )$ is strictly concave on $\mathbb{R}^n$ with $F(\phi) \rightarrow -\infty$ as $\|\phi\| \rightarrow \infty$. It is convenient to define, for $i=1,\ldots,n-1$, vectors $b_i = (b_{i,j})_{j=1}^n \in \mathbb{R}^n$ by
%\begin{align*}
%  b_i := - \biggl(0, ..., \underbrace{\Delta_i}_{\textrm{$(i+1)$-th position}}, \Delta_i + \Delta_{i+1}, ..., \sum_{j=i}^{n-1} \Delta_j \biggr) .
%\end{align*}
\[
b_{i,j} := - \sum_{k=i}^{j-1} \delta_k,
\]
where, as usual, we interpret an empty sum as $0$, and also define $b_n := \mathbf{1}_n \in \mathbb{R}^n$, the all-one vector.  It follows from this definition that $b_i^{\top} v_i = -1$ for $i\in\{1,\ldots,n-1\}$ and $ b_i^\top v_j = 0$ for all $i \in \{1,\ldots,n\}$ and $j \in \{1,\ldots,n-1\}$ with $i \neq j$.  Finally, given $\phi \in \bar{\mathbf{\Phi}}_n$ and $\phi' \in \mathbb{R}^n$, we define 
\[
t(\phi, \phi') := \max\biggl\{\frac{v_i^\top \phi'}{ v_i^\top(\phi' - \phi)}:i \in \{1,\ldots,n-1\} \setminus A(\phi)\, , \, v_i^\top\phi' > 0\biggr\}.
\] 
We are now in a position to present the full algorithm; see Algorithm~\ref{Alg:ActiveSet}.  It is guaranteed to terminate in finitely many steps with the exact solution.   
%The algorithm is guaranteed to output a solution to optimization~\eqref{Eq:Optimization} in finite iterations, as shown in \cite[][Section 3]{dumbgen2007active}.

\begin{algorithm}
\caption{Computing the spherically symmetric log-concave MLE}
\label{Alg:ActiveSet}
\textbf{Input}: $X_1,\ldots, X_n \in \mathbb{R}^p$. 
\textbf{Output}: $\phi \in \bar{\Phi}_n$.

\begin{algorithmic}[1]
\State $Z_i \leftarrow \| X_i\|$ for all $i=1,\ldots,n$.
\State $A \leftarrow \{1,\ldots,n-1\}$.
\State $\phi \leftarrow V^*(A)$.
\While{ $\max_{i=1,\ldots,n} b_i^\top \nabla F(\phi) \geq 0$ }
 \State $i^* \leftarrow \argmax_{i=1,\ldots,n} b_i^\top \nabla F(\phi) $
\State $\phi' \leftarrow V^*(A \setminus \{i^*\})$
 \While{ $\phi' \notin \bar{\Phi}_n$}
    \State $\phi \leftarrow t(\phi, \phi') \phi + \{1 - t(\phi, \phi')\} \phi'$ 
    \State $A \leftarrow A(\phi)$
    \State $\phi' \leftarrow V^*(A)$
 \EndWhile
 \State $\phi \leftarrow \phi'$
 \State $A \leftarrow A(\phi)$
\EndWhile   
\end{algorithmic}
\end{algorithm}

%\subsubsection{Optimization over an active set}

We complete this section by providing further detail on how to solve the optimisation problem in~\eqref{Eq:OptimizationFixedA}. Given the active set $A \subseteq \{1,\ldots,n-1\}$, let us define $I := \{1,\ldots,n\} \setminus A$. We index the elements of $I$ by $i_1 < \ldots < i_T$ where $T := |I|$.  Given $v \in \mathbb{R}^{(n-1) \times n}$, we also write $v_A$ for the matrix in $\mathbb{R}^{|A| \times n}$ obtained by extracting the rows of $v$ with indices in $A$.  Observe that the set $\{ \phi = (\phi_1,\ldots,\phi_n)^\top \in \mathbb{R}^n \,:\, v_A \phi = 0 \}$ is the subspace of $\mathbb{R}^n$ where for $j < i_1$, we have $\phi_j = \phi_{i_1}$, and for $j \in \{i_t+1,\ldots,i_{t+1}-1\}$, we have 
\[
\phi_j = \frac{Z_{(i_{t+1})} - Z_{(j)}}{Z_{(i_{t+1})} - Z_{(i_t)}} \phi_{i_t} +
         \frac{Z_{(j)} - Z_{(i_t)}}{Z_{(i_{t+1})} - Z_{(i_t)}} \phi_{i_{t+1}}.
\]
It follows  we can solve the optimisation problem~\eqref{Eq:OptimizationFixedA} by solving instead an unconstrained optimisation over $T$ variables, i.e.~by computing
\begin{align*}
\max_{\phi_I \in \mathbb{R}^T} & \frac{1}{n} \biggl( i_1 \phi_{i_1} + 
          \sum_{t=1}^{T-1} \sum_{j=i_t + 1}^{i_{t+1}}   
         \frac{Z_{(i_{t+1})} - Z_{(j)}}{Z_{(i_{t+1})} - Z_{(i_t)}} \phi_{i_t} +
         \frac{Z_{(j)} - Z_{(i_t)}}{Z_{(i_{t+1})} - Z_{(i_t)}} \phi_{i_{t+1}} \biggr) \\
     &  - \frac{1}{p}\exp(\phi_{i_1}) Z_{(i_1)}^p - 
       \sum_{t=1}^{T-1} \int_{Z_{(i_t)}}^{Z_{(i_{t+1})}} r^{p-1} \exp
       \biggl( \frac{Z_{(i_{t+1})} -r}{Z_{(i_{t+1})} -Z_{(i_t)}} \phi_i + \frac{r -Z_{(i_t)}}{Z_{(i_{t+1})} -Z_{(i_t)}} \phi_{i_{t+1}}\biggr) \, dr.
\end{align*}
We solve this latter problem via Newton's method. 

\section{Empirical performance}
\label{Sec:Simulations}

In this section, we compare the performance of the spherically symmetric log-concave MLE against two alternative approaches. The first competitor is the spherically symmetric estimator $\tilde{f}_n$ defined in the introduction, based on computing the univariate log-concave MLE of the density of the norms of the data.  The second competitor is the spherically symmetric estimator $\tilde{f}^{\mathrm{ker}}_n$, which is similar but estimates the density of the norms of the data via kernel smoothing.
%estimates the density of the norms of the data via kernel smoothing, and then sets $f^{\mathrm{ker}}_n(x) = h^{\mathrm{ker}}_n(\|x\|)/(c_p \|x\|^{p-1} )$. 

In the first experiment, we generate data from an isotropic Gaussian distribution $f_0$ and measure the error in Kullback--Leibler divergence $d_{\mathrm{KL}}(\cdot, f_0)$. For each setting of $n$ and $p$, we repeat the simulation $100$ times and report the average error. The results are shown in Figure~\ref{Fig:Gaussian_p} and~\ref{Fig:Gaussian_n}. In Figure~\ref{Fig:Gaussian_n}, we plot $n^{-4/5}$ on the $x$-axis for various sample sizes to demonstrate the scaling proved in Theorem~\ref{Thm:WorstCase}.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{0.45\linewidth}
    \centering
    \includegraphics[scale=0.36]{figs/gaussian_p.pdf}\vspace{-0.1in}
    \caption{\label{Fig:Gaussian_p} Gaussian; $p$ varying; $n=3000$.}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\linewidth}
    \centering
    \includegraphics[scale=0.36]{figs/gaussian_n.pdf}\vspace{-0.1in}
    \caption{\label{Fig:Gaussian_n} Gaussian; $n$ varying; $p=1000$.}
  \end{subfigure}
%\vspace{0.2in}
  \begin{subfigure}[b]{0.45\linewidth}
    \centering
    \includegraphics[scale=0.36]{figs/uniform_p.pdf}\vspace{-0.1in}
    \caption{\label{Fig:Uniform_p} Uniform; $p$ varying; $n=3000$.}
  \end{subfigure}
%\vspace{0.2in}
  \begin{subfigure}[b]{0.45\linewidth}
    \centering
    \includegraphics[scale=0.36]{figs/uniform_n.pdf}\vspace{-0.1in}
    \caption{\label{Fig:Uniform_n} Uniform; $n$ varying; $p=1000$.}
  \end{subfigure}
%\vspace{0.15in}
  \begin{subfigure}[b]{0.45\linewidth}
    \centering
    \includegraphics[scale=0.36]{figs/exponential_p.pdf}\vspace{-0.1in}
    \caption{\label{Fig:Exponential_n} Exponential; $p$ varying; $n=3000$.}
  \end{subfigure}
%\vspace{0.15in}
  \begin{subfigure}[b]{0.45\linewidth}
     \centering
     \includegraphics[scale=0.36]{figs/exponential_n.pdf}\vspace{-0.1in}
     \caption{\label{Fig:Exponential_p} Exponential; $n$ varying; $p=1000$.}
  \end{subfigure}
  \caption{Simulation studies.}
\end{figure}
In the second experiment, we generate data whose true density is uniform on a Euclidean ball, i.e., $f_0(x) = p^{-p} c_p^{-1} \mathbbm{1}_{\{\|x \| \leq p\}}$. We measure the error in Hellinger distance because $f_0$ has bounded support and there exist $x \in \mathbb{R}^p$ where $\tilde{f}^{\mathrm{ker}}_n(x) > 0$ but $f_0(x) = 0$. We plot the results in Figures~\ref{Fig:Uniform_p} and~\ref{Fig:Uniform_n}. For the simulation study described in Figure~\ref{Fig:Uniform_n}, the error for $\tilde{f}^{\mathrm{ker}}_n$ is always above $0.01$ and hence does not appear in the figure.  In Figure~\ref{Fig:Uniform_n}, we plot $n^{-1}$ on the x-axis to demonstrate the scaling proved in Theorem~\ref{Thm:Adaptation}.

In the third experiment, the true density is $f_0(x) = \Gamma(p)^{-1} c_p^{-1} e^{- \| x \|} $ and, as with the first experiment, we measure the error in Kullback--Leibler divergence. We plot the results in Figure~\ref{Fig:Exponential_n} and~\ref{Fig:Exponential_p}. In Figure~\ref{Fig:Exponential_n}, we plot $n^{-1}$ on the $x$-axis to demonstrate the scaling proved in Theorem~\ref{Thm:Adaptation}.

These simulations confirm the theoretical findings in Section~\ref{Thm:WorstCase} and illustrate the strong finite-sample performance of the spherically symmetric log-concave MLE.

\section{Proofs of main results}

\subsection{Proofs from Section~\ref{Sec:SSLCProjections}}

\begin{proof}[Proof of Proposition~\ref{Prop:SSLC}]
Suppose that $f$ is a density on $\mathbb{R}^p$ that can be written as $f(x) = e^{\phi(\|x\|)}$ for some $\phi \in \Phi$. Then $f$ is upper semi-continuous and spherically symmetric.  Moreover, for any $t \in (0,1)$, and $x, y \in \mathbb{R}^p$, and using the facts that $\phi$ is decreasing and concave,
\begin{align*}
\log f \bigl( t x + (1 - t) y\bigr) &=  \phi\bigl( \| t x + (1- t) y \|\bigr) \geq  
    \phi\bigl( t \| x \| + (1 - t) \| y \| \bigr)\\
   &\geq t \phi( \|x\| ) + (1 - t) \phi( \| y \|) = t \log f (x) + (1-t) \log f(y).
\end{align*}
Thus, $f$ is log-concave, as required.

We now turn to the converse.  Suppose that $f \in \mathcal{F}_p^{\mathrm{SS}}$.  Then since $f$ is upper semi-continuous and spherically symmetric, we can write $f(x) = g(\|x\|)$ for some upper semi-continuous $g:[0,\infty) \rightarrow [0,\infty)$.  It remains to show that $\log g$ is decreasing and concave.  To this end, suppose for a contradiction that there exists $r > r' \geq 0$ such that $\log g(r) > \log g(r')$, and let $x \in \mathbb{R}^p$ be such that $\|x \| = r$. Then 
\[
\log f\Bigl(\frac{r'x}{r}\Bigr) = \log g(r') < \log g(r) = \frac{r+r'}{2r}\log f(x) + \frac{r-r'}{2r}\log f(-x),
\]
contradicting the log-concavity of $f$.

On the other hand, if $g$ is not log-concave, then there exist $r, r' \in [0,\infty)$ and $t \in (0,1)$ such that $\log g( t r + (1-t) r') < t \log g(r) + (1-t) \log g(r')$.  But then, for any $x \in \mathbb{R}^p$ with $\|x \| = 1$, we have 
\begin{align*}
\log f ( t rx + (1-t) r'x) &= \log g(t r + (1-t) r')  \\
   &< t \log g(r) + (1-t) \log g(r') = t \log f( rx ) + (1 - t) \log f( r'x ),
\end{align*}
again contradicting the log-concavity of $f$.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{Prop:ProjectionExistence}]
(i) Fix $\phi \in \Phi_{a}$. Observe that if $\lim_{r \rightarrow \infty} \phi(r) = c > -\infty$, then $L(\phi, Q) \leq \phi(a) - e^c \int_{a}^\infty r^{p-1} \, dr = -\infty$.  Otherwise $\lim_{r \rightarrow \infty} \phi(r) = -\infty$, and then there exist $\alpha > 0, \beta \in \mathbb{R}$ such that $ \phi(r) \leq -\alpha r+ \beta$ \citep[][Lemma~1]{cule2010theoretical}. Hence, $L( \phi, Q) \leq \int_{a}^\infty \phi \, dQ \leq - \alpha \int_{a}^\infty r \, dQ(r) + \beta = - \infty$. 

(ii) Now suppose that $Q(\{a\}) = 1$ and let $e^{\phi_n(r)} := n \mathbbm{1}_{\{ r \in [a, a + n^{-1}]\}}$. Then, 
\begin{align*}
L(\phi_n, Q) &= \log n - n \int_{a}^{a + n^{-1}} r^{p-1}\, dr + 1 \geq \log n - (a + n^{-1})^{p-1} + 1 \rightarrow \infty
\end{align*}
as $n \rightarrow \infty$.

(iii) Finally, suppose that $Q \in \mathcal{Q}_{a}$.  For $\phi(r) := -r$, we have
\[
  L(\phi, Q) = - \int_{a}^\infty r\, dQ(r) - \int_{a}^\infty r^{p-1} e^{-r} \, dr + 1 \geq - \int_{a}^\infty r\,dQ(r)  - \Gamma(p) + 1 > - \infty,
  \]
so $\sup_{\phi \in \Phi_{a}} L(\phi, Q) > -\infty$.

For $\delta, \epsilon > 0$, let $\mathcal{Q}_a(\delta,\epsilon) := \bigl\{Q \in \mathcal{Q}_a:Q\bigl((a+\delta,\infty)\bigr) \geq \epsilon\bigr\}$.  Then, since $Q(\{a\}) < 1$, we have $Q \in \mathcal{Q}_a(\delta,\epsilon)$ for some $\delta,\epsilon > 0$.
%Define $r^* := \inf \bigl\{ r \in [a, \infty): Q([a,r)) \geq \bigl(Q(\{a\})+1\bigr)/2 \bigr\}$. We have that $r^* > a$ since $Q(\{a\}) < 1$. Let $r' := (a + r^*)/2$ and $c := Q([a, r'))$ so that $c \in [0,1)$.  
We also write $M := \phi(a)$ and $M' := \phi(a+\delta)$. Then 
%\[ 
% \int_{a}^\infty \phi \, dQ = \int_{[a,a+\delta)} \phi\, dQ + \int_{[a+\delta,\infty)} \phi\, dQ \leq
%     M (1-\epsilon) + M'\epsilon = M - (M - M')\epsilon.
%\]
by the concavity of $\phi$,
\begin{equation}
\label{Eq:L}
L(\phi, Q) \leq M (1-\epsilon) + M'\epsilon - \int_{a}^{a+\delta} r^{p-1} \exp \biggl(M - \frac{r - a}{\delta}(M - M') \biggr) \, dr + 1.
\end{equation}
If $M > 0$ and $(M - M')\epsilon > 2M$, then 
\begin{align*}
L(\phi, Q) &\leq M - e^M \int_{a}^{a+\delta} r^{p-1} \exp \biggl( - \frac{2M(r - a)}{\epsilon\delta} \biggr) \, dr + 1 \\
           &\leq M - e^M \biggl(\frac{\epsilon \delta}{2M} \biggr)^p \int_0^{2M/\epsilon} \biggl( \frac{2M a}{\epsilon\delta} + s \biggr)^{p-1}e^{-s}  \, ds + 1  \\
   &\leq M - e^M \biggl( \frac{\epsilon\delta}{2M} \biggr)^p \int_0^{2M/\epsilon} s^{p-1} e^{-s}  \, ds + 1. 
\end{align*}
%Since the RHS of the above inequality goes to $-\infty$ as $M \rightarrow \infty$, we may conclude that $L(\phi, Q) < \infty$.
On the other hand, if $M > 0$ and $(M - M')\epsilon > 2M$, then from~\eqref{Eq:L} we see that $L(\phi, Q) \leq -M+1$.  We deduce that there exists $M^*> 0$, depending only on $\delta$ and $\epsilon$, such that 
%\[
%\sup_{\phi \in \Phi_{a}:\phi(a) > 2M^*} L(\phi,Q) < \sup_{\phi \in \Phi_{a}:\phi(a) \leq M^*} L(\phi,Q).
%\]
%Moreover, if $M \leq 0$ then $L(\phi,Q) \leq 0$.  It follows that
\[
  \sup_{\phi \in \Phi_{a}}L(\phi, Q) = \sup_{\phi \in \Phi_{a}:\phi(a) \leq M^*}L(\phi, Q) < \infty.
\]
The existence of $\phi^*$ then follows from the proof of Theorem 2.2 in \cite{dumbgen2011approximation}.

(iv) By the change of variable formula \citep[e.g.][Theorem~16.13]{billingsley1995probability}, we have $\int_{\mathbb{R}^p} \phi( \| x \|) \, dP(x) = \int_{[0,\infty)} \phi(r) \, dQ(r)$ for all $\phi \in \Phi$. The result then follows from~(iii), specialised to the case $a = 0$.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{Prop:ProjectionBasicProperties}]
  (i) For any $\phi \in \Phi_{a}$, we may define $\phi_\alpha \in \Phi_{\alpha a}$ by $\phi_\alpha(r) := \phi(r/\alpha) - p \log \alpha$.  The map $\phi \mapsto \phi_\alpha$ is a bijection from $\Phi_{a}$ to $\Phi_{\alpha a}$.  Let $\phi^* = \phi_{a}^*(Q)$. Then, for any $\phi \in \Phi_{a}$, we have 
  \begin{align*}
    \int_{[\alpha a,\infty)} \phi^{*}_\alpha\, dQ_\alpha - \int_{\alpha a}^\infty r^{p-1} e^{\phi^{*}_\alpha (r)} \, dr
    &= \int_{[a,\infty)} \phi^* \, dQ - p \log \alpha - \int_{a}^\infty s^{p-1}e^{\phi^{*}(s)} \, ds \\
    &\geq \int_{[a,\infty)} \phi \, dQ - p \log \alpha - \int_{a}^\infty s^{p-1} e^{\phi(s)}  \, ds \\
    &\geq   \int_{[\alpha a,\infty)} \phi_\alpha\, dQ_\alpha - \int_{\alpha a}^\infty r^{p-1}e^{\phi_\alpha(r)} \, dr.
    \end{align*}
    This establishes that $\phi^{*}_\alpha = \phi_{\alpha a}^*(Q_\alpha)$ and thus proves scale equivariance.

    (ii) For any $t > 0$, we have
\begin{equation}
\label{Eq:DomConv}
0 \geq \frac{1}{t}\bigl\{L(\phi^*+t\Delta,Q) -L(\phi^*,Q)\bigr\} = \int_{[a,\infty)} \Delta \, dQ - \frac{1}{t}\int_{a}^\infty (e^{t\Delta(r)} - 1)h^*(r) \, dr.
\end{equation}
Choose $t_0 > 0$ small enough that $\phi^* + t_0 \Delta \in \Phi_{a}$.  Since $\int_{a}^\infty r^{p-1} e^{\phi^*(r)} \, dr < \infty$, we must have $\phi^*(r) \rightarrow -\infty$ as $r \rightarrow \infty$, and hence, by reducing $t_0 > 0$ if necessary, we may assume that $\int_{a}^\infty \Delta(r) r^{p-1} e^{\phi^*(r)+t_0\Delta(r)} \, dr < \infty$.  Now, for $t \in (0,t_0]$,
\[
\frac{1}{t}(e^{t\Delta(r)} - 1) \leq \frac{1}{t_0}(e^{t_0\Delta(r)} - 1)\mathbbm{1}_{\{\Delta(r) \geq 0\}} + \Delta(r)\mathbbm{1}_{\{\Delta(r) < 0\}}.
\]
Hence, if $\int_{a}^\infty \Delta(r) h^*(r) \, dr > -\infty$, then we may apply the dominated convergence theorem to~\eqref{Eq:DomConv} to take the limit as $t \searrow 0$ and reach the desired conclusion.  On the other hand, if $\int_{a}^\infty \Delta(r) h^*(r) \, dr = -\infty$, then for every $t \in (0,t_0]$,
\begin{align*}
\frac{1}{t} \int_{a}^\infty &(e^{t\Delta(r)} - 1)h^*(r) \, dr \\
&\leq \int_{a}^\infty \biggl(\frac{1}{t_0}(e^{t_0\Delta(r)} - 1)\mathbbm{1}_{\{\Delta(r) \geq 0\}} + \Delta(r)\mathbbm{1}_{\{\Delta(r) < 0\}}\biggr)h^*(r) \, dr = -\infty.
\end{align*}
The result follows.

(iii) Letting $\Delta(r) = -r$, this is a consequence of~(ii). 

(iv) Letting $\Delta(r) =\log \bigl(h_0(r)/h^*(r)\bigr)$, this also follows from an application of~(ii).
\end{proof}

\begin{proof}[Proof of Proposition~\ref{Prop:Continuity}]
The proof is very similar to (in fact, somewhat more straightforward than) the proof of \citet[][Theorem~4.5]{dumbgen2011approximation}, so we focus on the main differences.  We first observe that if $X_n \sim P_n$ and $X \sim P$ are defined on the same probability space, then 
\[
\bigl|\mathbb{E}\|X_n\| - \mathbb{E}\|X\|\bigr| \leq \mathbb{E}\|X_n-X\| \leq d_\mathrm{W}(P_n,P).
\]
Hence, writing $Q_n$ and $Q$ for the distributions of $\|X_n\|$ and $\|X\|$ respectively, we deduce that $d_\mathrm{W}(Q_n,Q) \leq d_\mathrm{W}(P_n,P) \rightarrow 0$.  It follows that $\int_0^\infty r \, dQ_n(r) \rightarrow \int_0^\infty r \, dQ(r) < \infty$, and $\limsup_{n \rightarrow \infty} Q_n(\{0\}) \leq Q(\{0\}) < 1$, so $Q_n \in \mathcal{Q}$ for $n \geq n_0$, say.  For such $n$, we write $\phi_n^* := \phi^*(Q_n)$ and $\phi^* := \phi^*(Q)$.  Let $n_0 \leq n_1 < n_2 < \ldots$ be an arbitrary, strictly increasing sequence of positive integers.  By extracting a further subsequence if necessary, we may assume that $L(\phi_{n_k}^*,Q_{n_k}) \rightarrow \alpha \in [-\infty,\infty]$.  First note that, by considering the function $\phi(r) = -r$,
\[
\alpha \geq \lim_{k \rightarrow \infty} \int_0^\infty -r \, dQ_{n_k}(r) - (p-1)! + 1 = \int_0^\infty -r \, dQ(r) - (p-1)! + 1 > -\infty.
\]
Our next claim is that $\limsup_{k \in \mathbb{N}} \sup_{r \in [0,\infty)} \phi_{n_k}^*(r) < \infty$.  To see this, recall the definition of the classes $\mathcal{Q}(\delta,\epsilon)$ from the proof of Proposition~\ref{Prop:ProjectionExistence}, and let $\delta_0,\epsilon_0 > 0$ be such that $Q \in \mathcal{Q}(\delta_0,\epsilon_0)$.  By examining the proof of Proposition~\ref{Prop:ProjectionExistence}, it suffices to prove that $Q_{n_k} \in \mathcal{Q}(\delta_0,\epsilon_0)$ for large $k$ (with $a=0$).  But $\liminf_{k \rightarrow \infty} Q_{n_k}\bigl((\delta_0,\infty)\bigr) \geq Q\bigl((\delta_0,\infty)\bigr) \geq \epsilon_0$, which establishes the claim.

Let $r_0 := \sup\{r \in [0,\infty):Q(r) < 1\}$.  Our next claim is that $\liminf_{k \rightarrow \infty} \phi_{n_k}^*(r) > -\infty$ for all $r \in [0,r_0)$.  To see this, note by our first claim that we may assume without loss of generality that there exists $M^* \geq \max(\alpha,0)$ such that $\sup_{k \in \mathbb{N}} \sup_{r \in [0,\infty)} \phi_{n_k}^*(r) \leq M^*$.  Then, for any $r \in [0,r_0)$, 
\[
L(\phi_{n_k}^*,Q_{n_k}) \leq \phi_{n_k}^*(0)\bigl\{1 - Q_{n_k}\bigl((r,\infty)\bigr)\bigr\} + \phi_{n_k}^*(r)Q_{n_k}\bigl((r,\infty)\bigr).
\]
Since $Q\bigl((r,\infty)\bigr) > 0$, we deduce that
\begin{align*}
\liminf_{k \rightarrow \infty} \phi_{n_k}^*(r) \geq \liminf_{k \rightarrow \infty} \frac{L(\phi_{n_k}^*,Q_{n_k}) - \phi_{n_k}^*(0)\bigl\{1 - Q_{n_k}\bigl((r,\infty)\bigr)\bigr\}}{Q_{n_k}\bigl((r,\infty)\bigr)} \geq -\frac{M^* - \alpha}{Q\bigl((r,\infty)\bigr)} > -\infty,
\end{align*}
as required.  

These two claims allow us to extract a further subsequence $(\phi_{n_{k(\ell)}}^*)$ that converges in an appropriate sense to a limit $\phi^* \in \Phi$ (in particular, this convergence occurs Lebesgue almost everywhere).  It turns out that $\phi^* = \phi^*(Q)$, that $L(\phi_{n_k(\ell)}^*,Q) \rightarrow L(\phi^*,Q)$, and, writing $f_\ell^* := f^*(P_{n_{k(l)}})$ , we have $\int_{\mathbb{R}^p} |f_\ell^* - f^*| \rightarrow 0$.  The desired total variation convergence~\eqref{Eq:TVConv} follows.  See the proof of Theorem~4.5 of \citet{dumbgen2011approximation} for details.

For the final claim, note that our previous argument allows us to conclude that $f^*(P_n)$ converges to $f^*(P)$ Lebesgue almost everywhere.  The conclusion therefore follows from \citet[][Theorem~10.8]{rockafellar1997convex}.
\end{proof}

\subsection{Proofs from Section~\ref{Sec:Moments}}

\begin{proof}[Proof of Proposition~\ref{Prop:MLEMeanPreservation}]
Let $r_0 := \sup \{ r \in [a, \infty) : \hat{\phi}_n(r) = \hat{\phi}_n(a) \}$ and define $s_0 := r_0  - a$.  By~\eqref{Eq:phibar}, we have that $r_0 = Z_i$ for some $i$ because $\mathbb{Q}_n$ is an empirical distribution. We also know that the right derivative of $\hat{\phi}_n$ at $r_0$ is strictly negative.  Hence, by Proposition~\ref{Prop:ProjectionBasicProperties}(ii), applied to the functions $\Delta(r) := \pm(r-r_0)_+$, we have
\begin{align}
\label{Eq:PosPart}
\int_{a}^\infty (r-r_0)_+ \hat{h}_n(r) \, dr + r_0 &= \frac{1}{n} \sum_{i=1}^n (Z_i - r_0)_+ + r_0 \nonumber \\
                       &= \frac{1}{n} \sum_{i=1}^n (Z_i - r_0) + \frac{1}{n} \sum_{i=1}^n (Z_i - r_0)_- + r_0 \nonumber \\
                       &= \bar{Z} + \frac{1}{n} \sum_{i=1}^n (r_0 - Z_i)_+.    
\end{align}
Now, since $\hat{\phi}_n(r) = \hat{\phi}_n(a)$ for all $r \in [a,r_0]$, we have 
\[
1 \geq e^{\hat{\phi}_n(a)} \int_{a}^{r_0} r^{p-1} \, dr = e^{\hat{\phi}_n(a)}\frac{r_0^p - a^p}{p}.
\]
We deduce that
\begin{align}
\label{Eq:NegPart}
\int_{a}^\infty (r-r_0)_- \hat{h}_n(r) \, dr &= e^{\hat{\phi}_n(a)} \int_{a}^{r_0} (r_0 - r) r^{p-1} \, dr \nonumber \\
  &\leq \frac{p}{r_0^p - a^p}\biggl( \frac{r_0^{p+1}}{p} - \frac{r_0 a^p}{p} - \frac{r_0^{p+1}}{p+1} + \frac{a^{p+1}}{p+1} \biggr) \nonumber \\
  &=  r_0 - \frac{p}{p+1} \frac{r_0^{p+1} - a^{p+1}}{r_0^p - a^p} \leq \frac{s_0}{\rho(s_0)},
\end{align}
where we used Lemma~\ref{Lem:EvilInequality} to obtain the final bound.  From~\eqref{Eq:PosPart} and~\eqref{Eq:NegPart}, we find that 
\begin{equation}
\label{Eq:muhatlower}
\mu_{\hat{h}_n} = \int_{a}^\infty (r-r_0)_+ \hat{h}_n(r) \, dr - \int_{a}^\infty (r-r_0)_- \hat{h}_n(r) \, dr + r_0 \geq \bar{Z} + \frac{1}{n} \sum_{i=1}^n (r_0 - Z_i)_+ - \frac{s_0}{\rho(s_0)}.
\end{equation}
In particular, $\mu_{\hat{h}_n} \geq \bar{Z} - \frac{s_0}{\rho(s_0)}$. 
%We next show that $ \frac{s_0}{\rho} - \frac{1}{n} \sum_{i=1}^n ((s_0+a) - Z_i)_+  \leq \frac{\bar{Z} - a}{\rho}$ which implies that  $\mathbb{E}_{\hat{h}} Z \geq \bar{Z} - \frac{\bar{Z} - a}{\rho(s_0)}$

For $i=1,\ldots,n$, let $\tilde{Z}_i := Z_i$ if $Z_i \leq r_0$ and $\tilde{Z}_i := s_0 + a$ if $Z_i > r_0$.  Then $n^{-1} \sum_{i=1}^n (r_0 - Z_i)_+ = s_0 - n^{-1} \sum_{i=1}^n (\tilde{Z}_i - a) \geq 0$ and $n^{-1} \sum_{i=1}^n \tilde{Z}_i \leq \bar{Z}$.  Hence
\begin{align}
\label{Eq:muhatlower2}
  \frac{s_0}{\rho(s_0)} - \frac{1}{n} \sum_{i=1}^n &(r_0 - Z_i)_+ \nonumber \\
&=  \frac{s_0 - n^{-1} \sum_{i=1}^n (\tilde{Z}_i - a)}{\rho(s_0)}
    + \frac{n^{-1} \sum_{i=1}^n (\tilde{Z}_i - a)}{\rho(s_0)} - \biggl(s_0 - \frac{1}{n} \sum_{i=1}^n (\tilde{Z}_i - a)\biggr) \nonumber \\
  &\leq  \biggl(s_0 - \frac{1}{n} \sum_{i=1}^n (\tilde{Z}_i - a)\biggr) \biggl( \frac{1}{\rho(s_0)} - 1 \biggr)
    + \frac{\bar{Z} - a}{\rho(s_0)} \leq \frac{\bar{Z} - a}{\rho(s_0)}.
\end{align}
The second lower bound for $\mu_{\hat{h}_n}$ follows from~\eqref{Eq:muhatlower} and~\eqref{Eq:muhatlower2}.  The upper bound on $\mu_{\hat{h}_n}$ follows from Proposition~\ref{Prop:ProjectionBasicProperties}.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{Prop:MLEVarPreservation}]
We have
  \begin{align*}
 \sup_{r \geq a} \log \hat{h}_n(r) \geq \int_{a}^\infty \log \hat{h}_n \, d\mathbb{Q}_n \geq \ell_{\hat{h}_n},
  \end{align*}
so since $\hat{h}_n$ is upper semi-continuous, there exists $r_0 \geq a$ such that $\log \hat{h}_n(r_0) \geq \ell_{\hat{h}_n}$. By Lemma~\ref{Lem:VarianceSupRelation}, we then have that $\sigma_{\hat{h}_n} \leq C' e^{- \ell_{\hat{h}_n}}$ for some universal constant $C' > 0$.  Now, let $M := 3 \, \vee \, \sup_{r \geq a} \log \hat{h}_n(r)$ and let $D_t := \{t \,:\, \log \hat{h}_n(r) \geq t \}$.  By \cite[][Lemma~4.1]{dumbgen2011approximation}, we have that $\lambda( D_{-2M} ) \leq 6 M e^{-M}$.  Suppose now for contradiction that $M > \max \bigl\{ - 2 \log(\delta_{\mathbb{Q}_n}/6),\,  - 2 \ell_{\hat{h}_n} \bigr\}$.  Then
    \begin{align*}
      \lambda( D_{-2M} ) < 6 e^{- M/2} < \delta_{\mathbb{Q}_n},
    \end{align*}
so $\mathbb{Q}_n( D_{-2M}) \leq 1/2$.  Hence
 \begin{align*}
    \int_{a}^\infty \log \hat{h}_n \, d \mathbb{Q}_n \leq - 2M \{ 1 - \mathbb{Q}_n(D_{-2M}) \} + M \mathbb{Q}_n(D_{-2M})                                \leq - M/2 < \ell_{\hat{h}_n},
  \end{align*}
a contradiction.  We deduce that~\eqref{Eq:Sup} holds, so by Lemma~\ref{Lem:VarianceSupRelation}, there exists a universal constant $c' > 0 $ such that $\sigma_{\hat{h}_n} \geq c' \min( \delta_{\mathbb{Q}_n}^2, e^{2\ell_{\hat{h}_n}},1 )$.  The lower bound for $\sigma_{\hat{h}_n}$ follows, with $C_\sigma := \max(C', 1/c')$.
  \end{proof}

\begin{proof}[Proof of Proposition~\ref{Prop:MLEMeanVarPreservation}]
We may assume that $n \geq 500$.  Let $E := \{ | \bar{Z} - \mu_{h_0} | \leq 1\}$, so that $\mathbb{P}(E^c) \leq 1/n$, by Chebychev's inequality.  On the event $E$, we have  $\mu_{\hat{h}_n} \leq \bar{Z} \leq \mu_{h_0} + 1$ by Proposition~\ref{Prop:MLEMeanPreservation}.  If $s_0 \leq 1$, then $\mu_{\hat{h}_n} \geq \bar{Z} - 1 \geq \mu_{h_0} - 2$ because $\rho(s_0) \geq 1$ (Lemma~\ref{Lem:RhoProperties}(iii)). If $s_0 > 1$, then by Proposition~\ref{Prop:MLEMeanPreservation} and Lemma~\ref{Lem:RhoProperties}(iv),
\begin{align*}
  \mu_{\hat{h}_n} \geq \bar{Z} - \frac{\bar{Z} - a_0}{\rho(s_0)} &\geq \mu_{h_0} - 1 - \frac{20(\mu_{h_0} + 1 - a_0)}{\rho(s_0+1)} \\
                               & \geq \mu_{h_0} - 21 - \frac{20(\mu_{h_0} - a_0)}{\rho(s_0+1)}.
\end{align*}
Hence, if $\rho(s_0)/s_0 < 2^{-7}$, then by Lemma~\ref{Prop:UnitVarianceMeanBound}, we have $\mu_{\hat{h}_n} - \mu_{h_0} \geq -21 - 20 \times 2^{12} \geq -2^{17}$.  On the other hand, if $\rho(s_0)/s_0 \geq 2^{-7}$, then by Proposition~\ref{Prop:MLEMeanPreservation},
\[
\mu_{\hat{h}_n} - \mu_{h_0} \geq \bar{Z} - \mu_{h_0} - \frac{s_0}{\rho(s_0)} \geq -1 - 2^7 \geq -2^8.
\]
It follows that there exists a universal constant $C_\mu > 0$ such that
\[
\mathbb{P}\bigl(|\mu_{\hat{h}_n} - \mu_{h_0}| > C_\mu\bigr) \leq 1/n.
\]
To bound $\sigma_{\hat{h}_n}$, we show that, taking $\delta_{\mathbb{Q}_n} = 2^{-11}$ and $\ell_{\hat{h}_n} = -3$, there exists a universal constant $C > 0$ such that the hypotheses of Proposition~\ref{Prop:MLEVarPreservation} hold with probability at least $1 - C/n$.  To verify the first condition, note that $\| h_0 \|_\infty \leq 2^9$ \citep[][Theorem~5.14(b) and (d)]{lovasz2007geometry}. Let $H_0$ and $\mathbb{H}_n$ denote the distribution functions corresponding to $h_0$ and $\mathbb{Q}_n$ respectively.  Let $(a, b]$ be an interval of length at most $2^{-11}$.  Then
\begin{align*}
  \mathbb{Q}_n\bigl((a,b]\bigr) = \mathbb{H}_n(b) - \mathbb{H}_n(a) &= \mathbb{H}_n(b) - H_0(b) + H_0(b) - H_0(a) + \{H_0(a) - \mathbb{H}_n(a)\} \\
                  &\leq 2 \| \mathbb{H}_n - H_0 \|_\infty + \| h_0 \|_\infty (b - a) \leq 2 \| \mathbb{H}_n - H_0 \|_\infty + \frac{1}{4}.
\end{align*}
Now, by the Dvoretsky--Kiefer--Wolfowitz inequality, 
\[
\mathbb{P}\biggl(\| \mathbb{H}_n - H_0 \|_{\infty} > \sqrt{\frac{1}{2n} \log (4n)}\biggr) \leq \frac{1}{2n}.
\]
Hence, for any $n \geq 20$, we have $\mathbb{P}\bigl(\| \mathbb{H}_n - H_0 \|_{\infty} > 1/8\bigr) \leq 1/(2n)$. Thus, when $\delta_{\mathbb{Q}_n} = 2^{-11}$, 
\[
\mathbb{P}\Biggl(\sup_{\substack{a_0 \leq a < b < \infty \\ |b-a| \leq \delta_{\mathbb{Q}_n}}} \mathbb{Q}_n\bigl((a,b]\bigr) > 1/2\Biggr) \leq \frac{1}{2n}.
\]
Now, by Lemma~\ref{Lem:DifferentialEntropyBound} and \citet[][Theorem~1.1]{bobkov2011concentration}, for $n \geq 500$ and $\ell_{\hat{h}_n} = -3$,
  \begin{align*}
\mathbb{P}\biggl(\frac{1}{n} \sum_{i=1}^n \log h_0(Z_i) \leq \ell_{\hat{h}_n}\biggr) &\leq \mathbb{P}\biggl(\biggl|\frac{1}{n} \sum_{i=1}^n \log h_0(Z_i) - \int_{a_0}^\infty h_0 \log h_0\biggr| > 1\biggr) \leq 2e^{-n^{1/2}/16} \leq \frac{250}{n}.
  \end{align*}
The result follows by Proposition~\ref{Prop:MLEVarPreservation}.
\end{proof}

\subsection{Proofs from Section~\ref{Sec:WorstCase}}

\begin{proof}[Proof of Theorem~\ref{Thm:WorstCase}]
Let $h_0$ denote the density of $\|X_1\|$, so that $h_0(r) = r^{p-1} e^{\phi(r)}$ for some $\phi \in \Phi$.  For $i=1,\ldots,n$, let $Z_i := \|X_i\|$, and write $\mathbb{Q}_n$ for the empirical distribution of $Z_1,\ldots,Z_n$.  Now let $\hat{h}_n := h^*(\mathbb{Q}_n)$, so that $\hat{h}_n(r) = r^{p-1}e^{\hat{\phi}_n(r)}$, where $\hat{\phi}_n := \phi^*(\mathbb{Q}_n) = \tilde{\phi}^*(\mathbb{P}_n)$, by Proposition~\ref{Prop:ProjectionExistence}(iv).  Then $\hat{h}_n(Z_i)/h_0(Z_i) = \hat{f}_n(X_i)/f_0(X_i)$ for $i=1,\ldots,n$, so $d^2_X(\hat{f}_n,f_0) = d^2_X(\hat{h}_n,h_0)$.  By scale equivariance (Proposition~\ref{Prop:ProjectionBasicProperties}(i)), we may assume without loss of generality that $\sigma_{h_0} = 1$.  By Proposition~\ref{Prop:MLEMeanVarPreservation}, there exist universal constants $C_\mu, C_\sigma, C > 0$ such that $\mathbb{P}\bigl(\hat{h}_n \notin \mathcal{H}(h_0, C_\mu, C_{\sigma})\bigr) \leq C/n$. By Lemma~\ref{Prop:NonlocalBracketingEntropy}, there exists a universal constant $K > 0$ such that for every $\delta > 0$,
\[
  \int_0^\delta H^{1/2}_{[]}\bigl(\epsilon, \mathcal{H}(h_0, C_\mu, C_\sigma), d_{\mathrm{H}}\bigr) \, d\epsilon \leq K \delta^{3/4}.
\]
Define $\Psi(\delta) := \max(K\delta^{3/4},\delta)$, so that $\delta \mapsto \Psi(\delta)/\delta^2$ is decreasing.  By choosing $\delta_* := K_0n^{-2/5}$ for a suitably large universal constant $K_0 > 0$, we may apply \citet[][Theorem 10]{kim2016adaptationsupp} (a minor restatement of \citet[][Corollary~7.5]{vandegeer2000empirical}), to deduce that there exists a universal constant $K_* > 0$ such that
\begin{align*}
\mathbb{E} d_X^2&( \hat{h}_n, h_0) \leq \int_0^{16 \log n}  \mathbb{P}\big( \bigl\{d_X^2(\hat{h}_n, h_0) \geq t\bigr\} \cap
                             \bigl\{\hat{h}_n \in \mathcal{H}(h_0, C_\mu, C_\sigma)\bigr\} \big) \, dt \\
                             &\hspace{2cm} +16 \log n \, \mathbb{P}\bigl( \hat{h}_n \notin \mathcal{H}(h_0, C_\mu, C_\sigma)\bigr) +
                             \int_{16 \log n}^\infty \mathbb{P}\big( d_X^2(\hat{h}_n, h_0) \geq t \big) \, dt
                             \\
&\leq \delta_*^2 + K_* \int_{\delta^2_*}^{16 \log n}  \exp\Bigl( - \frac{n t}{K_*^2} \Bigr) \, dt + \frac{16C \log n}{n} +
                             \int_{16 \log n}^\infty \mathbb{P}\bigg( \max_{i=1,\ldots,n} \log \frac{\hat{h}_n(Z_i)}{h_0(Z_i)}  \geq t \bigg) \, dt \\
&\lesssim n^{-4/5},
\end{align*}
where, to obtain the final inequality, we have applied Lemmas~\ref{Lem:ExtremeEventControl} and \ref{Lem:ExtremeEventControl2}.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{Thm:Adaptation}]
Fix $h_* \in \mathcal{H}^{(k)}$ where $h_*(r) = r^{p-1}e^{\phi_*(r)}$ and $\phi_* \in \Phi^{(k)}$, let $I_1,\ldots, I_k$ be the $k$ intervals on which $\phi_*$ is affine, and let $r_0 := \sup\{r \in [a,\infty):\phi_*(r) > -\infty\}$. Define $\mathcal{Z}_j := \{ i \,:\, Z_i \in I_j \}$ and $n_j := | \mathcal{Z}_j |$.  Let $J := \{ j \,:\, n_j \geq 1\}$ be the set of indices of intervals with at least one data point.  Define $\tilde{\Phi}$ to be the set of upper semi-continuous functions $\phi:[0,\infty) \rightarrow [-\infty,\infty)$ such that $\phi\big|_{I_j}$ is decreasing and concave for each $j=1,\ldots,k$, and such that $\phi(r) = -\infty$ for $r > r_0$.  Note that a function $\phi \in \tilde{\Phi}$ need not be globally decreasing and, in fact, need not be continuous on $[0,r_0]$.  
%\[
%\tilde{\phi}_n := \argmax_{\phi \in \tilde{\Phi}} \frac{1}{n} \sum_{i=1}^n \phi(Z_i) - \int_0^\infty r^{p-1} e^{\phi(r)}\, dr
%\]
%and $\tilde{h}_n(r) := r^{p-1}e^{\tilde{\phi}(r)}$.  Then $\tilde{h}_n$ is a density.  
Given any $\phi \in \tilde{\Phi}$ and $j \in J$, let $\phi^{(j)}(r) := \phi(r) + \log (n/n_j)$ for $r \in I_j$,  %For $r \in I_j$, let $\tilde{\phi}_n^{(j)}(r) := \tilde{\phi}_n(r) + \log (n/n_j)$, and 
and let $\Phi_{I_j} := \{ \phi|_{I_j}:\phi \in \Phi\}$.  Now define 
\[
\tilde{\phi}_n^{(j)} := \argmax_{\phi_j \in \Phi_{I_j}} \biggl\{\frac{1}{n_j} \sum_{i \in \mathcal{Z}_j} \phi_j(Z_i) - \int_{I_j} r^{p-1}e^{\phi_j(r)}\, dr\biggr\},
\]
and let $\tilde{\phi}_n(r) := \tilde{\phi}_n^{(j)}(r) - \log (n/n_j)$ for $r \in I_j$, and $\tilde{\phi}_n(r) := -\infty$ for $r > r_0$.
%  By definition of $\check{\phi}^{(j)}$ and by the fact that $\tilde{\phi}^{(j)} \in \Phi_{I_j}$,
%  \[
%    \frac{1}{n_j} \sum_{i \in \mathcal{Z}_j} \check{\phi}^{(j)}(Z_i) - \int_{I_j} e^{ \check{\phi}^{(j)}(r)} r^{p-1} dr
%    \geq \frac{1}{n_j} \sum_{i \in \mathcal{Z}_j} \tilde{\phi}^{(j)}(Z_i) - \int_{I_j} e^{ \tilde{\phi}^{(j)}(r)} r^{p-1} dr
%  \]
Then, for any $\phi \in \tilde{\Phi}$,
  \begin{align}
\label{Eq:LongDisplay}
  \frac{1}{n} \sum_{i=1}^n \tilde{\phi}_n(Z_i) &- \int_0^\infty r^{p-1} e^{\tilde{\phi}_n(r)} \, dr = \sum_{j \in J} \frac{n_j}{n}\biggl(  \frac{1}{n_j} \sum_{i \in \mathcal{Z}_j} \tilde{\phi}_n(Z_i) -
      \frac{n}{n_j} \int_{I_j} r^{p-1} e^{\tilde{\phi}_n(r)} \, dr \biggr) \nonumber \\
  &= \sum_{j \in J} \frac{n_j}{n}
    \biggl(  \frac{1}{n_j} \sum_{i \in \mathcal{Z}_j} \tilde{\phi}_n^{(j)}(Z_i) -
    \int_{I_j} r^{p-1} e^{\tilde{\phi}_n^{(j)}(r)} \, dr \biggr) - \sum_{j \in J} \frac{n_j}{n} \log \frac{n}{n_j} \nonumber \\
  &\geq \sum_{j \in J} \frac{n_j}{n}
    \biggl(  \frac{1}{n_j} \sum_{i \in \mathcal{Z}_j} \phi^{(j)}(Z_i) -
    \int_{I_j} r^{p-1} e^{\phi^{(j)}(r)} \, dr \biggr) - \sum_{j \in J} \frac{n_j}{n} \log \frac{n}{n_j} \nonumber \\
  &\geq  \frac{1}{n} \sum_{i=1}^n \phi(Z_i) - \int_0^\infty r^{p-1} e^{\phi(r)} \, dr. 
  \end{align}
%We conclude that $\tilde{\phi}_n$ maximises $L(\phi,\mathbb{Q}_n)$ over $\phi \in \tilde{\Phi}$. = \tilde{\phi}_n$. In particular, this implies that if we define $\tilde{h}_n^{(j)} = \tilde{h}_n \cdot \frac{n}{n_j}$, then each of $\tilde{h}_n^{(j)}$'s is a density. 
It follows that the function $\tilde{h}_n$ defined by $\tilde{h}_n(r) := r^{p-1}e^{\tilde{\phi}_n(r)}$ is a density.  Moreover, for $j \in J$, the function $\tilde{h}_n^{(j)} := \frac{n}{n_j}\tilde{h}_n|_{I_j}$ is a density.  Writing $p_j := \int_{I_j} h_0$, and $h_0^{(j)} := \frac{1}{p_j}h_0|_{I_j}$, we deduce from~\eqref{Eq:LongDisplay} that 
  \begin{align}
\label{Eq:dx2}
    \mathbb{E}_{h_0} d_X^2(\hat{h}_n,h_0)
    &\leq \mathbb{E}_{h_0} \frac{1}{n} \sum_{i=1}^n \log \frac{\tilde{h}_n(Z_i)}{h_0(Z_i)} \nonumber \\
       &= \mathbb{E}_{h_0} \sum_{j \in J} \frac{n_j}{n}
      \biggl( \frac{1}{n_j} \sum_{i \in \mathcal{Z}_j} \log \frac{\tilde{h}_n^{(j)}(Z_i) }{h_0^{(j)}(Z_i)} \biggr) + \mathbb{E}_{h_0} \sum_{j \in J} \frac{n_j}{n} \log \frac{n_j}{n p_j}.
  \end{align}
Now, since $n_j \sim \mathrm{Bin}(n,p_j)$ and $\log x \leq x-1$ for $x > 0$, we have
  \begin{align}
\label{Eq:SecondTerm}
    \mathbb{E}_{h_0} \sum_{j \in J}  \frac{n_j}{n} \log \frac{n_j}{n p_j} \leq \mathbb{E}_{h_0} \sum_{j \in J} \frac{n_j}{n} \biggl( \frac{n_j}{np_j} - 1 \biggr) &= \sum_{j \in J} \biggl( \frac{n^2 p_j^2 + np_j(1-p_j)}{n^2 p_j} - p_j \biggr) \nonumber \\
    &= \frac{1}{n}\sum_{j \in J} (1-p_j) \leq \frac{k}{n}.
  \end{align}
To bound the first term in~\eqref{Eq:dx2}, for $j=1,\ldots,k$, let us first define $q_j := \int_{I_j} h_*(r) \, dr$, $h_*^{(j)} := h_*/q_j$, $\nu^2_{*,j} := d_{\mathrm{H}}^2(h_0^{(j)}, h_*^{(j)})$, $\nu^2_* := \sum_{j=1}^k p_j \nu^2_{*,j}$, $J_0 := \{j:n_j\nu_{*,j}^2 \geq 1\}$ and temporarily assume that $k \leq e^{-1/4}n$.  By Proposition~\ref{Prop:AdaptiveRateAffine} (and the subsequent remark) below, applied conditionally on $n_1,\ldots,n_k$, a simple extension of Jensen's inequality using the fact that $x \mapsto \log^{5/4} x$ is concave on $[e^{1/4},\infty)$ \citep[e.g.][Lemma~2]{han2017isotonic} and the fact that $k \mapsto k \log^{5/4}(en/k)$ is increasing for $k \in [1,e^{-1/4}n]$,
  \begin{align}
    \label{Eqn:TripleJensenPrecursor}
    \mathbb{E}_{h_0} \sum_{j \in J} \frac{n_j}{n}
    \biggl( \frac{1}{n_j} \sum_{i \in \mathcal{Z}_j} &\log \frac{\tilde{h}_n^{(j)}(Z_i) }{h_0^{(j)}(Z_i)} \biggr) 
    \lesssim \mathbb{E}_{h_0} \sum_{j \in J} \frac{n_j}{n} \biggl(\frac{1}{n_j} \log^{5/4} (en_j) +  \frac{\nu_{*,j}^{2/5}}{n_j^{4/5}}\log \frac{en_j}{\nu_{*,j}} \biggr) \nonumber \\
    &\lesssim \frac{k}{n} \log^{5/4} \Bigl(\frac{en}{k}\Bigr) + \frac{1}{n} \mathbb{E}_{h_0} \sum_{j \in J} n_j^{1/5} \nu_{*,j}^{2/5} \bigl\{\log (n_j \nu_{*,j}^2) + \log(e/\nu_{*,j}^3) \bigr\}.
  \end{align} 
To bound the second term of~\eqref{Eqn:TripleJensenPrecursor}, observe that by two applications of Jensen's inequality,
  \begin{align}
    \label{Eqn:Jensen1}
    \frac{1}{n} \mathbb{E}_{h_0} \sum_{j \in J} n_j^{1/5} \nu_{*,j}^{2/5} \log(n_j \nu_{*,j}^2)
    &\leq \frac{1}{n} \mathbb{E}_{h_0} \biggl\{|J_0|^{4/5}\biggl(
      \sum_{j\in J_0} n_j \nu_{*,j}^2 \biggr)^{1/5} \log \biggl(  \frac{\sum_{j \in J_0} n_j \nu_{*,j}^2 }{|J_0|} \biggr)\biggr\} \nonumber \\
&\leq \frac{1}{n} \mathbb{E}_{h_0} \biggl\{|J_0|^{4/5}\biggl(
      \sum_{j=1}^k n_j \nu_{*,j}^2 \biggr)^{1/5} \log \biggl(  \frac{\sum_{j =1}^k n_j \nu_{*,j}^2 }{|J_0|} \biggr)\biggr\} \nonumber \\
&\leq \frac{k^{4/5}}{n} \log \Bigl(\frac{en}{k}\Bigr)\mathbb{E}_{h_0} \biggl\{\biggl(
      \sum_{j=1}^k n_j \nu_{*,j}^2 \biggr)^{1/5} \biggr\} \nonumber \\
    &\leq \frac{k^{4/5}}{n^{4/5}} \nu_*^{2/5} \log \frac{en}{k}. %\leq  \frac{2k^{4/5}}{n^{4/5}} \nu_k^{2/5} \log \frac{en}{k},
  \end{align}
But
\begin{align}
\label{Eq:nustar}
\nu_*^2 = \sum_{j=1}^k p_j \nu_{*,j}^2 \leq 2 \wedge \sum_{j=1}^k p_j d_{\mathrm{KL}}^2(h_0^{(j)}, h_*^{(j)}) &\leq 2 \wedge \biggl\{\sum_{j=1}^k p_j d_{\mathrm{KL}}^2(h_0^{(j)}, h_*^{(j)}) + \sum_{j=1}^k p_j \log \frac{p_j}{q_j}\biggr\} \nonumber \\
&= 2 \wedge d_{\mathrm{KL}}^2(h_0,h_*).
\end{align}
%where we used the fact that $\nu_*^2 = \sum_{j=1}^k p_j \nu_{*,j}^2 \leq 2 \wedge \sum_{j=1}^k p_j d_{\mathrm{KL}}^2(h_0^{(j)}, h_*^{(j)}) \leq \blue{\sout{\nu_k^2}}2 \wedge \sum_{j=1}^k p_j d_{\mathrm{KL}}^2(h_0^{(j)}, h_*^{(j)})$ in the final step.  
Moreover, by three further applications of Jensen's inequality, and using the fact that $x \mapsto \log^5 x$ is concave for $x \geq e^5$, we have
  \begin{align}
\label{Eq:TripleJensen}
    \frac{1}{n} \mathbb{E}_{h_0} \sum_{j \in J} &n_j^{1/5} \nu_{*,j}^{2/5} \log(e/\nu_{*,j}^3) \leq \frac{3}{n} \mathbb{E}_{h_0} \sum_{j =1}^k n_j^{1/5} \nu_{*,j}^{2/5} \log \frac{2^{1/2}e^5}{\nu_{*,j}} \nonumber \\
&\leq \frac{3k^{4/5}}{n} \mathbb{E}_{h_0} \biggl\{\biggl(\sum_{j=1}^k n_j \nu_{*,j}^2 \log^5 \frac{2^{1/2}e^5}{\nu_{*,j}}\biggr)^{1/5}\biggr\} \leq \frac{3k^{4/5}}{n^{4/5}}\biggl(\sum_{j=1}^k p_j \nu_{*,j}^2 \log^5 \frac{2^{1/2}e^5}{\nu_{*,j}}\biggr)^{1/5} \nonumber \\
                                                                                              &\leq \frac{3k^{4/5}}{n^{4/5}}\nu_*^{2/5}\log\biggl(2^{1/2}e^5\sum_{j=1}^k \frac{p_j\nu_{*,j}}{\nu_k^2}\biggr) \leq \frac{3k^{4/5}}{n^{4/5}}\nu_*^{2/5}\log \frac{2e^5}{\nu_*^2} \nonumber \\
&\leq \frac{3k^{4/5}}{n^{4/5}}\{2 \wedge d_{\mathrm{KL}}^2(h_0,h_*)\}^{1/5}\log \frac{2e^5}{2 \wedge d_{\mathrm{KL}}^2(h_0,h_*)},
%                                                                                                \red{ \frac{5k^{4/5}}{n^{4/5}}\nu_k^{2/5}\log \frac{2e^5}{\nu_k^2},}
  \end{align}
where the last step follows from the fact that $x \mapsto x^{1/5} \log \frac{2e^5}{x}$ is increasing for $x \leq 2$.  Combining~\eqref{Eq:dx2},~\eqref{Eq:SecondTerm}~\eqref{Eqn:TripleJensenPrecursor},~\eqref{Eqn:Jensen1},~\eqref{Eq:nustar} and~\eqref{Eq:TripleJensen}, the result follows in the case $k \leq e^{-1/4}n$.

Now suppose $k > e^{-1/4} n$. Then, by Theorem~\ref{Thm:WorstCase},
\begin{align*}
  \mathbb{E}_{h_0} d^2_X(\hat{h}_n, h_0) \lesssim n^{-4/5} \lesssim \frac{k}{n} \log^{5/4} \frac{en}{k},
\end{align*}
which completes the proof.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{Prop:AdaptiveRateAffine}]
By the scale equivariance described in Proposition~\ref{Prop:ProjectionBasicProperties}(i), we may assume without loss of generality that $\sigma_{h_0} = 1$.  Define $\nu := \inf \bigl\{ d_{\mathrm{H}}(h_0, h) \,:\, h \in \mathcal{H}_{a}^{(1)}, h_0 \ll h \bigr\} \in [0,2^{1/2}]$. By Lemma~\ref{Lem:LocalBracketing}, if $\delta \in (0, 2^{-9}-\nu)$, then for every $\epsilon > 0$, it holds that 
\begin{align*}
  H_{[]}\bigl(\epsilon, \mathcal{H}(h_0, C_\mu, C_\sigma) \cap \mathcal{H}(h_0, \delta), d_{\mathrm{H}}\bigr) 
  \leq  H_{[]}(\epsilon, \mathcal{H}(h_0, \delta), d_{\mathrm{H}}) \lesssim \frac{(\delta + \nu)^{1/2}}{\epsilon^{1/2}} \log^{5/4} \frac{1}{\delta}.
\end{align*}
On the other hand, if $\delta \geq 2^{-9} - \nu$, then by Lemma~\ref{Prop:NonlocalBracketingEntropy}, for every $\epsilon > 0$, we have that 
\begin{align*}
    H_{[]}\bigl(\epsilon, \mathcal{H}(h_0, C_\mu, C_\sigma) \cap \mathcal{H}(h_0, \delta), d_{\mathrm{H}}\bigr) \leq  H_{[]}\bigl(\epsilon, \mathcal{H}(h_0, C_\mu, C_\sigma), d_{\mathrm{H}}\bigr) \lesssim \frac{1}{\epsilon^{1/2}} \lesssim \frac{(\delta + \nu)^{1/2}}{\epsilon^{1/2}}.
\end{align*}
It follows that 
\begin{align*}
  \int_0^\delta H_{[]}^{1/2}\bigl(\epsilon, \mathcal{H}(h_0, \delta) \cap
    \mathcal{H}(h_0, C_\mu, C_\sigma), d_{\mathrm{H}}\bigr) \, d\epsilon &\lesssim (\delta+\nu)^{1/4}
    \bigl\{ \log^{5/8} (1/\delta) \vee 1 \bigr\}
    \int_0^{\delta} \epsilon^{-1/4} \, d\epsilon \\
  &\lesssim \delta^{3/4} (\delta+\nu)^{1/4} \bigl\{ \log^{5/8} (1/\delta) \vee 1 \bigr\}.
\end{align*}
Define $\Psi(\delta) :=  C \delta^{3/4} (\delta+\nu)^{1/4}\{ \log^{5/8} (1/\delta) \vee 1\}$, where $C > 0$ is chosen such that 
\[
\Psi(\delta) \geq \max\biggl\{\int_0^\delta H_{[]}^{1/2}\bigl(\epsilon, \mathcal{H}(h_0, \delta) \cap
    \mathcal{H}(h_0, C_\mu, C_\sigma), d_{\mathrm{H}}\bigr) \, d\epsilon \, , \, \delta\biggr\},
\]
%Since $\delta \mapsto \Psi(\delta)/\delta^2$ is decreasing on $(0,\infty)$
%. $\frac{\Psi(\delta)}{\delta^2} = C \frac{(\delta + \nu)^{1/4}}{\delta^{5/4}}  \left( \log(1/\delta) \vee 1 \right)$ is decreasing in $\delta$. 
Set $\delta_n := K\bigl\{\frac{\nu^{2/5}}{n^{4/5}} \log \frac{en}{\nu} + \frac{1}{n} \log^{5/4}(en)\bigr\}^{1/2}$ for a universal constant $K > 0$ to be chosen later.  Then, because $\Psi(\delta)/\delta^2$ is non-increasing, we have
\begin{align*}
  \inf_{\delta \geq \delta_n} \frac{n^{1/2}\delta^2}{\Psi(\delta)} = & \frac{n^{1/2}\delta_n^2}{\Psi(\delta_n)} = \frac{n^{1/2}\delta_n^{5/4}}{C(\delta_n+\nu)^{1/4}\{ \log^{5/8} (1/\delta_n) \vee 1\}}.
\end{align*}
By choosing the universal constant $K > 0$ sufficiently large, we can ensure that this ratio is larger than the universal constant required to apply Theorem~10 in the online supplement of \citet{kim2016adaptationsupp} (a minor restatement of \citet[][Corollary~7.5]{vandegeer2000empirical}).  We deduce from this result that there exists a universal constant $C > 0$ such that for $\delta \geq \delta_n$,
\begin{equation}
\label{Eq:vdg}
\mathbb{P}\bigl(d_X^2(\hat{h}_n,h_0) \geq \delta^2\bigr) \leq C\exp\Bigl(-\frac{n\delta^2}{C}\Bigr).
\end{equation}
Moreover, by Lemmas~\ref{Lem:ExtremeEventControl} and \ref{Lem:ExtremeEventControl2}, for $n \geq 4$, 
\begin{align}
\label{Eq:Max}
  \int_{16 \log n}^\infty \mathbb{P}\bigg( \max_{i=1,\ldots,n} \log &\frac{\hat{h}_n(Z_i)}{h_0(Z_i)}  \geq t \bigg) \, dt
  = \log n \int_{16}^\infty  \mathbb{P}\bigg( \max_{i=1,\ldots,n} \log \frac{\hat{h}_n(Z_i)}{h_0(Z_i)}  \geq s\log n \bigg) \, ds \nonumber \\
  &\lesssim \log n \int_{16}^\infty n^{-(s/2-1)} + \Bigl(\frac{2^8 e}{n^{s/16}}\Bigr)^n + n^{-s n^{1/2}/256} \, ds \lesssim \frac{\log n}{n}.
\end{align}
It follows from~\eqref{Eq:vdg},~\eqref{Eq:Max} and Proposition~\ref{Prop:MLEMeanVarPreservation} that
\begin{align*}
   \mathbb{E} d_X^2( \hat{h}_n, h_0) 
  & \leq  \int_0^{16 \log n}  \mathbb{P}\Big( \bigl\{d_X^2(\hat{h}_n, h_0) \geq t\bigr\} \cap
         \bigl\{\hat{h}_n \in \mathcal{H}(h_0, C_\mu, C_\sigma)\bigr\} \Big) \, dt  \\
  & \hspace{1cm}+ 16 \log n \, \mathbb{P}\bigl( \hat{h}_n \notin \mathcal{H}(h_0, C_\mu, C_\sigma)\bigr) +
    \int_{16 \log n}^\infty \mathbb{P}\big( d_X^2(\hat{h}_n, h_0) \geq t \big) \, dt
  \\
  &\leq  \delta_n^2 + C \int_{\delta_n^2}^{\infty} \exp\Bigl(-\frac{nt}{C}\Bigr) \, dt + \frac{C \log n}{n} +
         \int_{16 \log n}^\infty \mathbb{P}\bigg( \max_{i=1,\ldots,n} \log \frac{\hat{h}_n(Z_i)}{h_0(Z_i)}  \geq t \bigg) \, dt \\
&\lesssim \frac{\nu^{2/5}}{n^{4/5}}\log\frac{en}{\nu} + \frac{\log^{5/4}(en)}{n},
\end{align*}
as required.
\end{proof}

\section{Auxiliary results}

\subsection{Lemmas used in results in Section~\ref{Sec:Moments}} 

The mean $\mu_h$ of any $h \in \mathcal{H}_a$ is constrained because $h(r) = r^{p-1}e^{\phi(r)}$ and $\phi$ is decreasing. The next lemma formalises this notion.
\begin{lemma}
  \label{Lem:UnitVarianceMeanBound}
  Let $a \geq 0$ and let $h \in \mathcal{H}_{a}$ with $\sigma_h = 1$.  For $s \in (0,\infty)$, let $\rho(s) := \frac{(a + s)^{p-1} p s}{(a + s)^p - a^p}$.  Then, writing $s^* := \sup \{ s \in (0,\infty) : \frac{\rho(s)}{s} \geq 2^{-7} \}$, we have
\[
a \leq \mu_h \leq a + 2^{12} \rho(s^*+1).
\]
\end{lemma}
\begin{remark}
Even though we cannot obtain an analytic expression for $\rho(s^*+1)$, we can apply the bounds developed in Lemma~\ref{Lem:RhoProperties} to control $\mu_h$.  For example, since $\rho(s) \leq p$ for any $a \geq 0$ by Lemma~\ref{Lem:RhoProperties}(iii), we have that $\mu_h - a \lesssim p$. When $a = 0$, this bound is sharp up to the universal constant, because taking $e^{\phi(r)} = \frac{p}{a^p}\mathbbm{1}_{\{r \in [0,a]\}}$, where $a = (p+1)(p+2)^{1/2}/p^{1/2}$, yields $\mu_h = p^{1/2}(p+2)^{1/2}$. 
\end{remark}
\begin{proof}
If $h \in \mathcal{H}_{a}$ with $\sigma_h = 1$, then $h(r) = 0$ for $r \in (-\infty, a)$ and thus, $\mu_h \geq a$.  For the upper bound on $\mu_h$, we first observe that $h(\mu_h) \geq 2^{-7}$ by \citet[][Theorem~5.14(d)]{lovasz2007geometry}.  Since $\rho(s)/s$ is decreasing by Lemma~\ref{Lem:RhoProperties}(ii) and $1 \leq \rho(s) \leq p$ by Lemma~\ref{Lem:RhoProperties}(iii), we have that $s^* \in (0,\infty)$.  Suppose for a contradiction that $\mu_h > a + 2^{12}\rho(s^*+1)$.  Then, since $\rho$ is increasing  by Lemma~\ref{Lem:RhoProperties}(i), we have $\frac{\rho(s + 1)}{\mu_h - a} < 2^{-12}$ for all $s \leq s^*$. Moreover, by definition of $s^*$, we have $\rho(s)/s < 2^{-7}$ for all $s > s^*$. Hence $\sup_{s \in (0,\infty)} \min \bigl( 24 \frac{\rho(s)}{\mu_h - a}, \frac{\rho(s)}{s} \bigr) < 2^{-7}$.  But then Lemma~\ref{Lem:MeanConstrainedSupBound} establishes a contradiction, so $\mu_h \leq a + 2^{12} \rho(s^*+1)$, as desired.  
\end{proof}

The next lemma is used in the proof of Lemma~\ref{Prop:UnitVarianceMeanBound}.
\begin{lemma}
  \label{Lem:MeanConstrainedSupBound}
For any $a \geq 0$, $h \in \mathcal{H}_{a}$ and any $s \in (0, \infty)$, 
  \[
   h(a + s) \leq \min\biggl\{ 16 \frac{\rho(s+1)}{\mu_h - a}, \frac{\rho(s)}{s} \biggr\}.
  \]
\end{lemma}

%Note that because $(a + r)^p - a^p = p (a + r')^{p-1} r$ for some $r' \in [0, r]$, we have that $\rho(r) \geq 1$ for all $r$. If $a = 0$, then $\rho(r) = \frac{(a+r)^{p-1} p r}{(a + r)^p - a^p} = p$. If $a \rightarrow \infty$ and $p$ is fixed, then $\frac{(a+r)^{p-1} r p }{(a + r)^p - a^p} = \frac{r p}{(a + r) - \left( \frac{ a}{a + r} \right)^{p-1} a } \rightarrow \frac{p}{p-1}$.

\begin{proof}

  Let us fix $h \in \mathcal{H}_{a}$ and define $\tilde{h}(s) := h(a + s)$. Observe that $\tilde{h}$ is a density of the form $\tilde{h}(s) = e^{\phi(s)} (a + s)^{p-1}$ for some $\phi \in \Phi_0$.  We will show that $\tilde{h}(s) \leq \min \bigl\{ 16 \frac{\rho(s+1)}{\mu_{\tilde{h}}}, \frac{\rho(s)}{s} \bigr\}$ for all $s \in [0, \infty)$. The lemma then follows because $\mu_{\tilde{h}} = \mu - a$.

  To this end, fix $s_0 \in (0, \infty)$ and define $\tilde{h}_{s_0}(s) := \alpha(s+a)^{p-1}\mathbbm{1}_{\{s \in [0, s_0]\}}$ where $\alpha^{-1} := \int_0^{s_0} (a + s)^{p-1} \, ds = \frac{(a + s_0)^p}{p} - \frac{a^p}{p}$.  Then

  \begin{align*}
   1 = \alpha \int_0^{s_0} (a + s)^{p-1} \, ds \geq \int_0^{s_0} e^{\phi(s)} (a + s)^{p-1} \, ds 
      \geq e^{\phi(s_0)} \int_0^{s_0} (a + s)^{p-1} \, ds.
  \end{align*}
Thus, we see that $e^{\phi(s_0)} \leq \alpha$ and therefore $\tilde{h}(s_0) \leq \tilde{h}_{s_0}(s_0) = \alpha (s_0 + a)^{p-1} \leq \rho(s_0)/s_0$, as desired. 

  To prove the second part of the bound, observe that if $s_0 \geq \mu_{\tilde{h}}/8$, then $\frac{\rho(s_0)}{s_0} \leq 8 \frac{\rho(s_0)}{\mu_{\tilde{h}}}$ and lemma follows.  We therefore fix $s_0 \in (0, \mu_{\tilde{h}}/8)$, and also define $M := \log 4 \, \vee \, \sup_{s \in [0, \infty)} \log \tilde{h}(s)$ and fix $m \in (-\infty, M-2]$.  For $t \in [m,M]$, let $D_t := \{s \in [0,\infty) \,:\, \log \tilde{h}(s) \geq t\}$.  Since $\tilde{h}$ is itself a log-concave density, we have that, any $t \in [m,M]$ and $s \in D_m$, 
\[
\log \tilde{h} \biggl(\frac{t-m}{M-m}s_0 + \frac{M-t}{M-m}s\biggr) \geq \frac{(t-m)M}{M-m} + \frac{(M-t)m}{M-m} = t.
\]
Hence, writing $\lambda$ for Lebesgue measure on $\mathbb{R}$,
\[
\lambda(D_t) \geq \lambda \biggl(\frac{t-m}{M-m}s_0 + \frac{M-t}{M-m}D_m\biggr) = \frac{M-t}{M-m}\lambda(D_m).
\]
Using Fubini's theorem as in \citet[][Lemma~4.1]{dumbgen2011approximation}, we can now compute
\begin{align*}
1 &\geq \int_{D_m} \tilde{h}(s) - e^m \, ds \geq \int_{D_m} \int_m^M e^t \mathbbm{1}_{\{\log \tilde{h}(s) \geq t\}} \, dt \, ds \\
&= \int_m^M e^t \lambda(D_t) \, dt \geq \frac{\lambda(D_m)}{M-m}\int_m^M (M-t)e^t \, dt = \frac{\lambda(D_m)e^M}{M-m}\int_0^{M-m} xe^{-x} \, dx \\
&\geq \frac{\lambda(D_m)e^M}{2(M-m)}.
\end{align*}                
Since $D_m$ is an interval containing $s_0$, we conclude that $\log \tilde{h}(s) \leq m$ whenever $|s-s_0| > 2(M-m)e^{-M}$.  Thus, for $|s-s_0| \geq 4e^{-M}$, we have
\[
\log \tilde{h}(s) \leq \inf\bigl\{m \in (-\infty,M-2] :2(M-m)e^{-M} < |s-s_0|\bigr\} = M - \frac{|s-s_0|e^M}{2}.
\]
We now consider two cases.  If $s_0 - 4e^{-M} > 0$, then using the bound $\tilde{h}(s) \leq \rho(s)/s$, Lemma~\ref{Lem:RhoProperties}(i) and the fact that $e^M \geq 4$, we have 
\begin{align*}
  \mu_{\tilde{h}} \leq \int_0^\infty s \tilde{h}(s) \, ds
  &\leq \int_0^{s_0 - 4e^{-M}} s \exp\biggl\{M - \frac{(s_0-s)e^M}{2}\biggr\} \, ds + \int_{s_0 - 4e^{-M}}^{s_0 + 4e^{-M}} \rho(s) \, ds \\  
  &\hspace{3cm} + \int_{s_0 + 4e^{-M}}^\infty s \exp\biggl\{M - \frac{(s-s_0)e^M}{2}\biggr\} \, ds \\ %% NEW LINE
  &\leq 2\int_2^\infty \biggl(s_0 - \frac{2t}{e^M}\biggr) e^{-t} \, dt + 8e^{-M} \rho(s_0 + 1) + 2\int_2^\infty \biggl(s_0 + \frac{2t}{e^M}\biggr) e^{-t} \, dt \\  %% NEW LINE
  &\leq 4 s_0 + 8 e^{-M} \rho(s_0 + 1).
\end{align*}
Since $4s_0 < \mu_{\tilde{h}}/2$, we conclude that $e^M \leq 16 \frac{\rho(s_0 + 1)}{\mu_{\tilde{h}}}$. 

Now, suppose $s_0 - 4e^{-M} \leq 0$. Then, similarly, 
\begin{align*}
\mu_{\tilde{h}} \leq \int_0^\infty s \tilde{h}(s) \, ds &\leq  \int_0^{s_0 + 4e^{-M}} \rho(s) \, ds + \int_{s_0 + 4e^{-M}}^\infty s \exp\biggl\{M - \frac{(s-s_0)e^M}{2}\biggr\} \, ds \\
                                    &\leq 8e^{-M} \rho(s_0 + 1) + 2\int_2^\infty \biggl(s_0 + \frac{2t}{e^M}\biggr) e^{-t} \, dt \\
                                     &\leq 2s_0 + 4e^{-M}\bigl\{2\rho(s_0 + 1) +1\bigr\}.
\end{align*}
Using the facts that $s_0 \leq \mu_{\tilde{h}}/8$ and $\rho(s_0 + 1) \geq 1$ yields the desired upper bound on $e^M$.
\end{proof}

The next lemma provides basic properties of the function $\rho$ defined in Lemma~\ref{Prop:UnitVarianceMeanBound}.
\begin{lemma}
  \label{Lem:RhoProperties}
  For any $p \in \mathbb{N}$ and $a \geq 0$, we have
  \begin{enumerate}[(i)]
        \item $\rho(s)$ is increasing;
        \item $\frac{\rho(s)}{s}$ is decreasing;
        \item $1 \leq \rho(s) \leq p$ for all $s \in (0,\infty)$;
        \item $\frac{\rho(s+1)}{\rho(s)} \leq 20$ for $s \geq 1$.
    \end{enumerate}
\end{lemma}
\begin{proof}
$(i)$ Define $\alpha := (a+s)/a \in (1,\infty)$.  Then
  \begin{align*}
    \rho(s) & =  \frac{(a + s)^{p-1} p s}{(a + s)^p - a^p} = \frac{ \alpha^{p-1} p (\alpha - 1)}{\alpha^p - 1} 
              = p \biggl( 1 - \frac{\alpha^{p-1} - 1}{\alpha^p - 1} \biggr),
  \end{align*}
which is increasing in $\alpha$, as required.

$(ii)$ If $a > 0$, then
  \begin{align*}
    \frac{\rho(s)}{s} = \frac{p}{a} \frac{\alpha^{p-1} }{\alpha^p - 1},
  \end{align*}
which is decreasing in $\alpha$. If $a = 0$, then $\rho(s)/s = p/s$ and the claim also follows.

$(iii)$ The lower bound follows from $(i)$ and the fact that $\lim_{s \searrow 0} \rho(s) = 1$, while the upper bound follows from $(i)$ and the fact that $\lim_{s \rightarrow \infty} \rho(s) = p$.

$(iv)$ We have
  \begin{align*}
    \frac{\rho(s+1)}{\rho(s)} &= \frac{(a + s + 1)^{p-1}}{(a + s)^{p-1}} \frac{s+1}{s} \frac{(a + s)^p - a^p}{(a + s+ 1)^p - a^p} \\
                              &\leq 2 \biggl( 1 + \frac{1}{a + s} \biggr)^{p-1}
                                \biggl\{ \frac{1 - ( \frac{a}{a+s} )^p}
                                { (1 + \frac{1}{a + s})^p - ( \frac{a}{a + s} )^p } \biggr\}.
  \end{align*}
Hence, if $a + s \geq p/2$, then $\rho(s+1)/\rho(s) \leq 2 e^2 \leq 20$. On the other hand, if $a + s < p/2$, then $(1 + \frac{1}{a + s})^p - (\frac{a}{a + s})^p \geq \frac{1}{2} (1 + \frac{1}{a + s})^p$ and the claim follows again. 
\end{proof}

\begin{lemma}
  \label{Lem:EvilInequality}
  For any $p \geq 1$ and $r_0 > a \geq 0$, we have that
  \[
    r_0 \leq \frac{p}{p+1} \frac{r_0^{p+1} - a^{p+1}}{r_0^p - a^p}
    + \frac{r_0^p - a^p}{r_0^{p-1}  p}
  \]
\end{lemma}
\begin{proof}
Writing $x := 1 - (a/r_0)^p$, we are required to prove that for $x \in (0,1]$,
  \begin{align}
    1 \leq \frac{p}{p+1} \frac{1 - (1-x)^{\frac{p+1}{p}}}{x} + \frac{x}{p}.
  \end{align}
The inequality holds when $x \searrow 0$ and at $x = 1$. To finish the proof, it suffices to show that $t_p(x) := \frac{1 - (1 - x)^{(p+1)/p}}{x}$ is concave on $(0,1)$.  But
\begin{align*}
p^2(1-x)x^3 t_p''(x) &= 2p^2\bigl\{1 - (1-x)^{1/p}\}(1-x)^{1-1/p} - p(2-x)x - x^2 \\
&\leq 2px\bigl\{1 - (1-1/p)x\bigr\} - p(2-x)x - x^2 = -(p-1)x^2 \leq 0,
\end{align*}
as required.
\end{proof}

  \begin{lemma}
    \label{Lem:VarianceSupRelation}
    Let $h \in \mathcal{F}_1$.  
    \begin{enumerate}[(i)]
    \item There exists a universal constant $c > 0$ such that $\sigma_h \geq c/h(\mu_h)$;
    \item There exists a universal constant $C > 0$ such that $\sigma_h \leq C/\sup_{r \in \mathbb{R}} h(r)$.
    \end{enumerate}
  \end{lemma}

  \begin{proof}
(i)  Let $f(r) := \sigma_h h(\sigma_h r + \mu_h)$, so $f$ is a log-concave density with $\mu_f = 0$ and $\sigma_f=1$. Then
    \[
 \sigma_h = \frac{f(0)}{h(\mu_h)} \geq \frac{2^{-7}}{h(\mu_h)},
    \]
where the final inequality follows from \citet[][Theorem~5.14(d)]{lovasz2007geometry}.

(ii) Since $h$ is upper semi-continuous, there exists $r_0 \in \mathbb{R}$ such that $h(r_0) = \sup_{r \in \mathbb{R}} h(r)$.  Thus
    \[
\sigma_h \leq \frac{f(r_0/\sigma_h)}{h(r_0)} \leq \frac{\|f\|_\infty}{\sup_{r \in \mathbb{R}} h(r)} \leq \frac{2^7}{\sup_{r \in \mathbb{R}} h(r)},
    \]
by \citet[][Theorem~5.14(b) and (d)]{lovasz2007geometry}.
  \end{proof}

\begin{lemma}
  \label{Lem:DifferentialEntropyBound}
  For any $f \in \mathcal{F}_1$ with $\sigma_f = 1$, we have that
  \[
    -\frac{1}{2} - \frac{1}{2}\log (2\pi) \leq    \int_{-\infty}^\infty f \log f \leq 9 \log 2.
  \]
\end{lemma}
\begin{proof}
By the location invariance of the entropy functional, we may assume without loss of generality that $\int_{-\infty}^\infty xf(x) \, dx = 0$.  Since $\|f\|_\infty \leq 2^9$ \citep[][Theorem~5.14(b) and (d)]{lovasz2007geometry}, we have
  \begin{align*}
    \int_{-\infty}^\infty f\log f \leq 9 \log 2.
  \end{align*}
Now let $g(x) := (2\pi)^{-1/2}e^{-x^2/2}$, so by the non-negativity of Kullback--Leibler divergence, 
  \begin{align*}
    \int_{-\infty}^\infty f \log f \geq \int_{-\infty}^\infty f \log g = - \frac{1}{2}\int_{-\infty}^\infty x^2 f(x) \, dx - \frac{1}{2}\log (2\pi) = -\frac{1}{2} - \frac{1}{2}\log (2\pi),
  \end{align*}
as required.
\end{proof}


\subsection{Lemmas used in both Theorem~\ref{Thm:WorstCase} and Proposition~\ref{Prop:AdaptiveRateAffine}}
\label{Sec:BracketExtremeEvent}

The next lemma is a very slight generalisation of \cite[][Theorem~4]{kim2016global} and can be proved in the same manner, with  minor modifications to handle the general mean and variance perturbation.
\begin{lemma} \citet[][Theorem 4]{kim2016global}
  \label{Prop:NonlocalBracketingEntropy}
Fix $C_\mu, C_\sigma > 0$ and $h_0 \in \mathcal{H}_a$.  There exists $C > 0$, depending only on $C_\mu, C_\sigma$, such that for every $\epsilon > 0$,
  \[
    H_{[]}( \epsilon, \mathcal{H}(h_0, C_\mu, C_{\sigma}), d_{\mathrm{H}}) \leq \frac{C}{\epsilon^{1/2}}.
  \]
\end{lemma}

\begin{lemma}
  \label{Lem:ExtremeEventControl}
Let $a \geq 0$ and let $Z_1,\ldots, Z_n \stackrel{\mathrm{iid}}{\sim} h_0 \in \mathcal{H}_{a}$ with $\sigma_{h_0} = 1$.  Then for $n \geq 4$ and $t \geq 8$,
\[
\mathbb{P}\biggl(\sup_{r \geq a} \log \hat{h}_n(r) > t\log n\biggr) \leq \Bigl(\frac{2^8 e}{n^{t/8}}\Bigr)^n + 2n^{-t n^{1/2}/128}.
\]
\end{lemma}
\begin{proof}
  Let $\mathbb{Q}_n$ denote the empirical distribution of $Z_1,\ldots,Z_n$.  Let $E_{1}$ be the event that $\mathbb{Q}_n(A) \leq 1/2$ for every interval $A$ of length at most $6n^{-t/2}$.  Since $\|h_0\|_\infty \leq 2^9$ \citep[][Theorem~5.14(b) and (d)]{lovasz2007geometry}, we have that

\begin{align*}
  \mathbb{P}(E_1^c) &= \mathbb{P}\Biggl(\bigcup_{i=1}^n \, \bigcup_{\substack{ A \subset \{1,\ldots,n\} \setminus \{i\} \\ |A| = \lfloor n /2 \rfloor} } \, \bigcap_{j \in A} \{Z_j \in [Z_i,Z_i+6n^{-t/2}]\} \Biggr)  \\
                    &\leq n \binom{n-1}{ \lfloor n/2 \rfloor } \mathbb{P} \biggl( \bigcap_{j=2}^{ \lfloor n/2\rfloor + 1} \{ Z_j \in [Z_1, Z_1 + 6 n^{-t/2}] \} \biggr) \\
    &\leq n \binom{n-1}{ \lfloor n/2 \rfloor } (2^9 6 n^{-t/2})^{\lfloor n/2 \rfloor} \leq  \Bigl(\frac{2^8 e}{n^{t/8}}\Bigr)^n.
\end{align*}
%}
Now let $E_{2} := \bigl\{\int_{a}^\infty  \log h_0 \, d\mathbb{Q}_n > -(t/2) \log n\bigr\}$, so by Lemma~\ref{Lem:DifferentialEntropyBound} and \citet[][Theorem~1.1]{bobkov2011concentration},
  \begin{align*}
\mathbb{P}(E_2^c) &\leq \mathbb{P}\biggl(\biggl|\int h_0(r) \log h_0(r) \, dr - \int \log h_0 \, d\mathbb{Q}_n\biggr| \geq -\frac{1}{2} - \frac{1}{2}\log(2\pi) + \frac{t}{2} \log n\biggr) \\
 &\leq \mathbb{P}\biggl(\biggl|\int h_0(r) \log h_0(r) \, dr - \int \log h_0 \, d\mathbb{Q}_n\biggr| \geq \frac{t}{8} \log n\biggr) \leq 2n^{-t n^{1/2}/128}.
\end{align*}
But on $E_1 \cap E_2$, and applying Proposition~\ref{Prop:MLEVarPreservation} with $\delta_{\mathbb{Q}_n} = 6n^{-t/2}$ and $\ell_{\hat{h}_n} = -(t/2)\log n$, we find that 
\[
\sup_{r \geq a} \log \hat{h}_n(r) \leq \max \biggl\{ - 2 \log \Bigl( \frac{\delta_{\mathbb{Q}_n}}{6} \Bigr),\, - 2 \ell_{\hat{h}_n} \, , \, 3\biggr\} \leq t \log n.
\]
The result follows.

\end{proof}

\begin{lemma} 
  \label{Lem:ExtremeEventControl2}
Let $a \geq 0$, let $h_0 \in \mathcal{H}_{a}$ with $\sigma_{h_0}=1$, and suppose that $Z_1,\ldots,Z_n \stackrel{\mathrm{iid}}{\sim} h_0$. Then there exists a universal constant $C > 0$ such that for any $n \geq 2$ and $t \geq 4$,
  \[
\mathbb{P}\biggl(\sup_{r \in [Z_{(1)}, Z_{(n)}]} \log \frac{1}{h_0(r)} \geq t \log n\biggr) \leq Cn^{-(t-1)}.
  \]
\end{lemma}
\begin{proof}
This result follows from the proof of \citet[][Lemma~2]{kim2016adaptationsupp}.
\end{proof}

\subsection{Lemmas used in Proposition~\ref{Prop:AdaptiveRateAffine}}

\label{Sec:LocalBrackets}
 
For $h_0 \in \mathcal{H}_a$ and $\delta > 0$, let 
\[
  \mathcal{H}(h_0, \delta) := \bigl\{ h \in \mathcal{H}_a : h \ll h_0, d_{\mathrm{H}}(h,h_0) \leq \delta \bigr\}.
\]
\begin{lemma}
  \label{Lem:LocalBracketing}
  Fix $a \geq 0$ and let $h_0 \in \mathcal{H}_{a}$. Assume that $\nu := \inf \{ d_{\mathrm{H}}(h_0, h) \,:\, h \in \mathcal{H}_{a}^{(1)}, h_0 \ll h \} < 2^{-9}$. Then there exists a universal constant $C > 0$ such that, for all $\delta \in (0, 2^{-9} - \nu)$ and all $\epsilon > 0$,
  \[
    H_{[]}\bigl(\epsilon, \mathcal{H}(h_0, \delta), d_{\mathrm{H}}\bigr) \leq
    C \biggl( \frac{\delta + \nu}{\epsilon} \biggr)^{1/2} \log^{5/4} \frac{1}{\delta}.
  \]
\end{lemma}
\begin{proof}
Fix $h_1 \in \mathcal{H}^{(1)}$ with $h_0 \ll h_1$ and let $\nu_1 := d_{\mathrm{H}}(h_0, h_1)$.  Then, by the triangle inequality,
  \[
    \mathcal{H}(h_0, \delta) \subseteq \mathcal{H}(h_1, \delta + \nu_1).
  \]
Since $h_1(r) = r^{p-1} e^{\phi_1(r)}$ where $\phi_1$ is affine, $ \log (h/h_1)$ is concave for any $h \in  \mathcal{H}(h_1, \delta + \nu_1)$. We therefore have
  \[
    \mathcal{H}(h_1, \delta + \nu_1) \subseteq \tilde{\mathcal{F}}(h_1, \delta + \nu_1)
  \]
  where the right-hand side is defined in~\eqref{eqn:local_density_ball}. The result then follows from Lemma~\ref{Prop:GeneralLocalBracketing}.
\end{proof}

\begin{lemma}
  \label{Lem:HellingerMomentBound}
  Let $f, g \in \mathcal{F}_1$. Then there exist universal constants $C'_\mu, C'_\sigma$ such that for all $\delta \in (0, 2^{-8}]$, if $d_{\mathrm{H}}(f,g) \leq \delta$, then
  \[
    \frac{1}{C'_\sigma} \leq \frac{\sigma_g}{\sigma_f} \leq C'_\sigma,
    \qquad
    |\mu_g - \mu_f| \leq C'_\mu. 
  \]
\end{lemma}
\begin{proof}
Since the Hellinger distance is affine invariant, we may assume without loss of generality that $\mu_f = 0$ and $\sigma_f = 1$.  By \citet[][Theorem 5.14(d)]{lovasz2007geometry}, $f(x) \geq 2^{-10}$ for $x \in [-1/9, 1/9]$. We claim that $g(x) \geq 2^{-12}$ for some $x \in [-1/9, 1/9]$. To see this, suppose for a contradiction that $g(x) \leq 2^{-12}$ for all $x \in [-1/9, 1/9]$. Then,
  \begin{align*}
    \int_{-\infty}^\infty (f^{1/2} - g^{1/2})^2 \geq \int_{-1/9}^{1/9} ( 2^{-5} - 2^{-6} )^2 \, dx \geq (2/9) 2^{-12} > 2^{-16} \geq \delta^2,
  \end{align*}
a contradiction.  By Lemma~\ref{Lem:VarianceSupRelation}, it follows that $\sigma_g \leq C_\sigma'$ for some universal constant $C'_{\sigma} > 0$.  The lower bound on $\sigma_g$ follows by symmetry.
%\blue{Now suppose for a contradiction that $g(\mu_g) \geq e^{10}$.  Then for any $x_0 > \mu_g + e^{-10}$, we have
%\[
%1 \geq \int_{\mu_g}^{x_0} \exp\biggl\{\biggl(\frac{x-\mu_g}{x_0-\mu_g}\biggr)\log g(x_0) + \biggl(\frac{x_0-x}{x_0-\mu_g}\biggr)\log g(\mu_g)\biggr\} \, dx
%\]
%Finish this.}

Now assume without loss of generality that $\mu_g \geq 0$.  By the first part and \citet[][Lemma~13]{fresen2013multivariate}, there exist universal constants $\alpha > 0$ and $\beta \in \mathbb{R}$ such that $g(x) \leq e^{- \alpha \mu_g + \beta}$ for all $x \leq 0$.  It follows that if $\mu_g \geq (\beta + 12\log 2)/\alpha$, then 
\[
\int_{-\infty}^\infty (f^{1/2} - g^{1/2})^2 \geq \int_{-1/9}^0 ( 2^{-5} - 2^{-6} )^2 \, dx \geq (1/9) 2^{-12} > 2^{-16},
\]
a contradiction.  The result follows.
%  If $p$ has mean $\mu_p$ and standard deviation $\sigma_p$, then, we use the fact that the Hellinger distance is scale and location invariant and apply our results on the densities $\sigma_p p\left( \sigma_p(x - \mu_p) \right)$, $\sigma_p q\left(\sigma_p(x - \mu_p) \right)$ to deduce that $\frac{\sigma_p}{\sigma_q} \leq c'_{\sigma}$. By symmetry, we also have $\frac{\sigma_q}{\sigma_p} \leq c'_{\sigma}$. It also follows that  $|\mu_p - \mu_q| \leq c''_\mu \sigma^{-1}_p \leq c''_\mu c^{\prime -1}_\sigma$. We complete the proof of the lemma by letting $c'_\mu = c''_\mu c^{\prime -1}_\sigma$. 
\end{proof}


We prove a general result on the local bracketing entropy of log-concave densities.
Recall from the introduction that $\mathcal{F}_p$ denotes the class of all upper semi-continuous, log-concave densities on $\mathbb{R}^p$. Let $f_0 \in \mathcal{F}_1$. We make no assumption on the support of $f_0$. We define
\begin{align}
  \tilde{\mathcal{F}}(f_0, \delta) := \bigl\{ f \in \mathcal{F}_1 \,:\, \log (f/f_0) \textrm{ concave}, f \ll f_0, d_{\mathrm{H}}(f,f_0) \leq \delta\bigr\}, \label{eqn:local_density_ball}
\end{align}
where we adopt the convention that $0/0 := 0$.  %$\tilde{\mathcal{F}}(h_0, \delta)$ is the class of densities $\delta$-close to $h_0$ in the Hellinger sense and where the log-likelihood ratio is concave. 

\begin{lemma}
  \label{Prop:GeneralLocalBracketing}
  Suppose that $\delta \in (0,2^{-9}]$ and that $f_0 \in \mathcal{F}_1$ with $\sigma_{f_0}=1$. Then, for every $\epsilon > 0$,
  \[
    H_{[]}\bigl(\epsilon, \tilde{\mathcal{F}}(f_0, \delta), d_{\mathrm{H}}\bigr) \lesssim \frac{\delta^{1/2}}{\epsilon^{1/2}} \log^{5/4} \frac{1}{\delta}.
  \]
\end{lemma}
\begin{proof} 
  In this proof, we let $C$ be a generic universal constant whose value may vary from instance to instance. Also, without loss of generality, let us suppose that $\mu_{f_0}=0$.  Since $f_0(0) \geq 2^{-7}$ \citep[][Theorem~5.14(d)]{lovasz2007geometry}, we may define $a_L := \inf \{ r \in \mathbb{R} \,:\, f_0(r) \geq \delta^2\}$ and $a_R := \sup \{ r \in \mathbb{R}\,:\, f_0(r) \geq \delta^2\}$. By Lemma~\ref{Lem:HellingerMomentBound} and \citet[][Lemma~13]{fresen2013multivariate}, there exist $\alpha > 0$ and $\beta \in \mathbb{R}$ such that for any $f \in \tilde{\mathcal{F}}(f_0, \delta)$ and $x \in \mathbb{R}$, we have $f(x) \leq e^{-\alpha |x| + \beta}$.  Let $b_L := - \frac{1}{\alpha} \log \frac{e^\beta}{\delta^4}$ and $b_R := \frac{1}{\alpha} \log \frac{e^\beta}{\delta^4}$.  Then, for any $f \in \tilde{\mathcal{F}}(f_0, \delta)$, $f(r) < \delta^4$ for $r \in (-\infty,b_L) \cup (b_R,\infty)$, and $[a_L, a_R] \subseteq [b_L, b_R]$.

First, we will bracket the region $[b_L, a_L) \cup (a_R, b_R]$.  To this end, fix $\epsilon > 0$, and let $K_L := \min \{ k \in \mathbb{N}\,:\, e^{- \alpha ( k - a_L) + \beta} \leq \delta^4 \}$ and $K_R := \min \{ k \in \mathbb{N}\,:\, e^{ - \alpha ( k + a_R)+\beta} \leq \delta^4\}$, so that $\max(K_L,K_R) \lesssim \log(1/\delta)$.  By these definitions, $a_L - K_L \leq b_L$ and $a_R + K_R \geq b_R$. We segment $[b_L, a_L)$ into subintervals $S_k$ for $k=1,\ldots,K_L$, where
\begin{align*}
  S_k &:= \bigl[a_L - k, a_L - (k-1)\bigr),  \quad k=1,\ldots,K_L-1, \\
  S_{K_L} &:= \bigl[b_L, a_L - (K_L - 1)\bigr).
\end{align*}
For $k=1,\ldots,K$, define $\epsilon^2_k := \frac{\epsilon^2}{16 K_L}$.  For any $r \in [b_L,a_L)$, we have that $f_0(r) \leq \delta^2$ because $r < a_L$ and, moreover, $e^{-\alpha|r|+\beta} \geq \delta^4$ because $r \geq b_L$. Hence, by Lemma~\ref{Lem:EnvelopeUpperBound}, $f(r) \leq f_0(r) e^{ C \delta \log \frac{1}{\delta}} \lesssim f_0(r) \leq \delta^2$ for any $f \in \tilde{\mathcal{F}}(f_0, \delta)$ and $r \in [b_L,a_L)$.  Hence, by Lemma~\ref{Prop:SegmentBracket1}, 
  \begin{align}
\label{Eq:bLaL}
    H_{[]}\biggl(\frac{\epsilon}{4}, \tilde{\mathcal{F}}(f_0, \delta), d_{\mathrm{H}}, [b_L, a_L]\biggr)
    &\leq  \sum_{k=1}^{K_L}
    H_{[]}(\epsilon_k, \tilde{\mathcal{F}}(f_0, \delta), d_{\mathrm{H}}, S_k) \nonumber \\
    & \lesssim \sum_{k=1}^{K_L} \frac{\delta^{1/2}}{\epsilon_k^{1/2}} \lesssim \frac{\delta^{1/2}}{\epsilon^{1/2}} \log^{5/4} \frac{1}{\delta}.
  \end{align}
By symmetry, we obtain the same bound for $H_{[]}\bigl(\epsilon/4, \tilde{\mathcal{F}}(f_0, \delta), d_{\mathrm{H}}, [a_R, b_R]\bigr)$.
  
Now we bracket the region $(-\infty, b_L) \cup (b_R, \infty)$.  For $k \in \mathbb{N}$, define $S_k := [b_L - k, b_L - (k-1))$ and set $\epsilon^2_k := C \epsilon^2 e^{-\alpha(k - b_L)/2}$ where $C$ is a constant chosen such that $\sum_{k=1}^\infty \epsilon^2_k = \epsilon^2/16$.  Then, 
%for $r \in S_k$ and $h \in \tilde{\mathcal{F}}(h_0, \delta)$, we have $h(r) \leq e^{ - \alpha |r|+\beta} \leq e^{- \alpha (k - b_L-1)+\beta}$ by Lemma~\ref{Lem:LocalBracketEnvelopeBound}.  Hence, 
by Lemma~\ref{Prop:SegmentBracket1} again, 
  \begin{align}
\label{Eq:bL}
H_{[]}\bigl(\epsilon/4, \tilde{\mathcal{F}}(f_0, \delta), d_{\mathrm{H}}, (-\infty, b_L)\bigr) &\leq \sum_{k=1}^\infty H_{[]}(\epsilon_k, \tilde{\mathcal{F}}(f_0, \delta), d_{\mathrm{H}}, S_k) 
    \lesssim \sum_{k=1}^\infty \frac{e^{-\alpha(k - b_L-1)/4}}{\epsilon_k^{1/2}} \nonumber \\
    &\lesssim \frac{1}{\epsilon^{1/2}}\sum_{k=1}^\infty e^{-\alpha(k - b_L-1)/8} \lesssim \frac{\delta^{1/2}}{\epsilon^{1/2}}.
  \end{align}
The same bound holds for $H_{[]}\bigl( \epsilon/4, \tilde{\mathcal{F}}(f_0, \delta), d_{\mathrm{H}}, [b_R, \infty)\bigr)$.
    
Next, we bracket the region $[a_L, -1/16]$, and recall that by Lemma~\ref{Lem:LogConcaveCentralMass}, $a_L \leq -1/9$.  To this end, we write $s_0 := a_L$ and partition $[s_0,-1/16]$ into segments $[s_0, s_1]$, $[s_1, s_2]$, $\ldots$, $[s_{J-1},s_J]$ (where $s_J = -1/16$) as follows:
  \begin{enumerate}
  \item Choose $s_1 > s_0$ such that $\int_{s_0}^{s_1} f_0(t) \, dt = 4 \delta^2$.
  \item For each $j \geq 2$, if there exists $t_0 < -1/16$ such that $\int_{s_{j-1}}^{t_0} f_0(t) \, dt \geq 2\int_{-\infty}^{s_{j-1}} f_0(t) \, dt$, then choose $s_j$ such that $\int_{s_{j-1}}^{s_j} f_0(t) \, dt = 2\int_{-\infty}^{s_{j-1}} f_0(t) \, dt$. Otherwise, set $J := j$ and choose $s_J = -1/16$.
  \end{enumerate}
Let $\phi_0(r) := \log f(r)$ and write $\mathrm{Range}_j(\phi_0) := \sup_{t \in [s_{j-1},s_j]} \phi_0(t) - \inf_{t \in [s_{j-1},s_j]} \phi_0(t)$.  We make the following six claims:
  \begin{enumerate}
  \item[(1)] $s_1 < -1/16$;
  \item[(2)] $(s_1 - s_0)\sup_{t \in [s_0, s_1]} f_0(t) \lesssim \delta^2 \log \frac{1}{\delta}$;
  \item[(3)] $\int_{s_J}^{\infty} f_0(t) \, dt > 2^{-11}$;
  \item[(4)] $(s_j - s_{j-1})\sup_{t \in [s_{j-1}, s_j]} f_0(t) \lesssim \mathrm{Range}_j(\phi_0) \int_{-\infty}^{s_{j-1}} f_0(t) \, dt$ for $j=2,\ldots,J-1$;
  \item[(5)] $\int_{-\infty}^{s_J} f_0(t) \, dt \geq 2^{-13}$;
  \item[(6)] $J \lesssim \log(1/\delta)$.
  \end{enumerate}
To verify claim~(1), observe by \citet[][Theorem~5.14(a)]{lovasz2007geometry} that $f_0(t) \geq 2^{-8}$ for all $t \in [-1/9, 1/9]$.  Hence $\int_{s_0}^{-1/16} f_0(t)\, dt \geq (1/9 - 1/16) 2^{-8} > 4\delta^2$, so $s_1 < -1/16$.

For claim~(2), note that $-2 \log (1/\delta) \leq \phi_0(t) \leq \beta$ for $t \in  [s_0, s_1]$.  Thus
  by the second part of Lemma~\ref{lem:integral_approximation} and the definition of $s_1$, we have $(s_1 - s_0) \sup_{t \in [s_0, s_1]} f_0(t) \lesssim \log (1/\delta) \int_{s_0}^{s_1} f_0(t) \, dt \lesssim \delta^2 \log (1/\delta)$.
  
  For claim (3), we have $\int_{s_j}^{\infty} f_0(t) \, dt \geq \int_{-1/16}^{1/9} f_0(t) \, dt \geq 2^{-8} (1/9 + 1/16) > 2^{-11}$.

  For claim (4), observe that $2 \int_{-\infty}^{s_{j-1}} f_0(t) \, dt = \int_{s_{j-1}}^{s_j} f_0(t) \, dt$ for $j=2,\ldots,J-1$. Hence
  \begin{align*}
    \frac{(s_j - s_{j-1}) \sup_{t \in [s_{j-1}, s_j]} f_0(t) }{\int_{-\infty}^{s_{j-1}} f_0(t) \, dt } =
    \frac{(s_j - s_{j-1}) \sup_{t \in [s_{j-1}, s_j]} f_0(t) }{2 \int_{s_{j-1}}^{s_j} f_0(t) \, dt } 
    \lesssim \mathrm{Range}_j(\phi_0),
  \end{align*}
  where the final bound follows from Lemma~\ref{lem:integral_approximation}.

  For claim (5), we have 
\begin{align*}
%\int_{-\infty}^{s_{J-1}} e^{\phi_0(t)} \, dt &= \frac{1}{3}\biggl(\int_{-\infty}^{s_J} e^{\phi_0(t)} \, dt - \int_{s_{J-1}}^{s_J} e^{\phi_0(t)} \, dt + 2\int_{-\infty}^{s_{J-1}} e^{\phi_0(t)} \, dt\biggr) \\
%&\geq \frac{1}{3}\int_{-1/9}^{-1/16} e^{\phi_0(t)} \, dt \geq \frac{2^{-8}}{3}(1/9-1/16) \geq 2^{-14}.
\int_{-\infty}^{s_J} f_0(t) \, dt \geq \int_{-1/9}^{-1/16} f_0(t) \, dt \geq 2^{-8}(1/9-1/16) \geq 2^{-13}.
\end{align*}

 Finally, for claim~(6), we have 
\[
\int_{-\infty}^{s_j} f_0(t) \, dt = \int_{-\infty}^{s_{j-1}} f_0(t) \, dt + \int_{s_{j-1}}^{s_j} f_0(t) \, dt = 3  \int_{-\infty}^{s_{j-1}} f_0(t) \, dt
\]
for $j=2,\ldots,J-1$. Since $\int_{-\infty}^{s_1} f_0(t) \, dt \geq \int_{s_0}^{s_1} f_0(t) \, dt = 2\delta^2$ by definition of $s_1$ and since $\int_{-\infty}^\infty f_0(t) \, dt = 1$, we conclude that $J \lesssim \log(1/\delta)$.

  Now set $\tilde{\epsilon} := \epsilon/(2J^{1/2})$.  Then, by Lemma~\ref{Prop:SegmentBracket1} and claim~(2),
  \begin{align}
\label{Eq:s0}
    H_{[]}\bigl(\tilde{\epsilon}, \tilde{\mathcal{F}}(f_0, \delta), d_{\mathrm{H}}, [s_0, s_1]\bigr)
    &\lesssim \frac{(s_1 - s_0)^{1/4}}{\tilde{\epsilon}^{1/2}}\sup_{e^\phi \in \tilde{\mathcal{F}}(f_0, \delta) }\sup_{t \in [s_0, s_1]} e^{\phi(t)/4} \nonumber \\
   % &\lesssim \frac{(s_1 - s_0)^{1/4}}{\tilde{\epsilon}^{1/2}}
   %   \sup_{t \in [s_0, s_1]} e^{\phi_0(t)/4}\sup_{e^\phi \in \tilde{\mathcal{F}}(h_0, \delta) }\sup_{t \in [s_0, s_1]} e^{(\phi(t) - \phi_0(t))/4} \\
    &\lesssim \frac{\delta^{1/2}}{\tilde{\epsilon}^{1/2}} \log^{1/4}(1/\delta)\sup_{e^\phi \in \tilde{\mathcal{F}}(f_0, \delta) }\sup_{t \in [s_0, s_1]} e^{(\phi(t) - \phi_0(t))/4} \nonumber \\
    &\lesssim  \frac{\delta^{1/2}}{\epsilon^{1/2}} \log^{1/2} \frac{1}{\delta},
  \end{align}
where the final bound follows from Lemma~\ref{Lem:EnvelopeUpperBound}.

Now let $j \in \{2,\ldots,J\}$, and observe by claim~(1) that $s_1,\ldots,s_J$ are strictly increasing.  Let $\check{\mathcal{F}}(f_0,\delta) := \bigl\{e^{\phi - \phi_0}:e^\phi \in \tilde{\mathcal{F}}(f_0,\delta)\bigr\}$.  If $\{(\check{\psi}_j^L,\check{\psi}_j^U):j=1,\ldots,N\}$ is an $\tilde{\epsilon}$-Hellinger bracketing set for $\check{\mathcal{F}}(f_0,\delta)$ with $\log N = H_{[]}\bigl(\epsilon,\check{\mathcal{F}}(f_0,\delta),d_{\mathrm{H}},[s_{j-1},s_j]\bigr)$, then we can define $\{(\tilde{\psi}_j^L,\tilde{\psi}_j^U):j=1,\ldots,N\}$ by $\tilde{\psi}_j^L := f_0 \check{\psi}_j^L$ and $\tilde{\psi}_j^U := f_0 \check{\psi}_j^U$.  Then
\[
d_{\mathrm{H}}(\tilde{\psi}_j^L,\tilde{\psi}_j^U) \leq \sup_{t \in [s_{j-1},s_j]} f_0(t)^{1/2} d_{\mathrm{H}}(\check{\psi}_j^L,\check{\psi}_j^U) \leq \tilde{\epsilon} \sup_{t \in [s_{j-1},s_j]} f_0(t)^{1/2}.
\]
Moreover, on $[s_{j-1}, s_j]$, the conditions of Lemma~\ref{Lem:EnvelopeLowerBound} are fulfilled because $\int_{-\infty}^{s_{j-1}} f_0(t)\, dt \geq \int_{s_0}^{s_1} f_0(t)\, dt = 4\delta^2$ and $\int_{s_j}^\infty f_0(t)\, dt > 2^{-11} \geq 4\delta^2$ by claim~(3). Thus, we may combine Lemmas~\ref{Lem:EnvelopeLowerBound} and~\ref{Lem:EnvelopeUpperBound} with claim~(4) and Lemma~\ref{Prop:SegmentBracket2} to obtain
  \begin{align}
\label{Eq:KeyDisplay}
   & H_{[]}\bigl(\tilde{\epsilon}, \tilde{\mathcal{F}}(f_0, \delta), d_{\mathrm{H}}, [s_{j-1}, s_j]\bigr) \nonumber \\
&\leq H_{[]}\biggl(\frac{\tilde{\epsilon}}{\sup_{t \in [s_{j-1},s_j]} e^{\phi_0(t)/2}}, \check{\mathcal{F}}(f_0, \delta), d_{\mathrm{H}}, [s_{j-1}, s_j]\biggr) \nonumber \\
&\lesssim \frac{\sup_{t \in [s_{j-1},s_j]} e^{\phi_0(t)/4}}{\tilde{\epsilon}^{1/2}}\biggl[|s_{j-1}|\delta + \frac{\delta}{\bigl\{\int_{-\infty}^{s_{j-1}} f_0(t) \, dt \bigr\}^{1/2}}\biggr]^{1/2}(s_j - s_{j-1})^{1/4}\sup_{t \in [s_{j-1},s_j]}e^{(\phi(t)-\phi_0(t))/4} \nonumber \\
& \lesssim \frac{\delta^{1/2}}{\tilde{\epsilon}^{1/2}}\mathrm{Range}_j^{1/4}(\phi_0)\biggl[|s_{j-1}|^{1/2} \biggl\{\int_{-\infty}^{s_{j-1}} f_0(t) \, dt \biggr\}^{1/4} + 1\biggr] \lesssim \frac{\delta^{1/2}}{\tilde{\epsilon}^{1/2}}\mathrm{Range}_j^{1/4}(\phi_0),
  \end{align}
where the final bound follows because $s_{j-1}^2 \int_{-\infty}^{s_{j-1}} f_0(t) \, dt  \leq 1$ by Markov's inequality.  

By symmetry, we obtain the same bracketing entropy bound over $[1/16, a_R]$.  For the region $[-1/16, 1/16]$, since $\int_{-\infty}^{-1/16} f_0(t) \,dt \geq 2^{-13}$ and $\int_{1/16}^\infty f_0(t) \,dt \geq 2^{-13}$ by claim~(5), so arguing as in~\eqref{Eq:KeyDisplay}, we obtain
\begin{equation}
\label{Eq:1/16}
 H_{[]}\biggl( \frac{\epsilon}{2}, \tilde{\mathcal{F}}(f_0, \delta), d_{\mathrm{H}}, [-1/16, 1/16] \biggr) \lesssim \frac{\delta^{1/2} }{\epsilon^{1/2}}.
\end{equation}
Now, by Jensen's inequality and the fact that $\phi_0$ is unimodal,
\begin{equation}
\label{Eq:Jensen}
\sum_{j=2}^J \mathrm{Range}_j^{1/4}(\phi_0) \lesssim J \biggl(\frac{\log(1/\delta)}{J}\biggr)^{1/4} \lesssim \log(1/\delta). 
\end{equation}
We conclude from~\eqref{Eq:Jensen},~\eqref{Eq:1/16},~\eqref{Eq:KeyDisplay},~\eqref{Eq:s0},~\eqref{Eq:bL},~\eqref{Eq:bLaL} and claim~(6) that
  \begin{align*}
    & H_{[]} \bigl( \epsilon, \tilde{\mathcal{F}}(f_0, \delta), d_{\mathrm{H}} \bigr) \\
    &\leq H_{[]}\biggl( \frac{\epsilon}{2}, \tilde{\mathcal{F}}(f_0, \delta), d_{\mathrm{H}}, [-1/16, 1/16] \biggr) + 2\sum_{j=1}^J H_{[]}\biggl( \frac{\epsilon}{2J^{1/2}}, \tilde{\mathcal{F}}(f_0, \delta), d_{\mathrm{H}}, [s_{j-1}, s_J] \biggr) \\
&\hspace{2cm}+ 2 H_{[]}\biggl( \frac{\epsilon}{4}, \tilde{\mathcal{F}}(f_0, \delta), d_{\mathrm{H}}, [b_L,a_L] \biggr) + 2H_{[]}\biggl( \frac{\epsilon}{4}, \tilde{\mathcal{F}}(f_0, \delta), d_{\mathrm{H}}, (-\infty, b_L) \biggr)
\\
  & \lesssim \frac{\delta^{1/2}}{\epsilon^{1/2}} \log^{5/4}\frac{1}{\delta},
  \end{align*}
as required.
\end{proof}

%\begin{lemma}
%  \label{Lem:LocalBracketEnvelopeBound}
%  Let $h_0 \in \mathcal{F}_1$ with $\mu_{h_0}=0$ and $\sigma_{h_0}=1$, and suppose that $\delta \in (0,2^{-14}]$. Then there exist universal constants $C_\mu > 0$ and $C_\sigma > 0$ such that for any $h \in \tilde{\mathcal{F}}(h_0, \delta)$, we have
%  \[
%    |\mu_h| \leq C_\mu \qquad
%    \frac{1}{C_\sigma} \leq \sigma_h \leq C_\sigma.
%  \]
%Moreover, there exist universal constants $e^\beta \geq 1, 1 \geq a_1 > 0$ such that for any $h \in \tilde{\mathcal{F}}(h_0, \delta)$,
%  \[
%    h(r) \leq e^\beta e^{- a_1|r|}
%  \]
%\end{lemma}
%
%\begin{proof}
%The first claim follows from lemma~\ref{Lem:HellingerMomentBound}. The second claim then immediately follows. 
%\end{proof}
\begin{lemma}
  \label{Lem:EnvelopeLowerBound}
  Let $e^{\phi} \in \tilde{\mathcal{F}}(e^{\phi_0}, \delta)$ for some concave $\phi_0:\mathbb{R} \rightarrow [-\infty,\infty)$ and $\delta \in (0,\infty)$. Let $r \in \mathbb{R}$ be such that $\int_{-\infty}^r e^{\phi_0(t)} \, dt \wedge \int_r^{\infty} e^{\phi_0(t)} \, dt \geq 4\delta^2$. Then

  \[ 
    \frac{\phi(r) - \phi_0(r)}{2} \geq
    - \frac{2\delta}{\bigl\{ \int_{-\infty}^r e^{\phi_0(t)} \, dt \wedge \int_r^{\infty} e^{\phi_0(t)} \, dt \bigr\}^{1/2}}.
  \]
\end{lemma}
\begin{proof}
As a shorthand, let us write $\psi := (\phi - \phi_0)/2$.  Now fix $r \in \mathbb{R}$ and assume without loss of generality that $\psi(r) < 0$ (because otherwise the result is immediate).  Since $\psi$ is concave, $\psi(t) \leq \psi(r)$ either for $t \in (-\infty, r)$ or for $t \in (r, \infty)$.  In the former case,
  \[
    \delta^2 \geq \int_{-\infty}^\infty e^{\phi_0(t)} (1 - e^{\psi(t)})^2 \, dt \geq (1 - e^{\psi(r)})^2 \int_{-\infty}^r e^{\phi_0(t)} \, dt.
  \]
Hence
\[
   \psi(r) \geq \log \biggl(1 -   \frac{\delta}{\bigl\{\int_{-\infty}^r e^{\phi_0(t)} \, dt \bigr\}^{1/2}} \biggr) \geq - \frac{2\delta}{\bigl\{\int_{-\infty}^r e^{\phi_0(t)} \, dt\bigr\}^{1/2}}.
\]
On the other hand, if $\psi(t) < \psi(r)$ for $t \in (r, \infty)$, we can apply an almost identical argument to see that
  \[
    \psi(r) \geq -\frac{2\delta}{\bigl\{ \int_r^{\infty} e^{\phi_0(t)} \, dt \bigr\}^{1/2}},
  \]
as required.
\end{proof}

\begin{lemma}
  \label{Lem:EnvelopeUpperBound}
  Let $f_0 = e^{\phi_0} \in \mathcal{F}_1$ with $\mu_{f_0}=0$ and $\sigma_{f_0}=1$, and let $e^{\phi} \in \tilde{\mathcal{F}}(f_0, \delta)$ for some $\delta \in (0,2^{-9})$.  Then there exists a universal constant $C > 0$ such that for any $r \in \mathbb{R}$,
  \[
    \frac{\phi(r) - \phi_0(r)}{2} \leq C (|r|+1) \delta. 
  \]
\end{lemma}
\begin{proof}
Again, we write $\psi := (\phi - \phi_0)/2$.  Since we seek an upper bound for $\psi$, we may assume without loss of generality that $\psi$ is upper semi-continuous, and by symmetry, it suffices to prove the bound at a fixed $r \geq 0$.  Further, we assume without loss of generality that $\psi(r) > 0$ (because otherwise the result is immediate).  

Let $r_0 := r + 1$.  Define $S := \bigl\{ t \in [-r_0, r_0] \,:\,  \psi(t) \geq \frac{\psi(r)}{2^{18}c_1r_0} \bigr\}$, $s_L := \inf \{ t:t \in S \}$, and $s_R := \sup \{ t:t \in S \}$.  We note that $r \in S$ since $2^{18}c_1r_0 \geq 2$. Then, since $e^x - 1 \geq x$ for any $x \geq 0$, we have
  \begin{align}
    \delta^2 &\geq  \int_{s_L}^{s_R} e^{\phi_0(t)} (e^{\psi(t)} - 1)^2 \, dt \geq  \int_{s_L}^{s_R} e^{\phi_0(t)} \psi(t)^2  \, dt \geq  \left(\frac{\psi(r)}{2^{18}c_1r_0}\right)^2
               \int_{s_L}^{s_R} e^{\phi_0(t)} \, dt.
                \label{eqn:upper_bound1}
  \end{align}
Now, define $S' := \bigl\{ t \in [-r_0, r_0] \,:\, \psi(t) \geq - \frac{\psi(r)}{2^{18}c_1r_0} \bigr\}$, $s'_L := \inf \{t: t \in S' \}$, and $s'_R := \sup \{t: t \in S' \}$. Then 
  \begin{align*}
    \delta^2 \geq \int_{[-r_0, r_0]\setminus [s'_L, s'_R]} e^{\phi_0(t)} ( 1 - e^{\psi(t)})^2 \, dt \geq \bigl\{ 1 - e^{-\frac{\psi(r)}{2^{18}c_1r_0}} \bigr\}^2 \int_{[-r_0, r_0]\setminus [s'_L, s'_R]} e^{\phi_0(t)} \, dt.
  \end{align*}
If $\int_{[-r_0, r_0]\setminus [s'_L, s'_R]} e^{\phi_0(t)} \, dt > 0$, then, by rearranging terms, we have
  \begin{align}
    \psi(r) \leq   -  2^{18}c_1r_0
    \log \biggl(1-  \frac{\delta}{ \bigl\{ \int_{[-r_0, r_0]\setminus [s'_L, s'_R]} e^{\phi_0(t)} \, dt \bigr\}^{1/2}} \biggr). \label{eqn:upper_bound2}
  \end{align}
 As a shorthand, let us define
  \begin{align*}
    T_1 &:= \int_{s_L}^{s_R} e^{\phi_0(t)} \, dt, \quad
    T_2 := \int_{s'_L}^{s_L} e^{\phi_0(t)} \, dt +  \int_{s_R}^{s'_R} e^{\phi_0(t)} \, dt, \\
    T_3 &:= \int_{-r_0}^{s'_L} e^{\phi_0(t)} \, dt + \int_{s'_R}^{r_0} e^{\phi_0(t)} \, dt.
  \end{align*}
Inequalities~\eqref{eqn:upper_bound1} and \eqref{eqn:upper_bound2} yield
  \begin{align}
    \psi(r) &\leq  2^{18}c_1r_0 \frac{ \delta}{ T_1^{1/2} } \label{Eqn:PsiUpperBound1} \\
    \psi(r) &\leq - 2^{18}c_1r_0 \log \biggl( 1 - \frac{\delta}{T_3^{1/2}} \biggr) \quad \text{ if $T_3 > 0$}. \label{Eqn:PsiUpperBound2}
  \end{align}
 Since $T_1 + T_2 + T_3 = \int_{-r_0}^{r_0} e^{\phi_0(t)} \, dt$, we have that $T_1 + T_2 + T_3 \geq 2^{-14}$ from the proof of Lemma~\ref{Lem:LogConcaveCentralMass}.

We claim that $T_2 \leq 2^{-15}$.  To see this, note that by concavity of $\psi$ (cf.~the proof of Theorem~1 of \citet{CSS2010}),
  \begin{align*}
    s'_R - s'_L \leq (s_R - s_L) \frac{ 1 + \frac{1}{2^{18}c_1r_0}}
      {1 - \frac{1}{2^{18}c_1r_0}} \leq (s_R - s_L) \biggl( 1 + \frac{4}{2^{18}c_1r_0} \biggr).
  \end{align*}
Hence
  \begin{align*}
    T_2 = \int_{[s'_L, s_L] \cup [s_R, s'_R]} e^{\phi_0(t)} \, dt \leq c_1( s'_R - s_R + s_L - s'_L) \leq (s_R - s_L) \frac{4}{2^{18} r_0 } \leq 2^{-15}.
  \end{align*}
It follows that either $T_1 \geq 2^{-16}$ or $T_3 \geq 2^{-16}$. If $T_1 \geq 2^{-16}$, then the result follows from~\eqref{Eqn:PsiUpperBound1}.  On the other hand, if $T_3 \geq 2^{-16}$, then from~\eqref{Eqn:PsiUpperBound2}, we have $\psi(r) \leq 2^{19} r_0 \delta$, which again proves the claim.
\end{proof}

\begin{lemma}
  \label{Lem:LogConcaveCentralMass}
  Let $f \in \mathcal{F}_1$ with $\mu_f = 0$ and $\sigma_f=1$. Let $\delta \in (0,2^{-4}]$. Then $a_L := \inf \{ r \,:\, f(r) \geq \delta^2\} \leq -1/9$, $a_R := \inf \{ r \,:\, f(r) \geq \delta^2\} \geq 1/9$, and
  \begin{equation}
\label{Eq:Int}
    \int_{a_L}^{a_R} f(r) \, dr \geq 2^{-14}.
  \end{equation}
\end{lemma}
\begin{proof}
By \citet[][Theorem~5.14(d)]{lovasz2007geometry}, $2^{-7} \leq f(0) \leq 2^4$ and $f(r) \geq 2^{-8}$ for all $r \in [-1/9, 1/9]$. Since log-concave functions are unimodal, we conclude that $a_L \leq -1/9$ and $a_R \geq 1/9$.  Now define $\alpha := \log f(0)$ and $\beta := \log f(a_L)$.  Then 
  \begin{align*}
    \int_{a_L}^0 f(r) \, dr
    & = |a_L| \int_0^1 f( (1 - s) a_L ) \, d s \geq |a_L| \int_0^1 e^{s\alpha + (1-s) \beta} \, d s \\
    %& = |a_L| e\int_0^1 e^\alpha e^{- s(\alpha - \beta)} ds \\
    &= |a_L| e^{\beta} \frac{1}{\alpha - \beta} (e^{\alpha - \beta}-1) \geq (1/9) 2^{-8} \frac{1}{\alpha - \beta} (e^{\alpha - \beta}-1).
  \end{align*}
Now, by \citet[][Theorems~5.14(b) and~5.14(d)]{lovasz2007geometry} $\alpha - \beta \geq -7 \log 2 - 9 \log 2 = -16\log 2$, so we deduce that 
\[
\int_{a_L}^0 f(r) \, dr \geq (1/9) 2^{-8} \frac{1}{16 \log 2}(1-2^{-16}) \geq 2^{-15}.
\]
By symmetry, the same bound holds for $\int_0^{a_R} f(r) \, dr$. The lemma follows.
\end{proof}

\begin{lemma}
  \label{lem:integral_approximation}
Let $\phi_0:\mathbb{R} \rightarrow [-\infty,\infty)$ be concave. Let $a < b$ and fix an arbitrary $t^* \in [a, b]$.  Let
\[
q(s) := \left\{ \begin{array}{ll} (1 - e^{-s})/s & \mbox{if $s \neq 0$} \\
1 & \mbox{if $s = 0$.} \end{array} \right.
\]
Then
\begin{align*}
    \int_a^b e^{\phi_0(t)} \, dt \geq (t^*-a) e^{\phi_0(t^*)} q\bigl(\phi_0(t^*) - \phi_0(a)\bigr) + (b - t^*) e^{\phi_0(t^*)}q\bigl(\phi_0(t^*) - \phi_0(b)\bigr).
  \end{align*}
Moreover, if $\max\{\phi_0(a), \phi_0(b)\} \leq \phi_0(t^*) \leq \max\{\phi_0(a), \phi_0(b)\} + \tau$ for some $\tau \geq \log 2$, then
  \[
  \int_a^b e^{\phi_0(t)} \, dt \geq (b-a) e^{\phi_0(t^*)} \frac{1}{2 \tau}.
  \]
\end{lemma}
\begin{proof}
  Let us first suppose that $t^* > a$. We have $\phi_0(t) \geq s \phi_0(a) + (1 - s) \phi_0(t^*)$ for $t \in [a,t^*]$, where $s := \frac{t^* - t}{t^* - a}$.  Hence
  \[
    \int_a^{t^*} e^{\phi_0(t)} \, dt \geq (t^* - a) \int_0^1 e^{ s \phi_0(a) + (1 - s) \phi_0(t^*)} \, ds \geq (t^*- a)e^{\phi_0(t^*)}q\bigl(\phi_0(t^*) - \phi_0(a)\bigr).
  \]
We can bound $\int_{t^*}^b e^{\phi_0(t)} dt$ when $t^* < b$ by a similar argument to yield the first conclusion. 

For the second part, observe that $q$ is strictly decreasing, so from the first part,
\[
\int_a^{t^*} e^{\phi_0(t)} \, dt \geq (b-a)e^{\phi_0(t^*)}q(\tau) \geq (b-a)e^{\phi_0(t^*)}\frac{1}{2\tau},
\]
for $\tau \geq \log 2$.
\end{proof}

The following two lemmas are from \citet{kim2016adaptation}, though the first is only a minor restatement of \citet[][Theorem~4.1]{doss2016global}. For $a < b$ and $-\infty \leq B_1 < B_2 < \infty$, we define $\tilde{\mathcal{F}}([a,b], B_1, B_2)$ to be the set of log-concave functions $f:[a,b] \rightarrow [e^{B_1},e^{B_2}]$. 
\begin{lemma}
  \label{Prop:SegmentBracket1}
There exists a universal constant $C > 0$ such that for every $a < b$ and $B, \epsilon > 0$,
\[
H_{[]}( \epsilon, \tilde{\mathcal{F}}([a,b],-\infty, B), d_{\mathrm{H}}, [a,b]) \leq C \frac{ e^{B/4} (b-a)^{1/4}}{\epsilon^{1/2}}.
\]
\end{lemma}
\begin{lemma}
  \label{Prop:SegmentBracket2}
There exists a universal constant $C > 0$ such that 
\[
H_{[]}( \epsilon, \tilde{\mathcal{F}}([a,b],B_1,B_2), d_{\mathrm{H}}, [a,b]) \leq C (B_2 - B_1)^{1/2} \frac{ e^{B_2/4} (b-a)^{1/4}}{\epsilon^{1/2}}.
\]
for every $\epsilon > 0$, $a < b$ and $-\infty \leq B_1 \leq B_2 < \infty$. 
\end{lemma}

\noindent \textbf{Acknowledgements}: The second author would like to thank Sharmodeep Bhattacharya and Richard Lockhart for helpful conversations.  The research of the second author is supported by EPSRC fellowship EP/J017213/1 and a grant from the Leverhulme Trust RG81761.


\bibliographystyle{dcu}
\bibliography{refs}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:


\begin{align}
  \mathcal{H}(h_0, C_\mu, c_{\sigma}) = \left\{ h,\, \,:\, \int h(r) dr = 1,\, h \textrm{ log-concave},\,
  |\mathbb{E}_h - \mathbb{E}_{h_0}| \leq C_\mu,\, \frac{1}{C_\sigma} \leq \textrm{Var}_{h} \leq C_\sigma \right\}
  \label{eqn:moment_bounded_class}
\end{align}

\begin{lemma}
  \[
    \sup_{h \in \mathcal{H}(h_0, C_\mu, C_\sigma)} h(r) \leq \left\{
      \begin{array}{cc}
        C_0 C_\sigma  & \textrm{ if $r \in [\mu_{h_0} - C_\mu, \mu_{h_0} + C_\mu]$} \\
        C_0 C_\sigma \exp( - \frac{a}{c_{\sigma}} |r - (\mu_{h_0} - C_\mu)|)  & \textrm{ if $r < \mu_{h_0} - C_\mu$} \\
        C_0 C_\sigma \exp( - \frac{a}{c_{\sigma}} |r - (\mu_{h_0} + C_\mu)|)  & \textrm{ if $r > \mu_{h_0} - C_\mu$}
      \end{array} \right.
  \]
  where $C_0, a$ are absolute constants.
\end{lemma}

\begin{proof}

Let $\mathcal{H}_{\mu, \sigma}$ be the set of univariate log-concave densities with mean $\mu$ and variance $\sigma^2$.
By \citet[Theorem 2]{kim2016global}, there exists $a_0, C_0$
such that $\sup_{h \in \mathcal{H}_{\mu, \sigma}} h(x) \leq \frac{C_0}{\sigma} \exp\left( - a_0 \left| \frac{x - \mu}{\sigma} \right| \right)$. 

Therefore,
\begin{align*}
  \sup_{h \in \mathcal{H}(h_0, C_\mu, C_\sigma)} h(r) &\leq
  \sup_{\mu \in [\mu_{h_0} - C_\mu,\, \mu_{h_0} + C_\mu]}
                                                        \sup_{\sigma \in [\frac{1}{C_\sigma},\, C_\sigma]} \sup_{h \in \mathcal{H}_{\mu, \sigma} } h(r) \\
    &\leq  \sup_{\mu \in [\mu_{h_0} - C_\mu,\, \mu_{h_0} + C_\mu]}
      \sup_{\sigma \in [\frac{1}{C_\sigma},\, C_\sigma]}
      \frac{C_0}{\sigma} \exp\left(
             - a_0 \left| \frac{x - \mu}{\sigma} \right| \right)
\end{align*}

The result follows readily. 
\end{proof}

We can derive the bracketing entropy from the envelope bound.
\begin{lemma}
  \label{Lem:NonlocalBracketingEntropy}

  \[
    H_{[]}( \epsilon, \mathcal{H}(h_0, C_\mu, c_{\sigma}), d_{\mathrm{H}}) \leq \frac{C}{\epsilon^{1/2}}
  \]

  where $C$ depends only on $C_\mu, C_\sigma$.
\end{lemma}

\begin{proof}
Let $\epsilon > 0$ be fixed. Suppose $\epsilon \leq C_0 C_\sigma$.
  
  We construct the bracketing by dividing the region $\mathbb{R}^+$ into segments.

  Let $J = \lfloor \mu_{h_0} - C_\mu \rfloor$.
  \begin{align*}
    S_0 &= [\mu_{h_0} - C_\mu,\, \mu_{h_0} + C_\mu] \\
    S^L_j &= [(\mu_{h_0} - C_\mu) - j, (\mu_{h_0} - C_\mu) - (j - 1)] \\
    S^R_j &= [(\mu_{h_0} + C_\mu) + (j-1), (\mu_{h_0} + C_\mu) + j] 
  \end{align*}

  where $S^R_j$ is defined for $j=1,...,\infty$ and $S^L_j$ is defined for $j=1,...,J+1$ where $S^L_{J+1} = [0, (\mu_{h_0} - C_\mu) - J]$.

  Let $\mathcal{F}([a,b], -\infty, B)$ be the set of log-concave functions $f$ such that $f$ is supported on $[a,b]$ and that $ \log f(x) \leq B$. On $S^R_j \cup S^L_j$, $h(r) \leq e^{B_j} \equiv C_0 C_\sigma e^{- \frac{a_0}{C_\sigma} (j-1)}$ for any $h \in \mathcal{H}(h_0, C_\mu, c_{\sigma})$. Therefore, any $h \in \mathcal{H}(h_0, C_\mu, C_\sigma)$, when restricted to $S^R_j$, lies in $\mathcal{F}(S^R_j, -\infty, B_j)$; likewise for $S^L_j$. 

  Then, we have that, for any $\epsilon_0, ..., \epsilon_j, ...$ that satisfy $\epsilon_0^2 + \sum_{j=0}^\infty 2 \epsilon_j^2 = \epsilon^2$,
  \begin{align*}
    H_{[]}(\epsilon, \mathcal{H}(h_0, C_\mu, C_\sigma), d_{\mathrm{H}}) \leq &
    H_{[]}(\epsilon_0, \mathcal{F}(S_0, -\infty, B_0), d_{\mathrm{H}}, S_0) + \\ 
    & \sum_{j=1}^\infty H_{[]}(\epsilon_j, \mathcal{F}(S^R_j, -\infty, B_j), d_{\mathrm{H}}, S^R_j) + \\
    & \sum_{j=1}^{J+1} H_{[]}(\epsilon_j, \mathcal{F}(S^L_j, -\infty, B_j), d_{\mathrm{H}}, S^L_j)
  \end{align*}

  Define $\epsilon_j^2 = c' \exp\left( - \frac{a_0}{C_\sigma} \frac{1}{2} (j-1) \right) \epsilon^2$ and $\epsilon_0^2 = c' \epsilon^2$ where $c'$ ensures that $c' + 2c' \sum_{j=1}^\infty \exp\left( - \frac{a_0}{C_\sigma} \frac{1}{2} (j-1) \right)$ sums to 1.  
  
  By Corollary~\ref{Cor:SegmentBracket}, for $j=1,...,\infty$,
  \begin{align*}
    H_{[]}(\epsilon_j, \mathcal{F}(S^R_j, -\infty, B_j), d_{\mathrm{H}}, S^R_j) &\leq
                                                                       C (2 C_0 C_\sigma)^{1/4} e^{ - \frac{a_0}{C_\sigma 4} (j-1)} \frac{1}{\epsilon_j^{1/2}} \\
     &\leq  C (2 (1/c') C_0 C_\sigma)^{1/4} e^{ - \frac{a_0}{C_\sigma 8} (j-1)} \frac{1}{\epsilon^{1/2}}
  \end{align*}
  And likewise for $S^L_j$.

  \begin{align*}
    H_{[]}(\epsilon_j, \mathcal{F}(S_0, -\infty, B_0), d_{\mathrm{H}}, S_0) &\leq
                                                                   C (2 C_0 C_\sigma)^{1/4}  \frac{1}{\epsilon_0^{1/2}} \\
     &\leq      C (2 (1/c') C_0 C_\sigma)^{1/4}  \frac{1}{\epsilon^{1/2}}
  \end{align*}

  The result directly follows. 
\end{proof}











In more detail, for a subset $K$ of $\mathbb{R}^d$ with the origin belonging to its interior, we define the Minkowski functional $\mu_K:\mathbb{R}^d \rightarrow [0,\infty)$ by
\[
\mu_K(x) := \inf\{t > 0: x \in tK\}.
\]
We then consider the class of densities $f \in \mathcal{F}_p$ of the form $f(x) = g\bigl(\mu_K(x)\bigr)$ for some $g:[0,\infty) \rightarrow [0,\infty)$.  Identifiability can be ensured, for instance, by fixing the $p$-dimensional Lebesgue measure of $K$.  It turns out (cf.~Proposition~\ref{Prop:NecSuff} below) that $f \in \mathcal{F}_p$ if and only if we have both that $g$ is log-concave and decreasing, and that $K$ is convex.  This observation 

This class has a one-to-one correspondence with the set of $\nu$-spherical densities introduced by \citet{fernandez1995modeling}.  



% Let the true density be of the form: $h_0(r) = r^{p-1} e^{b_0}$ for $r \leq r_0$ and $h_0(r) = 0$ otherwise.

% Let us also assume that $\sigma^2_{h_0} = 1$. Simple calculation yields that
% \begin{align*}
%   e^{b_0} &= \frac{p}{r_0^p} \\
%   r_0 &= \sqrt{ \frac{p+2}{p}} (p + 1)\\
%   \mu_{h_0} &= r_0 \frac{p}{p+1}
% \end{align*}


% Define
% \begin{align*}
%   \mathcal{H}(h_0, \delta, C_\mu, C_\sigma) = \Big\{ h \,:\, & h(r) = r^{p-1} e^{\phi(r)},\, \phi \in \Phi, \\
%                                                          & h << h_0,\, \int (\sqrt{h(r)} - \sqrt{h_0(r)})^2 dr \leq \delta^2, \\
%                                 &  |\mu_h - \mu_{h_0}| \leq C_\mu,\, \frac{1}{C_\sigma} \leq \sigma^2_h \leq C_\sigma \Big\}
% \end{align*}

% \begin{proposition}

%   Let $h \in \mathcal{H}(h_0, \delta, C_\mu, C_\sigma)$ and be of the form $h(r) = r^{p-1} e^{\phi(r)}$. Then, we have that, for any $r \in [0, r_0]$,
%   \begin{align*}
%     \phi(r) - b_0 &\leq \delta \left( \frac{r_0}{r} \right)^{p/2} \\
%     \phi(r) - b_0 &\geq \delta \frac{1}{\left( 1 - \left(\frac{r}{r_0}\right)^p \right)^{1/2} }
%   \end{align*}

%   Furthermore, we have that
%   \[
%     e^{ \phi(r) - b_0} \leq e^{- a_1 | r - r_0| + b_1 } \left( \frac{r_0}{r} \right)^p \frac{r}{p}
%   \]
%   for some absolute constant $a_1, b_1$.
  
% \end{proposition}

% \begin{proof}
%   \begin{align*}
%     \int (\sqrt{h(r)} - \sqrt{h_0(r)})^2 dr &\leq \delta^2  \quad (\Rightarrow) \\
%     \int_0^{r_0} e^{b_0}  r^{p-1} \left( e^{ \frac{\phi(r) - b_0}{2}} - 1 \right)^2 dr &\leq \delta^2 \quad (\Rightarrow) \\
%     \int_0^{r_0} e^{b_0} r^{p-1} ( \phi(r) - b_0)^2 dr \leq 4 \delta^2 
%   \end{align*}

%   Fix $r$. Suppose $\phi(r) - b_0 = M$ at $r$. Then, since $\phi(r) - b_0$ is decreasing, we have that
%   \begin{align*}
%     \int_0^r e^{b_0} t^{p-1} M^2 dt &\leq \delta^2 \\
%     M^2 \frac{r^p}{p} e^{b_0} & \leq \delta^2 \\
%     M^2 &\leq \delta^2 \left( \frac{r_0}{r} \right)^p \\
%     M & \leq \delta \left( \frac{r_0}{r} \right)^{p/2}
%   \end{align*}

%   Likewise, suppose $\phi(r) - b_0 = -M$ at $r$. 

%   \begin{align*}
%     \int_r^{r_0} M^2 t^{p-1} e^{b_0} dt &\leq \delta^2 \\
%     M^2 e^{b_0} \left( \frac{r_0^p}{p} - \frac{r^p}{p} \right) &\leq \delta^2 \\
%     M^2 \left( 1 - e^{b_0} \frac{r^p}{p} \right) &\leq \delta^2 \\
%     M^2 \left( 1 - \left( \frac{r}{r_0} \right)^p \right) &\leq \delta^2 \\
%     M &\leq \delta \frac{1}{\left( 1 - \left( \frac{r}{r_0} \right)^p \right)^{1/2} }
%   \end{align*}

%   Lastly, we have that $h(r) = r^{p-1} e^{\phi(r)} \leq e^{- a_1 | r - r_0 | + b_1}$ for some constant $a_1, b_1$. [TODO:reference previous results]

%   \begin{align*}
%     e^{\phi(r) - b_0} &\leq e^{- a_1 | r - r_0| + b_1} r^{-(p-1)} e^{-b_0} \\
%                       &\leq e^{-a_1 |r - r_0| + b_1} \left( \frac{r_0}{r} \right)^p \frac{r}{p}
%   \end{align*}
  
% \end{proof}








%%% COMMENT
% \section{Envelope bounds}

% We define the following:
% \begin{align}
%   \Phi &\equiv \{ \phi: \mathbb{R}^+ \rightarrow \mathbb{R} \,:\, \textrm{ decreasing, concave} \} \label{defn:Phi} \\
%   \mathcal{G} &\equiv \{ e^{\phi} \,:\, \phi \in \Phi \}\\
%   \mathcal{H} &\equiv \left\{ h(r) \,:\, h(r) = c_p r^{p-1} e^{\phi(r)}, \, \phi \in \Phi,\, \int h(r) dr = 1,\, \int r^2 h(r) dr = p \right\}
%                  \label{defn:H_second_moment}
% \end{align}

% Thus $\mathcal{H}$ consists of densities of random variables $\|X\|$, where $X$ has a spherically symmetric, log-concave density on $\mathbb{R}^p$, and $\mathbb{E}(\|X\|^2) = p$.

% The following result provides crude upper bounds for $\mathcal{H}$.
% \begin{lemma}
% \label{Lemma:Crude}
% For all $r \in [0,\infty)$, we have
% \begin{equation}
% \label{Eq:ThreeBounds}
% \sup_{h \in \mathcal{H}} h(r) \leq \left\{ \begin{array}{ll} \min(\sqrt{2},1/r) & \mbox{if $p=1$} \\
% \min\Bigl\{\frac{(p+1)^{p/2}}{(p-1)!}r^{p-1} \, , 24r \, , \frac{p}{r}\Bigr\} & \mbox{if $p \geq 2$.} \end{array} \right.
% \end{equation}
% \end{lemma}
% \textbf{Remark:} The only difference between the cases $p=1$ and $p \geq 2$ is that the bound $\sup_{h \in \mathcal{H}} h(r) \leq 24r$ does not hold when $p=1$.  The bounds $\frac{(p+1)^{p/2}}{(p-1)!}r^{p-1}$ and $p/r$ are sharp when $r=0$ and $r = (p+2)^{1/2}$ respectively.  The first of these facts is trivial unless $p=1$, but in that case one can observe that if we define $h:[0,\infty) \rightarrow [0,\infty)$ by $h(r) := \sqrt{2}e^{-\sqrt{2}r}$ then $h \in \mathcal{H}$ and $h(0) = \sqrt{2}$.  The second fact follows because if we define $h:[0,\infty) \rightarrow [0,\infty)$ by $h(r) := \frac{p}{(p+2)^{p/2}}r^{p-1}\mathbbm{1}_{\{r \in [0,(p+2)^{1/2}]\}}$, then $h \in \mathcal{H}$ and $h(\sqrt{p+2}) = p/(p+2)^{1/2}$.

% \textbf{Remark for us:} The second bound in~\eqref{Eq:ThreeBounds} seems to be unnecessary.
% \begin{proof}
% For the first bound in~\eqref{Eq:ThreeBounds} (treating the cases $p=1$ and $p \geq 2$ simultaneously), for $r \in [0,\infty)$, let 
% \[
% g_0^*(r) := \frac{(p+1)^{p/2}}{c_p(p-1)!}e^{-(p+1)^{1/2}r},
% \]
% so $g_0^* \in \mathcal{G}$, and let $h_0^*(r) := c_pr^{p-1}g_0^*(r)$.  Then $h_0^*$ is the $\Gamma(p,(p+1)^{1/2})$ density, so $h_0^* \in \mathcal{H}$.  Suppose for a contradiction that $g \in \mathcal{G}$ satisfies the conditions the function $h:[0,\infty) \rightarrow [0,\infty)$ given by $h(r) := c_pr^{p-1}g(r)$ belongs to $\mathcal{H}$, and $g(0) > g_0^*(0)$.  Then since $\log g_0^*$ is an affine function and $h$ is a log-concave density, there exists $r_0 \in (0,\infty)$ such that $g(r) > g_0^*(r)$ for $r < r_0$ and $g(r) < g_0^*(r)$ for $r > r_0$.  But then $h <_{\mathrm{st}} h^*$, so $c_p \int_0^\infty r^{p+1}g(r) \, dr < p$, which establishes our desired contradiction.  %Hence 
% %\[
% %\sup_{h \in \mathcal{H}} h(0) = h_0^*(0) = \frac{(p+1)^{p/2}}{\Gamma(p)}.
% %\]
% But since every $\phi \in \Phi$ is decreasing, it follows that $r \mapsto \sup_{g \in \mathcal{G}} g(r)$ is decreasing, so
% \[
% \sup_{h \in \mathcal{H}} h(r) = c_p \sup_{g \in \mathcal{G}} r^{p-1} g(r) \leq c_pr^{p-1}\sup_{g \in \mathcal{G}} g(0) = c_pr^{p-1}g_0^*(0) = \frac{(p+1)^{p/2}}{(p-1)!}r^{p-1}.
% \]
% Next we establish the third bound in~\eqref{Eq:ThreeBounds}, again treating $p=1$ and $p \geq 2$ simultaneously.  For $a \in (0,\infty)$ and $r \in (0,\infty)$, consider the function
% \[
% g_a(r) := \frac{p}{c_p a^p}\mathbbm{1}_{\{r \in [0,a]\}}.
% \]
% Then $g_a \in \mathcal{G}$ and $c_p \int_0^\infty r^{p-1}g_a(r) \, dr = 1$.  Thus if $g \in \mathcal{G}$ satisfies $g(a) > g_a(a)$, then $g(r) > g_a(r)$ for all $r \in [0,a]$ and $g(r) \geq g_a(r)$ for all $r \in [0,\infty)$.  But then $c_p \int_0^\infty r^{p-1}g(r) \, dr > 1$, so the function $h:[0,\infty) \rightarrow [0,\infty)$ given by $h(r) := c_pr^{p-1}g(r)$ does not belong to $\mathcal{H}$.  We deduce that for every $r \in (0,\infty)$,
% \[
% \sup_{h \in \mathcal{H}} h(r) \leq c_pr^{p-1}g_r(r) = \frac{p}{r}.
% \]
% Finally, we prove the second bound in~\eqref{Eq:ThreeBounds} in the case $p \geq 2$.  To this end, fix $M \geq \log 16$, and $m \in (-\infty,M-2]$.  Suppose that $h \in \mathcal{H}$ satisfies $\log h(r_0) \geq M$ for some $r_0 \in (1/4,p^{1/2}]$, and for $t \in [m,M]$, let $D_t := \{r \in [0,\infty):\log h(r) \geq t\}$.  First note that for any $t \in [m,M]$ and $r \in D_m$, we have
% \[
% \log h\biggl(\frac{t-m}{M-m}r_0 + \frac{M-t}{M-m}r\biggr) \geq \frac{(t-m)M}{M-m} + \frac{(M-t)m}{M-m} = t.
% \]
% Hence, writing $\mu$ for Lebesgue measure on $\mathbb{R}$,
% \[
% \mu(D_t) \geq \mu\biggl(\frac{t-m}{M-m}r_0 + \frac{M-t}{M-m}D_m\biggr) = \frac{M-t}{M-m}\mu(D_m).
% \]
% Using Fubini's theorem, we can now compute
% \begin{align*}
% 1 &\geq \int_{D_m} h(r) - e^m \, dr \geq \int_{D_m} \int_m^M e^s \mathbbm{1}_{\{\log h(r) \geq s\}} \, ds \, dr \\
% &= \int_m^M e^s \mu(D_s) \, ds \geq \frac{\mu(D_m)}{M-m}\int_m^M (M-s)e^s \, ds = \frac{\mu(D_m)e^M}{M-m}\int_0^{M-m} te^{-t} \, dt \\
% &\geq \frac{\mu(D_m)e^M}{2(M-m)}.
% \end{align*}
% Since $D_m$ is an interval containing $r_0$, we conclude that $\log h(r) \leq m$ whenever $|r-r_0| \geq 2(M-m)e^{-M}$.  Thus
% \[
% \log h(r) \leq M - \frac{|r-r_0|e^M}{2}
% \]
% for $|r-r_0| \geq 4e^{-M}$.  Noting that $r_0 - 4e^{-M} > 0$ and using the bound $h(r) \leq p/r$, it now follows that
% \begin{align*}
% p = \int_0^\infty r^2h(r) \, dr &\leq \int_0^{r_0 - 4e^{-M}} r^2 \exp\biggl\{M - \frac{(r_0-r)e^M}{2}\biggr\} \, dr + p\int_{r_0 - 4e^{-M}}^{r_0 + 4e^{-M}} r \, dr \\
% &\hspace{5cm}+ \int_{r_0 + 4e^{-M}}^\infty r^2 \exp\biggl\{M - \frac{(r-r_0)e^M}{2}\biggr\} \, dr \\
% &\leq 2\int_2^\infty \biggl(r_0 - \frac{2s}{e^M}\biggr)^2 e^{-s} \, ds + 8e^{-M}r_0p + 2\int_2^\infty \biggl(r_0 + \frac{2s}{e^M}\biggr)^2 e^{-s} \, ds \\
% &= 4e^{-2}r_0^2 + 32e^{-2M} + 8e^{-M}r_0p \leq p\biggl(\frac{2}{3} + 8e^{-M}r_0\biggr).
% \end{align*}
% We deduce that $e^{-M}r_0 \geq 1/24$, so $h(r) \leq \min(16,24r)$ for $r \in (1/4,p^{1/2}]$.  But our first bound in~\eqref{Eq:ThreeBounds} is at most $5r$ for $r \leq 1$ and $p \geq 2$, and the conclusion follows.
% \end{proof}
% \begin{corollary}
% \label{Cor:VarLowerBound}
% Let $Z \sim h \in \mathcal{H}$.  Then there exists a universal constant $c_0 > 0$ such that $\mathrm{Var}(Z) \geq c_0p^{-1}$.
% \end{corollary}
% \textbf{Remark:} Define $h:[0,\infty) \rightarrow [0,\infty)$ by $h(r) := \frac{p}{(p+2)^{p/2}}r^{p-1}\mathbbm{1}_{\{r \in [0,(p+2)^{1/2}]\}}$.  Then it can be shown that $h \in \mathcal{H}$, and if $Z \sim h$, then $\mathrm{Var}(Z) = p/(p+1)^2$.  Thus the bound given in Corollary~\ref{Cor:VarLowerBound} is sharp in terms of its dependence on $p$.
% \begin{proof}
% From the first bound in Lemma~\ref{Lemma:Crude}, we have 
% \[
% \sup_{h \in \mathcal{H}} \sup_{r \in [0,\infty)} h(r) \leq \sqrt{2}
% \]
% for $r \leq p^{1/2}/e$.  Write $\mu := \mathbb{E}(Z)$ and $\sigma^2 := \mathrm{Var}(Z)$.  By \citet[][Theorem~5.14(d)]{lovasz2007geometry}, we have
% \[
% \frac{1}{128\sigma} \leq h(\mu) \leq \sup_{h \in \mathcal{H}} \sup_{r \in [0,\infty)} h(r) \leq ep^{1/2}.
% \]
% The result follows.
% \end{proof}

% An upper bound on the variance of $Z \sim h \in \mathcal{H}$ is readiy available. 

% \begin{lemma} \citet[Lemma 1]{bobkov2003spectral}
%   \label{Lem:VarUpperBoundGeneral}
% Suppose $h$ is a density of the form $r^{p-1} g(r) c_p$ for some log-concave function $g(r)$, suppose $Z \sim h$, then, 
% \[
% \mathrm{Var}(Z) \leq \frac{1}{p} (\mathbb{E} Z)^2 
% \]
% \end{lemma}

% Under our constraint on $h \in \mathcal{H}$, we have that $(\mathbb{E} Z )^2 \leq \mathbb{E}[ Z^2] = p$. This gives us the following corollary:

% \begin{corollary}
% \label{Cor:VarUpperBound}
% Let $h \in \mathcal{H}$ and suppose $Z \sim h$. Then,
% \[
% \mathrm{Var}(Z) \leq 1
% \]
% \end{corollary}

% The upper bound is also tight. If we let $g(r) = e^{-a r} c$ where $a = \sqrt{\frac{(p+2)(p+1)}{p}}$ and $c$ be chosen such that $c c_p = \frac{a^p}{\Gamma(p)}$, then we have that the mean is $\sqrt{ \frac{p^3}{(p+2)(p+1)} }$ and the variance is $\frac{p^2}{(p+2)(p+1)}$. Thus, the variance of our chosen $g(r)$ gets arbitrarily close to $1$ for increasing $p$. 

% We need one more ingredient before we can state our envelope bound. 

% \begin{lemma}
% \label{Lem:UnivariateMuSigmaEnvelope}
% Let $\mathcal{F}^{\mu, \sigma^2} = \{ f \textrm{ log-concave density} \,:\, \mu_f = \mu,\, \sigma^2_f = \sigma^2 \}$. Then, there exists universal constants $A, B$ such that
% \[
% \sup_{f \in \mathcal{F}^{\mu, \sigma^2} } f(x) \leq \frac{A}{\sigma} \exp\left( - B \frac{| x - \mu |}{\sigma} \right)
% \]
% \end{lemma}

% \begin{proof}
% This follows directly from  \citet[Theorem 2]{kim2016global} by specializing to $d=1$ and performing a change of variables.
% \end{proof}

% The following theorem gives an envelope bound for the density class $\mathcal{H}$.
% \begin{theorem}
% For any absolute constant $c_1, c_2 > 0$, there exists constants $C_1, C_2$ such that
% \[
% \sup_{f \in \mathcal{H}} f(x) \leq 
%   \left\{
%    \begin{array}{cc} 
%    \frac{A'}{c_0} \sqrt{p} & \left| x - \sqrt{p} \right| \leq \frac{c_0}{B \sqrt{p}} \\
%    \frac{A'}{eB} \frac{1}{| x - \sqrt{p} |} & \frac{c_0}{B \sqrt{p}} \leq \left| x - \sqrt{p} \right| \leq \frac{1}{B} \\
%    A' e^{ - B | x - \sqrt{p} | } & \frac{1}{B} \leq \left| x - \sqrt{p} \right|  
%    \end{array} \right.
% \]
% \end{theorem}


% \begin{proof}

% Define $\mathcal{H}_\sigma = \{ f \in \mathcal{H} \,:\, \sigma_f = \sigma \}$ as the sub-class of $\mathcal{H}$ in which the densities have standard deviation $\sigma$. It is clear that $\mathcal{H}_\sigma = \emptyset$ for $\sigma \notin [\frac{c_0}{\sqrt{p}}, 1]$ by our upper and lower bounds on the variance of densities in $\mathcal{H}$ (Corollary~\ref{Cor:VarLowerBound}, \ref{Cor:VarUpperBound}).

% First, we observe
% \[
% \sup_{f \in \mathcal{H}} f(x) = \sup_{\sigma \in [\frac{c_0}{\sqrt{p}}, 1]} \sup_{f \in \mathcal{H}_\sigma} f(x) 
% \]

% And, by Lemma~\ref{Lem:UnivariateMuSigmaEnvelope} and by the fact that $\mu = \sqrt{ \mathbb{E} Z^2 - \sigma^2} = \sqrt{ p - \sigma^2}$, 
% \begin{align*}
% \sup_{f \in \mathcal{H}_\sigma} f(x) 
%       & \leq \frac{A}{\sigma} \exp \left( - B \frac{| x - \sqrt{p - \sigma^2} |}{\sigma} \right) \\
%    &\leq  \frac{A}{\sigma} \exp \left( - B \frac{| x - \sqrt{p} | - (\sqrt{p} - \sqrt{p - \sigma^2}) }{\sigma} \right) \\
%    &\leq \frac{A}{\sigma} \exp \left( - B \frac{| x - \sqrt{p} |}{\sigma} 
%                      + B \frac{ \frac{\sigma^2}{2 \sqrt{p - \sigma^2}}}{\sigma} \right)\\
%     &= \frac{A}{\sigma} \exp \left( - B \frac{| x - \sqrt{p} |}{\sigma} 
%                      + B \frac{\sigma}{2 \sqrt{p - \sigma^2}} \right)\\
%    &\leq \frac{A}{\sigma} \exp \left( - B \frac{| x - \sqrt{p} |}{\sigma} 
%                      + B \frac{1}{2 \sqrt{p - 1}} \right)\\
%     &\leq \frac{A'}{\sigma} \exp \left( - B \frac{| x - \sqrt{p} |}{\sigma} \right)
% \end{align*}

% The second inequality follows from triangle inequality. The third inequality follows because $\sqrt{p} - \sqrt{p - \sigma^2} \leq  \frac{d \sqrt{ p - \sigma^2} }{dp} \sigma^2$ by concavity of the square root function. The fourth inequality follows because $\sigma \leq 1$. And, in the last inequality, we define $A'$ to be a constant since $B \frac{1}{2 \sqrt{p-1}}$ is bounded by a constant for all $p$. 

% Let $H_\sigma(x) =  \frac{A'}{\sigma} \exp \left( - B \frac{| x - \sqrt{p} |}{\sigma} \right)$.

% If we write $\nu = \frac{1}{\sigma}$, then $\log H_\sigma (x)$ is a concave function of $\nu$. We solve for the optimal and get that $\sigma = \frac{1}{\sqrt{p}}$ if $B | x - \sqrt{p}| \leq \sqrt{c_0}{\sqrt{p}}$, $\sigma = B | x - \sqrt{p} |$ if $\frac{c_0}{\sqrt{p}} \leq B | x - \sqrt{p} | \leq 1$, and $\sigma = 1$ if $1 \leq B | x - \sqrt{p} |$. 

% Thus, if $B| x - \sqrt{p}| \leq \frac{c_0}{\sqrt{p}}$, we have that $\sup_{\sigma \in [c_0/\sqrt{p}, 1]} H_{\sigma}(x) \leq A' \frac{\sqrt{p} }{c_0}$. 
% If $\frac{c_0}{\sqrt{p}} \leq B | x - \sqrt{p} | \leq 1$, we have that $\sup_{\sigma \in [c_0/\sqrt{p}, 1]} H_{\sigma}(x) \leq \frac{A'}{e B | x - \sqrt{p} | }$. 
% If  $1 \leq B | x - \sqrt{p} |$, then $\sup_{\sigma \in [c_0/\sqrt{p}, 1]} H_{\sigma} (x) \leq A' \exp( - B | x - \sqrt{p}|)$. 

% \end{proof}
